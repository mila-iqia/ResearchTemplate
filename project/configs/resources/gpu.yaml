# @package _global_
trainer:
  accelerator: gpu
  devices: 1

hydra:
  mode: MULTIRUN
  launcher:
    nodes: 1
    ntasks_per_node: 1
    cpus_per_task: 4
    gpus_per_task: 1
    mem: 16G
    array_parallelism: 16 # max num of jobs to run in parallel
    # Other things to pass to `sbatch`:
    additional_parameters:
      time: 1-00:00:00 # maximum wall time allocated for the job (D-HH:MM:SS)


    ## A list of commands to add to the generated sbatch script before running srun:
    # setup:
    # - export LD_PRELOAD=/some/folder/with/libraries/
