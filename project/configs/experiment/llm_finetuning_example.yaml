# @package _global_

defaults:
  - override /algorithm: llm_finetuning
  - override /trainer/callbacks: default

algorithm:
  dataset_config:
    per_device_eval_batch_size: 4
    per_device_train_batch_size: 4
    block_size: 256
    validation_split_percentage: 10
    overwrite_cache: false

trainer:
  max_epochs: 10
  devices: auto
  strategy:
    _target_: lightning.pytorch.strategies.FSDPStrategy
    # https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html#optimize-the-sharding-strategy
    sharding_strategy: "FULL_SHARD"
  limit_val_batches: 1
  num_sanity_val_steps: 0
  val_check_interval: 50
  enable_checkpointing: true
  detect_anomaly: false # recommended to turn this on when debugging nans with fp16 training.
  callbacks:
    model_checkpoint:
      verbose: true
      # every_n_train_steps: 1000 # todo: restarting from a within-epoch checkpoint doesn't seem to work!

hydra:
  run:
    # output directory, generated dynamically on each run
    dir: logs/${name}
name: llm_finetuning_example
ckpt_path: last
