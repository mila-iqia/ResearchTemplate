_target_: project.algorithms.llm_finetuning_example.LLMFinetuningExample
network_config:
  _target_: project.algorithms.llm_finetuning_example.NetworkConfig
  _recursive_: false
  _convert_: object
  pretrained_model_name_or_path: facebook/opt-350m
  # torch_dtype:
  #   _target_: hydra.utils.get_object
  #   path: torch.float16
  # attn_implementation: "flash_attention_2"
  # device_map: auto  # use this (and run `uv add accelerate`) for models larger than a GPU's VRAM.
tokenizer_config:
  _target_: project.algorithms.llm_finetuning_example.TokenizerConfig
  _recursive_: false
  _convert_: object
  pretrained_model_name_or_path: facebook/opt-350m
  use_fast: true
  trust_remote_code: true
dataset_config:
  _target_: project.algorithms.llm_finetuning_example.DatasetConfig
  dataset_path: wikitext
  dataset_name: wikitext-103-v1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  block_size: 256
learning_rate: 2e-5
adam_epsilon: 1e-8
warmup_steps: 0
weight_decay: 0
init_seed: 42
