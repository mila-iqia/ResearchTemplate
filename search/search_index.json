{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#research-project-template","title":"Research Project Template","text":"<p>Work-in-Progress</p> <p>Please note: This is a work-in-progress and will get better over time! We want your feedback!\ud83d\ude4f</p> <p>This is a research project template. It is meant to be a starting point for ML researchers at Mila.</p> <p>For more context, see this  introduction to the project..</p> <ul> <li> <p> Set up in 5 minutes</p> <p>Get started quickly with an interactive installation script and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> Well-tested, robust codebase</p> <p>Focus on your research! Let tests take care of detecting bugs and broken configs!</p> <p> Check out the included tests</p> </li> <li> <p> Support for both PyTorch and Jax</p> <p>You can use both PyTorch and Jax for your algorithms! (Lightning handles the rest.)</p> <p> Check out the Jax example</p> </li> <li> <p> Ready-to-use examples</p> <p>Includes examples for Supervised learning(1) and NLP \ud83e\udd17, with unsupervised learning and RL coming soon.</p> <ol> <li>The source code for the example is available here</li> </ol> <p> Check out the examples here</p> </li> </ul>"},{"location":"#starting-a-new-project","title":"Starting a new project","text":""},{"location":"#setting-up-your-environment","title":"Setting up your environment","text":"Locally (Linux / Mac)Locally (Windows) <ol> <li> <p>Install <code>uv</code>:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> </li> <li> <p>Create your new project</p> <p>Use this command to create a new project from this template: (Replace <code>research_project</code> with the path to the new project root folder.)</p> <pre><code>uvx copier copy --trust gh:mila-iqia/ResearchTemplate research_project\n</code></pre> <p>This will ask you a few questions and help setup your project.</p> </li> </ol> <ol> <li>Install WSL following this guide</li> <li>Follow the installation instructions for Linux</li> </ol> <p>Navigate to this new project, open up your favorite IDE, and voila! You're all setup! \ud83c\udf8a</p> <p>Use this command to see all available options:</p> <pre><code>. .venv/bin/activate  # activate the virtual environment\npython project/main.py --help\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To see all available options:</p> <pre><code>python project/main.py --help\n</code></pre> <p>For a detailed list of examples, see the examples page.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Intro</li> <li>Features \ud83d\udd25<ul> <li>Magic Config Schemas</li> <li>Jax and Torch support with Lightning \u26a1</li> <li>Launching Jobs on Remote Clusters</li> <li>Thorough automated testing on SLURM clusters</li> <li>features/*.md</li> </ul> </li> <li>Examples \ud83e\uddea<ul> <li>Image Classification (\u26a1)</li> <li>Image Classification (jax+\u26a1)</li> <li>Text Classification (\ud83e\udd17+\u26a1)</li> <li>Fine-tuning an LLM (\ud83e\udd17+\u26a1)</li> <li>Reinforcement Learning (jax)</li> <li>Running sweeps</li> <li>Profiling your code\ud83d\udcce</li> <li>examples/*.md</li> </ul> </li> <li>Reference \ud83e\udd13<ul> <li>reference/*</li> </ul> </li> <li>Learning Resources</li> <li>Getting Help</li> <li>Contributing</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>TODOs:</p> <ul> <li>[ ] Describe how to contribute to the project.</li> </ul>"},{"location":"help/","title":"Getting Help","text":""},{"location":"help/#help-and-support","title":"Help and Support","text":""},{"location":"help/#faq","title":"FAQ","text":""},{"location":"help/#how-to-get-help","title":"How to get help","text":"<ul> <li>Make an Issue on GitHub</li> <li>Reach out via Slack (if you're a researcher at Mila)</li> <li>Reach out via email</li> </ul>"},{"location":"intro/","title":"Intro","text":""},{"location":"intro/#why-use-this-template","title":"Why use this template?","text":""},{"location":"intro/#why-should-you-use-a-template-in-the-first-place","title":"Why should you use a template in the first place?","text":"<p>For many good reasons, which are very well described here in a similar project! \ud83d\ude0a</p> <p>Other good reads:</p> <ul> <li>https://cookiecutter-data-science.drivendata.org/why/</li> <li>https://cookiecutter-data-science.drivendata.org/opinions/</li> <li>https://12factor.net/</li> <li>https://github.com/ashleve/lightning-hydra-template/tree/main?tab=readme-ov-file#main-ideas</li> </ul>"},{"location":"intro/#why-use-this-template_1","title":"Why use this template?","text":"<ul> <li>Cool, unique features that can only be found here (for now)!</li> </ul>"},{"location":"intro/#project-layout","title":"Project layout","text":"<pre><code>pyproject.toml   # Project metadata and dependencies\nproject/\n    main.py      # main entry-point\n    algorithms/  # learning algorithms\n    datamodules/ # datasets, processing and loading\n    networks/    # Neural networks used by algorithms\n    configs/     # Hydra configuration files\ndocs/            # documentation\nconftest.py      # Test fixtures and utilities\n</code></pre>"},{"location":"intro/#libraries-used","title":"Libraries used","text":"<p>This project makes use of the following libraries:</p> <ul> <li>Hydra is used to configure the project. It allows you to define configuration files and override them from the command line.</li> <li>PyTorch Lightning is used to as the training framework. It provides a high-level interface to organize ML research code.<ul> <li>\ud83d\udd25 Please note: You can also use Jax with this repo, as described in the Jax example \ud83d\udd25</li> </ul> </li> <li>Weights &amp; Biases is used to log metrics and visualize results.</li> <li>pytest is used for testing.</li> </ul>"},{"location":"resources/","title":"Learning Resources","text":""},{"location":"resources/#related-projects-and-resources","title":"Related projects and resources","text":""},{"location":"resources/#hydra-docs","title":"Hydra docs","text":""},{"location":"resources/#other-project-templates","title":"Other project templates","text":"<p>There are other project templates out there, that often have better documentation. If you need an introduction to Hydra, or Lightning, or good software development practices, these might have better guides and documentation for you.</p> <p>Here are some we'd recommend:</p>"},{"location":"resources/#lightning-hydra-template","title":"lightning-hydra-template","text":"<ul> <li>How it works: https://github.com/gorodnitskiy/yet-another-lightning-hydra-template/tree/main?tab=readme-ov-file#workflow---how-it-works</li> </ul> <p>For everything that has to do with Hydra and PyTorch-Lightning, their documentation also applies directly to this template. In order to avoid copying their documentation, we recommend you take a look at their nice readme.</p>"},{"location":"resources/#yet-another-lightning-hydra-template","title":"yet-another-lightning-hydra-template","text":"<pre><code>- Excellent template.  based on the lightning-hydra-template. Great documentation, which is referenced extensively in this project.\n- - Has a **great** Readme with lots of information\n- - Is really well organized\n- - doesn't support Jax\n- - doesn't have a devcontainer\n- Great blog: https://hackernoon.com/yet-another-lightning-hydra-template-for-ml-experiments\n</code></pre>"},{"location":"resources/#cookiecutter-data-science","title":"cookiecutter-data-science","text":"<pre><code>- Awesome library for data science.\n- Related projects: https://github.com/drivendataorg/cookiecutter-data-science/blob/master/docs/docs/related.md#links-to-related-projects-and-references\n</code></pre>"},{"location":"examples/","title":"Examples \ud83e\uddea","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>This template includes examples that use either Jax, PyTorch, or both!</p> Example link Research Area Reference link Frameworks Image Classification Supervised Learning (image classification) <code>ImageClassifier</code> Torch + \u26a1 Image Classification (Jax) Supervised Learning (image classification) <code>JaxImageClassifier</code> Torch + Jax + \u26a1 Text Classification NLP (text classification) <code>TextClassifier</code> Torch + \ud83e\udd17 + \u26a1 Reinforcement Learning (Jax) RL <code>JaxRLExample</code> Jax LLM Fine-tuning NLP (Causal language modeling) <code>LLMFineTuningExample</code> Torch + \ud83e\udd17 + \u26a1"},{"location":"examples/image_classification/","title":"Image Classification (\u26a1)","text":""},{"location":"examples/image_classification/#supervised-learning-pytorch","title":"Supervised Learning (PyTorch)","text":""},{"location":"examples/image_classification/#imageclassifier","title":"ImageClassifier","text":"<p>The <code>ImageClassifier</code> is a simple <code>LightningModule</code> for image classification. It accepts a vision datamodule as input.</p> Click to show the code of the ImageClassifier class. <pre><code>class ImageClassifier(LightningModule):\n    \"\"\"Example learning algorithm for image classification.\"\"\"\n\n    def __init__(\n        self,\n        datamodule: ImageClassificationDataModule,\n        network: HydraConfigFor[torch.nn.Module],\n        optimizer: HydraConfigFor[functools.partial[Optimizer]],\n        init_seed: int = 42,\n    ):\n        \"\"\"Create a new instance of the algorithm.\n\n        Parameters:\n            datamodule: Object used to load train/val/test data.\n                See the lightning docs for [LightningDataModule][lightning.pytorch.core.datamodule.LightningDataModule]\n                for more info.\n            network:\n                The config of the network to instantiate and train.\n            optimizer: The config for the Optimizer. Instantiating this will return a function \\\n                (a [functools.partial][]) that will create the Optimizer given the hyper-parameters.\n            init_seed: The seed to use when initializing the weights of the network.\n        \"\"\"\n        super().__init__()\n        self.datamodule = datamodule\n        self.network_config = network\n        self.optimizer_config = optimizer\n        self.init_seed = init_seed\n\n        # Save hyper-parameters.\n        self.save_hyperparameters(ignore=[\"datamodule\"])\n        # Used by Pytorch-Lightning to compute the input/output shapes of the network.\n\n        self.network: torch.nn.Module | None = None\n\n    def configure_model(self):\n        # Save this for PyTorch-Lightning to infer the input/output shapes of the network.\n        if self.network is not None:\n            logger.info(\"Network is already instantiated.\")\n            return\n        self.example_input_array = torch.zeros((self.datamodule.batch_size, *self.datamodule.dims))\n        with torch.random.fork_rng():\n            # deterministic weight initialization\n            torch.manual_seed(self.init_seed)\n            self.network = hydra_zen.instantiate(self.network_config)\n            if any(torch.nn.parameter.is_lazy(p) for p in self.network.parameters()):\n                # Do a forward pass to initialize any lazy weights. This is necessary for\n                # distributed training and to infer shapes.\n                _ = self.network(self.example_input_array)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the network.\"\"\"\n        assert self.network is not None\n        logits = self.network(input)\n        return logits\n\n    def training_step(self, batch: tuple[Tensor, Tensor], batch_index: int):\n        return self.shared_step(batch, batch_index=batch_index, phase=\"train\")\n\n    def validation_step(self, batch: tuple[Tensor, Tensor], batch_index: int):\n        return self.shared_step(batch, batch_index=batch_index, phase=\"val\")\n\n    def test_step(self, batch: tuple[Tensor, Tensor], batch_index: int):\n        return self.shared_step(batch, batch_index=batch_index, phase=\"test\")\n\n    def shared_step(\n        self,\n        batch: tuple[Tensor, Tensor],\n        batch_index: int,\n        phase: Literal[\"train\", \"val\", \"test\"],\n    ):\n        x, y = batch\n        logits: torch.Tensor = self(x)\n        loss = F.cross_entropy(logits, y, reduction=\"mean\")\n        self.log(f\"{phase}/loss\", loss.detach().mean())\n        acc = logits.detach().argmax(-1).eq(y).float().mean()\n        self.log(f\"{phase}/accuracy\", acc)\n        return {\"loss\": loss, \"logits\": logits, \"y\": y}\n\n    def configure_optimizers(self):\n        \"\"\"Creates the optimizers.\n\n        See [`lightning.pytorch.core.LightningModule.configure_optimizers`][] for more information.\n        \"\"\"\n        # Instantiate the optimizer config into a functools.partial object.\n        optimizer_partial = hydra_zen.instantiate(self.optimizer_config)\n        # Call the functools.partial object, passing the parameters as an argument.\n        optimizer = optimizer_partial(self.parameters())\n        # This then returns the optimizer.\n        return optimizer\n\n    def configure_callbacks(self) -&gt; Sequence[Callback] | Callback:\n        \"\"\"Creates callbacks to be used by default during training.\"\"\"\n        return [\n            ClassificationMetricsCallback.attach_to(self, num_classes=self.datamodule.num_classes)\n        ]\n</code></pre>"},{"location":"examples/image_classification/#running-the-example","title":"Running the example","text":"<p>Here is a configuration file that you can use to launch a simple experiment:</p> Click to show the yaml config file <pre><code># @package _global_\n\n# This is an \"experiment\" config, that groups together other configs into a ready-to-run example.\n\n# To execute this experiment, use:\n# python project/main.py experiment=example\n\ndefaults:\n  - override /algorithm: image_classifier\n  - override /algorithm/network: resnet18\n  - override /datamodule: cifar10\n  - override /trainer: default\n  - override /trainer/logger: tensorboard\n  - override /trainer/callbacks: default\n\n# The parameters below will be merged with parameters from default configurations set above.\n# This allows you to overwrite only specified parameters\n\n# The name of the experiment (for logging)\nname: example\n\nseed: ${oc.env:SLURM_PROCID,42}\n\nalgorithm:\n  optimizer:\n    lr: 0.002\n\ndatamodule:\n  batch_size: 64\n\ntrainer:\n  min_epochs: 1\n  max_epochs: 10\n  gradient_clip_val: 0.5\n</code></pre> <p>You can use it like so:</p> <pre><code>python project/main.py experiment=example\n</code></pre>"},{"location":"examples/jax_image_classification/","title":"Image Classification (jax+\u26a1)","text":""},{"location":"examples/jax_image_classification/#jax-pytorch-lightning","title":"Jax + PyTorch-Lightning \u26a1","text":""},{"location":"examples/jax_image_classification/#a-lightningmodule-that-trains-a-jax-network","title":"A LightningModule that trains a Jax network","text":"<p>The <code>JaxImageClassifier</code> algorithm uses a network which is a flax.linen.Module. The network is wrapped with <code>torch_jax_interop.JaxFunction</code>, so that it can accept torch tensors as inputs, produces torch tensors as outputs, and the parameters are saved as as <code>torch.nn.Parameter</code>s (which use the same underlying memory as the jax arrays). In this example, the loss function and optimizers are in PyTorch, while the network forward and backward passes are written in Jax.</p> <p>The loss that is returned in the training step is used by Lightning in the usual way. The backward pass uses Jax to calculate the gradients, and the weights are updated by a PyTorch optimizer.</p> <p>Info</p> <p>You could also very well do both the forward and backward passes in Jax! To do this, use the 'manual optimization' mode of PyTorch-Lightning and perform the parameter updates yourself. For the rest of Lightning to work, just make sure to store the parameters as torch.nn.Parameters. An example of how to do this will be added shortly.</p> <p>What about end-to-end training in Jax?</p> <p>See the Jax RL Example! </p>"},{"location":"examples/jax_image_classification/#jax-network","title":"Jax Network","text":"<pre><code>class JaxCNN(flax.linen.Module):\n    \"\"\"A simple CNN model.\n\n    Taken from https://flax.readthedocs.io/en/latest/quick_start.html#define-network\n    \"\"\"\n\n    num_classes: int = 10\n\n    @flax.linen.compact\n    def __call__(self, x: jax.Array):\n        x = to_channels_last(x)\n        x = flax.linen.Conv(features=32, kernel_size=(3, 3))(x)\n        x = flax.linen.relu(x)\n        x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = flax.linen.Conv(features=64, kernel_size=(3, 3))(x)\n        x = flax.linen.relu(x)\n        x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n\n        x = flatten(x)\n        x = flax.linen.Dense(features=256)(x)\n        x = flax.linen.relu(x)\n        x = flax.linen.Dense(features=self.num_classes)(x)\n        return x\n</code></pre>"},{"location":"examples/jax_image_classification/#jax-algorithm","title":"Jax Algorithm","text":"<pre><code>class JaxImageClassifier(LightningModule):\n    \"\"\"Example of a learning algorithm (`LightningModule`) that uses Jax.\n\n    In this case, the network is a flax.linen.Module, and its forward and backward passes are\n    written in Jax, and the loss function is in pytorch.\n    \"\"\"\n\n    def __init__(\n        self,\n        datamodule: ImageClassificationDataModule,\n        network: HydraConfigFor[flax.linen.Module],\n        optimizer: HydraConfigFor[functools.partial[Optimizer]],\n        init_seed: int = 123,\n        debug: bool = True,\n    ):\n        super().__init__()\n        self.datamodule = datamodule\n        self.network_config = network\n        self.optimizer_config = optimizer\n        self.init_seed = init_seed\n        self.debug = debug\n\n        # Create the jax network (safe to do even on CPU here).\n        self.jax_network: flax.linen.Module = hydra_zen.instantiate(self.network_config)\n        # We'll instantiate the parameters and the torch wrapper around the jax network in\n        # `configure_model` so the weights are directly on the GPU.\n        self.network: torch.nn.Module | None = None\n        self.save_hyperparameters(ignore=[\"datamodule\"])\n\n    def configure_model(self):\n        if self.network is not None:\n            logger.info(\"Network is already instantiated.\")\n            return\n        example_input = torch.zeros(\n            (self.datamodule.batch_size, *self.datamodule.dims),\n        )\n        # Save this for PyTorch-Lightning to infer the input/output shapes of the network.\n        self.example_input_array = example_input\n\n        # Initialize the jax parameters with a forward pass.\n        jax_params = self.jax_network.init(\n            jax.random.key(self.init_seed), torch_to_jax(example_input)\n        )\n\n        jax_network_forward = self.jax_network.apply\n        if not self.debug:\n            jax_network_forward = jax.jit(jax_network_forward)\n\n        # Wrap the jax network into a nn.Module:\n        self.network = WrappedJaxFunction(\n            jax_function=jax_network_forward,\n            jax_params=jax_params,\n            # Need to call .clone() when doing distributed training, otherwise we get a RuntimeError:\n            # Invalid device pointer when trying to share the CUDA tensors that come from jax.\n            clone_params=True,\n            has_aux=False,\n        )\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        assert self.network is not None\n        logits = self.network(input)\n        return logits\n\n    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_index: int):\n        return self.shared_step(batch, batch_index=batch_index, phase=\"train\")\n\n    def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_index: int):\n        return self.shared_step(batch, batch_index=batch_index, phase=\"val\")\n\n    def test_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_index: int):\n        return self.shared_step(batch, batch_index=batch_index, phase=\"test\")\n\n    def shared_step(\n        self,\n        batch: tuple[torch.Tensor, torch.Tensor],\n        batch_index: int,\n        phase: Literal[\"train\", \"val\", \"test\"],\n    ):\n        # This is the same thing as the `ImageClassifier.shared_step`!\n        x, y = batch\n        assert not x.requires_grad\n        # This calls self.forward, and is preferable to calling self.network directly, since it\n        # allows forward hooks to be called. This is useful for example when testing or debugging.\n        logits = self(x)\n\n        assert isinstance(logits, torch.Tensor)\n        # In this example we use a jax \"encoder\" network and a PyTorch loss function, but we could\n        # also just as easily have done the whole forward and backward pass in jax if we wanted to.\n        loss = F.cross_entropy(logits, y, reduction=\"mean\")\n        acc = logits.argmax(-1).eq(y).float().mean()\n        self.log(f\"{phase}/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(f\"{phase}/acc\", acc, prog_bar=True, sync_dist=True)\n        return {\"loss\": loss, \"logits\": logits, \"y\": y}\n\n    def configure_optimizers(self):\n        \"\"\"Creates the optimizers.\n\n        See [`lightning.pytorch.core.LightningModule.configure_optimizers`][] for more information.\n        \"\"\"\n        # Instantiate the optimizer config into a functools.partial object.\n        optimizer_partial = hydra_zen.instantiate(self.optimizer_config)\n        # Call the functools.partial object, passing the parameters as an argument.\n        optimizer = optimizer_partial(self.parameters())\n        # This then returns the optimizer.\n        return optimizer\n\n    def configure_callbacks(self) -&gt; list[Callback]:\n        assert isinstance(self.datamodule, ImageClassificationDataModule)\n        return [\n            MeasureSamplesPerSecondCallback(),\n            ClassificationMetricsCallback.attach_to(self, num_classes=self.datamodule.num_classes),\n        ]\n</code></pre>"},{"location":"examples/jax_image_classification/#configs","title":"Configs","text":""},{"location":"examples/jax_image_classification/#lightningmodule-config","title":"LightningModule config","text":"<pre><code># Config for the JaxImageClassifier algorithm\ndefaults:\n  - network: jax_cnn\n  - optimizer: SGD\n_target_: project.algorithms.jax_image_classifier.JaxImageClassifier\n# NOTE: Why _partial_ here? Because the config doesn't create the algo directly.\n# The datamodule is instantiated first and then passed to the algorithm.\n_partial_: true\n_recursive_: false\n\noptimizer:\n  lr: 0.001\n\ninit_seed: 123\ndebug: False\n</code></pre>"},{"location":"examples/jax_image_classification/#running-the-example","title":"Running the example","text":"<pre><code>$ python project/main.py algorithm=jax_image_classifier network=jax_cnn datamodule=cifar10\n</code></pre>"},{"location":"examples/jax_rl/","title":"Reinforcement Learning (jax)","text":""},{"location":"examples/jax_rl/#reinforcement-learning-in-jax","title":"Reinforcement Learning in Jax","text":"<p>This example follows the same structure as the other examples:</p> <ul> <li>An \"algorithm\" (in this case <code>JaxRLExample</code>) is trained with a \"trainer\" (<code>JaxTrainer</code>);</li> </ul> <p>However, there are some very important differences:</p> <ul> <li>There is no \"datamodule\". The algorithm accepts an Environment (<code>gymnax.Environment</code>) as input.</li> <li>The \"Trainer\" is a <code>JaxTrainer</code>, instead of a <code>lightning.Trainer</code>.</li> <li>The full training loop is written in Jax;</li> <li>Some (but not all) PyTorch-Lightning callbacks can still be used with the JaxTrainer;</li> <li>The <code>JaxRLExample</code> class is an algorithm based on rejax.PPO.</li> </ul>"},{"location":"examples/jax_rl/#jaxrlexample","title":"JaxRLExample","text":"<p>The <code>JaxRLExample</code> is based on rejax.PPO. It follows the structure of a <code>JaxModule</code>, and is trained with a <code>JaxTrainer</code>.</p> Click to show the code for JaxRLExample <pre><code>class JaxRLExample(\n    flax.struct.PyTreeNode,\n    JaxModule[PPOState[TEnvState], TrajectoryWithLastObs, EvalMetrics],\n    Generic[TEnvState, TEnvParams],\n):\n    \"\"\"Example of an RL algorithm written in Jax: PPO, based on `rejax.PPO`.\n\n    ## Differences w.r.t. rejax.PPO:\n\n    - The state / hparams are split into different, fully-typed structs:\n        - The algorithm state is in a typed `PPOState` struct (vs an untyped,\n            dynamically-generated struct in rejax).\n        - The hyper-parameters are in a typed `PPOHParams` struct.\n        - The state variables related to the collection of data from the environment is a\n            `TrajectoryCollectionState` instead of everything being bunched up together.\n            - This makes it easier to call the `collect_episodes` function with just what it needs.\n    - The seeds for the networks and the environment data collection are separated.\n\n    The logic is exactly the same: The losses / updates are computed in the exact same way.\n    \"\"\"\n\n    env: Environment[TEnvState, TEnvParams] = flax.struct.field(pytree_node=False)\n    env_params: TEnvParams\n    actor: flax.linen.Module = flax.struct.field(pytree_node=False)\n    critic: flax.linen.Module = flax.struct.field(pytree_node=False)\n    hp: PPOHParams\n\n    @classmethod\n    def create(\n        cls,\n        env_id: str | None = None,\n        env: Environment[TEnvState, TEnvParams] | None = None,\n        env_params: TEnvParams | None = None,\n        hp: PPOHParams | None = None,\n    ) -&gt; JaxRLExample[TEnvState, TEnvParams]:\n        from brax.envs import _envs as brax_envs\n        from rejax.compat.brax2gymnax import create_brax\n\n        # env_params: gymnax.EnvParams\n        if env_id is None:\n            assert env is not None\n            env_params = env_params or env.default_params  # type: ignore\n        elif env_id in brax_envs:\n            env, env_params = create_brax(  # type: ignore\n                env_id,\n                episode_length=1000,\n                action_repeat=1,\n                auto_reset=True,\n                batch_size=None,\n                backend=\"generalized\",\n            )\n        elif isinstance(env_id, str):\n            env, env_params = gymnax.make(env_id=env_id)  # type: ignore\n        else:\n            raise NotImplementedError(env_id)\n\n        assert env is not None\n        assert env_params is not None\n        return cls(\n            env=env,\n            env_params=env_params,\n            actor=cls.create_actor(env, env_params),\n            critic=cls.create_critic(),\n            hp=hp or PPOHParams(),\n        )\n\n    @classmethod\n    def create_networks(\n        cls,\n        env: Environment[gymnax.EnvState, TEnvParams],\n        env_params: TEnvParams,\n        config: _NetworkConfig,\n    ):\n        # Equivalent to:\n        # return rejax.PPO.create_agent(config, env, env_params)\n        return {\n            \"actor\": cls.create_actor(env, env_params, **config[\"agent_kwargs\"]),\n            \"critic\": cls.create_actor(env, env_params, **config[\"agent_kwargs\"]),\n        }\n\n    _TEnvParams = TypeVar(\"_TEnvParams\", bound=gymnax.EnvParams, covariant=True)\n    _TEnvState = TypeVar(\"_TEnvState\", bound=gymnax.EnvState, covariant=True)\n\n    @classmethod\n    def create_actor(\n        cls,\n        env: Environment[_TEnvState, _TEnvParams],\n        env_params: _TEnvParams,\n        activation: str | Callable[[jax.Array], jax.Array] = \"swish\",\n        hidden_layer_sizes: Sequence[int] = (64, 64),\n        **actor_kwargs,\n    ) -&gt; DiscretePolicy | GaussianPolicy:\n        activation_fn: Callable[[jax.Array], jax.Array] = (\n            getattr(flax.linen, activation) if not callable(activation) else activation\n        )\n        hidden_layer_sizes = tuple(hidden_layer_sizes)\n        action_space = env.action_space(env_params)\n\n        if isinstance(action_space, gymnax.environments.spaces.Discrete):\n            return DiscretePolicy(\n                action_space.n,\n                activation=activation_fn,\n                hidden_layer_sizes=hidden_layer_sizes,\n                **actor_kwargs,\n            )\n        assert isinstance(action_space, gymnax.environments.spaces.Box)\n        return GaussianPolicy(\n            np.prod(action_space.shape),\n            (action_space.low, action_space.high),  # type: ignore\n            activation=activation_fn,\n            hidden_layer_sizes=hidden_layer_sizes,\n            **actor_kwargs,\n        )\n\n    @classmethod\n    def create_critic(\n        cls,\n        activation: str | Callable[[jax.Array], jax.Array] = \"swish\",\n        hidden_layer_sizes: Sequence[int] = (64, 64),\n        **critic_kwargs,\n    ) -&gt; VNetwork:\n        activation_fn: Callable[[jax.Array], jax.Array] = (\n            getattr(flax.linen, activation) if isinstance(activation, str) else activation\n        )\n        hidden_layer_sizes = tuple(hidden_layer_sizes)\n        return VNetwork(\n            hidden_layer_sizes=hidden_layer_sizes, activation=activation_fn, **critic_kwargs\n        )\n\n    def init_train_state(self, rng: chex.PRNGKey) -&gt; PPOState[TEnvState]:\n        rng, networks_rng, env_rng = jax.random.split(rng, 3)\n\n        rng_actor, rng_critic = jax.random.split(networks_rng, 2)\n\n        obs_ph = jnp.empty([1, *self.env.observation_space(self.env_params).shape])\n\n        actor_params = self.actor.init(rng_actor, obs_ph, rng_actor)\n        critic_params = self.critic.init(rng_critic, obs_ph)\n\n        tx = optax.adam(learning_rate=self.hp.learning_rate)\n        # TODO: Why isn't the `apply_fn` not set in rejax?\n        actor_ts = TrainState.create(apply_fn=self.actor.apply, params=actor_params, tx=tx)\n        critic_ts = TrainState.create(apply_fn=self.critic.apply, params=critic_params, tx=tx)\n\n        env_rng, reset_rng = jax.random.split(env_rng)\n        obs, env_state = jax.vmap(self.env.reset, in_axes=(0, None))(\n            jax.random.split(reset_rng, self.hp.num_envs), self.env_params\n        )\n\n        collection_state = TrajectoryCollectionState(\n            last_obs=obs,\n            rms_state=RMSState.create(shape=obs_ph.shape),\n            global_step=0,\n            env_state=env_state,\n            last_done=jnp.zeros(self.hp.num_envs, dtype=bool),\n            rng=env_rng,\n        )\n\n        return PPOState(\n            actor_ts=actor_ts,\n            critic_ts=critic_ts,\n            rng=rng,\n            data_collection_state=collection_state,\n        )\n\n    def training_step(self, batch_idx: int, ts: PPOState[TEnvState], batch: TrajectoryWithLastObs):\n        \"\"\"Training step in pure jax.\"\"\"\n        trajectories = batch\n\n        ts, (actor_losses, critic_losses) = jax.lax.scan(\n            functools.partial(self.ppo_update_epoch, trajectories=trajectories),\n            init=ts,\n            xs=jnp.arange(self.hp.num_epochs),  # type: ignore\n            length=self.hp.num_epochs,\n        )\n\n        return ts, TrainStepMetrics(actor_losses=actor_losses, critic_losses=critic_losses)\n\n    def ppo_update_epoch(\n        self, ts: PPOState[TEnvState], epoch_index: int, trajectories: TrajectoryWithLastObs\n    ):\n        minibatch_rng = jax.random.fold_in(ts.rng, epoch_index)\n\n        last_val = self.critic.apply(ts.critic_ts.params, ts.data_collection_state.last_obs)\n        assert isinstance(last_val, jax.Array)\n        last_val = jnp.where(ts.data_collection_state.last_done, 0, last_val)\n        advantages, targets = calculate_gae(\n            trajectories, last_val, gamma=self.hp.gamma, gae_lambda=self.hp.gae_lambda\n        )\n        batch = AdvantageMinibatch(trajectories.trajectories, advantages, targets)\n        minibatches = shuffle_and_split(\n            batch, minibatch_rng, num_minibatches=self.hp.num_minibatches\n        )\n\n        # shuffle the data and split it into minibatches\n\n        num_steps = self.hp.num_steps\n        num_envs = self.hp.num_envs\n        num_minibatches = self.hp.num_minibatches\n        assert (num_envs * num_steps) % num_minibatches == 0\n        minibatches = shuffle_and_split(\n            batch,\n            minibatch_rng,\n            num_minibatches=num_minibatches,\n        )\n        return jax.lax.scan(self.ppo_update, ts, minibatches, length=self.hp.num_minibatches)\n\n    def ppo_update(self, ts: PPOState[TEnvState], batch: AdvantageMinibatch):\n        actor_loss, actor_grads = jax.value_and_grad(actor_loss_fn)(\n            ts.actor_ts.params,\n            actor=self.actor,\n            batch=batch,\n            clip_eps=self.hp.clip_eps,\n            ent_coef=self.hp.ent_coef,\n        )\n        assert isinstance(actor_loss, jax.Array)\n        critic_loss, critic_grads = jax.value_and_grad(critic_loss_fn)(\n            ts.critic_ts.params,\n            critic=self.critic,\n            batch=batch,\n            clip_eps=self.hp.clip_eps,\n            vf_coef=self.hp.vf_coef,\n        )\n        assert isinstance(critic_loss, jax.Array)\n\n        actor_ts = ts.actor_ts.apply_gradients(grads=actor_grads)\n        critic_ts = ts.critic_ts.apply_gradients(grads=critic_grads)\n\n        return ts.replace(actor_ts=actor_ts, critic_ts=critic_ts), (actor_loss, critic_loss)\n\n    def eval_callback(\n        self, ts: PPOState[TEnvState], rng: chex.PRNGKey | None = None\n    ) -&gt; EvalMetrics:\n        if rng is None:\n            rng = ts.rng\n        actor = make_actor(ts=ts, hp=self.hp)\n        ep_lengths, cum_rewards = evaluate(\n            actor,\n            ts.rng,\n            self.env,\n            self.env_params,\n            num_seeds=self.hp.num_seeds_per_eval,\n            max_steps_in_episode=self.env_params.max_steps_in_episode,\n        )\n        return EvalMetrics(episode_length=ep_lengths, cumulative_reward=cum_rewards)\n\n    def get_batch(\n        self, ts: PPOState[TEnvState], batch_idx: int\n    ) -&gt; tuple[PPOState[TEnvState], TrajectoryWithLastObs]:\n        data_collection_state, trajectories = self.collect_trajectories(\n            ts.data_collection_state,\n            actor_params=ts.actor_ts.params,\n            critic_params=ts.critic_ts.params,\n        )\n        ts = ts.replace(data_collection_state=data_collection_state)\n        return ts, trajectories\n\n    # @jit\n    def collect_trajectories(\n        self,\n        collection_state: TrajectoryCollectionState[TEnvState],\n        actor_params: FrozenVariableDict,\n        critic_params: FrozenVariableDict,\n    ):\n        collection_state, trajectories = jax.lax.scan(\n            lambda state, step: self.env_step(\n                state, step, actor_params=actor_params, critic_params=critic_params\n            ),\n            collection_state,\n            xs=jnp.arange(self.hp.num_steps),\n            length=self.hp.num_steps,\n        )\n        trajectories_with_last = TrajectoryWithLastObs(\n            trajectories=trajectories,\n            last_done=collection_state.last_done,\n            last_obs=collection_state.last_obs,\n        )\n        return collection_state, trajectories_with_last\n\n    # @jit\n    def env_step(\n        self,\n        collection_state: TrajectoryCollectionState[TEnvState],\n        step_index: jax.Array,\n        actor_params: FrozenVariableDict,\n        critic_params: FrozenVariableDict,\n    ):\n        # Get keys for sampling action and stepping environment\n        # doing it this way to try to get *exactly* the same rngs as in rejax.PPO.\n        rng, new_rngs = jax.random.split(collection_state.rng, 2)\n        rng_steps, rng_action = jax.random.split(new_rngs, 2)\n        rng_steps = jax.random.split(rng_steps, self.hp.num_envs)\n\n        # Sample action\n        unclipped_action, log_prob = self.actor.apply(\n            actor_params, collection_state.last_obs, rng_action, method=\"action_log_prob\"\n        )\n        assert isinstance(log_prob, jax.Array)\n        value = self.critic.apply(critic_params, collection_state.last_obs)\n        assert isinstance(value, jax.Array)\n\n        # Clip action\n        if self.discrete:\n            action = unclipped_action\n        else:\n            low = self.env.action_space(self.env_params).low\n            high = self.env.action_space(self.env_params).high\n            action = jnp.clip(unclipped_action, low, high)\n\n        # Step environment\n        next_obs, env_state, reward, done, _ = jax.vmap(self.env.step, in_axes=(0, 0, 0, None))(\n            rng_steps,\n            collection_state.env_state,\n            action,\n            self.env_params,\n        )\n\n        if self.hp.normalize_observations:\n            # rms_state, next_obs = learner.update_and_normalize(collection_state.rms_state, next_obs)\n            rms_state = _update_rms(collection_state.rms_state, obs=next_obs, batched=True)\n            next_obs = _normalize_obs(rms_state, obs=next_obs)\n\n            collection_state = collection_state.replace(rms_state=rms_state)\n\n        # Return updated runner state and transition\n        transition = Trajectory(\n            collection_state.last_obs, unclipped_action, log_prob, reward, value, done\n        )\n        collection_state = collection_state.replace(\n            env_state=env_state,\n            last_obs=next_obs,\n            last_done=done,\n            global_step=collection_state.global_step + self.hp.num_envs,\n            rng=rng,\n        )\n        return collection_state, transition\n\n    @property\n    def discrete(self) -&gt; bool:\n        return isinstance(\n            self.env.action_space(self.env_params), gymnax.environments.spaces.Discrete\n        )\n\n    def visualize(self, ts: PPOState, gif_path: str | Path, eval_rng: chex.PRNGKey | None = None):\n        actor = make_actor(ts=ts, hp=self.hp)\n        render_episode(\n            actor=actor,\n            env=self.env,\n            env_params=jax.tree.map(\n                lambda v: v if isinstance(v, int | float) else v.item() if v.ndim == 0 else v,\n                self.env_params,\n            ),\n            gif_path=Path(gif_path),\n            rng=eval_rng if eval_rng is not None else ts.rng,\n        )\n\n    ## These here aren't currently used. They are here to mirror rejax.PPO where the training loop\n    # is in the algorithm.\n\n    @functools.partial(jax.jit, static_argnames=\"skip_initial_evaluation\")\n    def train(\n        self,\n        rng: jax.Array,\n        train_state: PPOState[TEnvState] | None = None,\n        skip_initial_evaluation: Static[bool] = False,\n    ) -&gt; tuple[PPOState[TEnvState], EvalMetrics]:\n        \"\"\"Full training loop in jax.\n\n        This is only here to match the API of `rejax.PPO.train`. This doesn't get called when using\n        the `JaxTrainer`, since `JaxTrainer.fit` already does the same thing, but also with support\n        for some `JaxCallback`s (as well as some `lightning.Callback`s!).\n\n        Unfolded version of `rejax.PPO.train`.\n        \"\"\"\n        if train_state is None and rng is None:\n            raise ValueError(\"Either train_state or rng must be provided\")\n\n        ts = train_state if train_state is not None else self.init_train_state(rng)\n\n        initial_evaluation: EvalMetrics | None = None\n        if not skip_initial_evaluation:\n            initial_evaluation = self.eval_callback(ts)\n\n        num_evals = np.ceil(self.hp.total_timesteps / self.hp.eval_freq).astype(int)\n        ts, evaluation = jax.lax.scan(\n            self._training_epoch,\n            init=ts,\n            xs=None,\n            length=num_evals,\n        )\n\n        if not skip_initial_evaluation:\n            assert initial_evaluation is not None\n            evaluation = jax.tree.map(\n                lambda i, ev: jnp.concatenate((jnp.expand_dims(i, 0), ev)),\n                initial_evaluation,\n                evaluation,\n            )\n            assert isinstance(evaluation, EvalMetrics)\n\n        return ts, evaluation\n\n    def _training_epoch(\n        self, ts: PPOState[TEnvState], epoch: int\n    ) -&gt; tuple[PPOState[TEnvState], EvalMetrics]:\n        # Run a few training iterations\n        iteration_steps = self.hp.num_envs * self.hp.num_steps\n        num_iterations = np.ceil(self.hp.eval_freq / iteration_steps).astype(int)\n        ts = jax.lax.fori_loop(\n            0,\n            num_iterations,\n            # drop metrics for now\n            lambda i, train_state_i: self._fused_training_step(i, train_state_i)[0],\n            ts,\n        )\n        # Run evaluation\n        return ts, self.eval_callback(ts)\n\n    def _fused_training_step(self, iteration: int, ts: PPOState[TEnvState]):\n        \"\"\"Fused training step in jax (joined data collection + training).\n\n        This is the equivalent of the training step from rejax.PPO. It is only used in tests to\n        verify the correctness of the training step.\n        \"\"\"\n\n        data_collection_state, trajectories = self.collect_trajectories(\n            collection_state=ts.data_collection_state,\n            actor_params=ts.actor_ts.params,\n            critic_params=ts.critic_ts.params,\n        )\n        ts = ts.replace(data_collection_state=data_collection_state)\n        return self.training_step(iteration, ts, trajectories)\n</code></pre>"},{"location":"examples/jax_rl/#jaxmodule","title":"JaxModule","text":"<p>The <code>JaxModule</code> class is made to look a bit like the <code>lightning.LightningModule</code> class:</p> <pre><code>@runtime_checkable\nclass JaxModule(Protocol[Ts, _B, _MetricsT]):\n    \"\"\"A protocol for algorithms that can be trained by the `JaxTrainer`.\n\n    The `JaxRLExample` is an example that follows this structure and can be trained with a\n    `JaxTrainer`.\n    \"\"\"\n\n    def init_train_state(self, rng: chex.PRNGKey) -&gt; Ts:\n        \"\"\"Create the initial training state.\"\"\"\n        raise NotImplementedError\n\n    def get_batch(self, ts: Ts, batch_idx: int) -&gt; tuple[Ts, _B]:\n        \"\"\"Produces a batch of data.\"\"\"\n        raise NotImplementedError\n\n    def training_step(\n        self, batch_idx: int, ts: Ts, batch: _B\n    ) -&gt; tuple[Ts, flax.struct.PyTreeNode]:\n        \"\"\"Update the training state using a \"batch\" of data.\"\"\"\n        raise NotImplementedError\n\n    def eval_callback(self, ts: Ts) -&gt; _MetricsT:\n        \"\"\"Perform evaluation and return metrics.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"examples/jax_rl/#jaxtrainer","title":"JaxTrainer","text":"<p>The <code>JaxTrainer</code> follows a roughly similar structure as the <code>lightning.Trainer</code>: - <code>JaxTrainer.fit</code> is called with a <code>JaxModule</code> to train the algorithm.</p> Click to show the code for JaxTrainer <pre><code>class JaxTrainer(flax.struct.PyTreeNode):\n    \"\"\"A simplified version of the `lightning.Trainer` with a fully jitted training loop.\n\n    ## Assumptions:\n\n    - The algo object must match the `JaxModule` protocol (in other words, it should implement its\n      methods).\n\n    ## Training loop\n\n    This is the training loop, which is fully jitted:\n\n    ```python\n    ts = algo.init_train_state(rng)\n\n    setup(\"fit\")\n    on_fit_start()\n    on_train_start()\n\n    eval_metrics = []\n    for epoch in range(self.max_epochs):\n        on_train_epoch_start()\n\n        for step in range(self.training_steps_per_epoch):\n\n            batch = algo.get_batch(ts, step)\n\n            on_train_batch_start()\n\n            ts, metrics = algo.training_step(step, ts, batch)\n\n            on_train_batch_end()\n\n        on_train_epoch_end()\n\n        # Evaluation \"loop\"\n        on_validation_epoch_start()\n        epoch_eval_metrics = self.eval_epoch(ts, epoch, algo)\n        on_validation_epoch_start()\n\n        eval_metrics.append(epoch_eval_metrics)\n\n    return ts, eval_metrics\n    ```\n\n    ## Caveats\n\n    - Some lightning callbacks can be used with this trainer and work well, but not all of them.\n    - You can either use Regular pytorch-lightning callbacks, or use `jax.vmap` on the `fit` method,\n      but not both.\n      - If you want to use [jax.vmap][] on the `fit` method, just remove the callbacks on the\n        Trainer for now.\n\n    ## TODOs / ideas\n\n    - Add a checkpoint callback with orbax-checkpoint?\n    \"\"\"\n\n    max_epochs: int = flax.struct.field(pytree_node=False)\n\n    training_steps_per_epoch: int = flax.struct.field(pytree_node=False)\n\n    limit_val_batches: int = 0\n    limit_test_batches: int = 0\n\n    # TODO: Getting some errors with the schema generation for lightning.Callback and\n    # lightning.pytorch.loggers.logger.Logger here if we keep the type annotation.\n    callbacks: Sequence = dataclasses.field(metadata={\"pytree_node\": False}, default_factory=tuple)\n\n    logger: Any | None = flax.struct.field(pytree_node=False, default=None)\n\n    # accelerator: str = flax.struct.field(pytree_node=False, default=\"auto\")\n    # strategy: str = flax.struct.field(pytree_node=False, default=\"auto\")\n    # devices: int | str = flax.struct.field(pytree_node=False, default=\"auto\")\n\n    # path to output directory, created dynamically by hydra\n    # path generation pattern is specified in `configs/hydra/default.yaml`\n    # use it to store all files generated during the run, like checkpoints and metrics\n\n    default_root_dir: str | Path | None = flax.struct.field(\n        pytree_node=False,\n        default_factory=lambda: HydraConfig.get().runtime.output_dir,\n    )\n\n    # State variables:\n    # TODO: figure out how to cleanly store / update these.\n    current_epoch: int = flax.struct.field(pytree_node=True, default=0)\n    global_step: int = flax.struct.field(pytree_node=True, default=0)\n\n    logged_metrics: dict = flax.struct.field(pytree_node=True, default_factory=dict)\n    callback_metrics: dict = flax.struct.field(pytree_node=True, default_factory=dict)\n    # todo: get the metrics from the callbacks?\n    # lightning.pytorch.loggers.CSVLogger.log_metrics\n    # TODO: Take a look at this method:\n    # lightning.pytorch.callbacks.progress.rich_progress.RichProgressBar.get_metrics\n    # return lightning.Trainer._logger_connector.progress_bar_metrics\n    progress_bar_metrics: dict = flax.struct.field(pytree_node=True, default_factory=dict)\n\n    verbose: bool = flax.struct.field(pytree_node=False, default=False)\n\n    @functools.partial(\n        jax.jit,\n        static_argnames=\"skip_initial_evaluation\",\n    )\n    def fit(\n        self,\n        algo: JaxModule[Ts, _B, _MetricsT],\n        rng: chex.PRNGKey,\n        train_state: Ts | None = None,\n        skip_initial_evaluation: Static[bool] = False,\n    ) -&gt; tuple[Ts, _MetricsT]:\n        \"\"\"Full training loop in pure jax (a lot faster than pytorch-lightning).\n\n        Some of the lightning Callbacks can be used with this Trainer.\n        \"\"\"\n        # TODO: The WandbLogger with Lightning doesn't currently print the things that it usually\n        # would with Lightning like the run URL and such at the start and end of the run.\n        if train_state is None and rng is None:\n            raise ValueError(\"Either train_state or rng must be provided\")\n\n        train_state = train_state if train_state is not None else algo.init_train_state(rng)\n\n        if self.progress_bar_callback is not None:\n            if self.verbose:\n                jax.debug.print(\"Enabling the progress bar callback.\")\n            jax.experimental.io_callback(self.progress_bar_callback.enable, ())\n\n        self._callback_hook(\"setup\", self, algo, ts=train_state, partial_kwargs=dict(stage=\"fit\"))\n        self._callback_hook(\"on_fit_start\", self, algo, ts=train_state)\n        self._callback_hook(\"on_train_start\", self, algo, ts=train_state)\n\n        for logger in self.loggers:\n            jax.experimental.io_callback(\n                lambda algo: logger.log_hyperparams(hparams_to_dict(algo)),\n                (),\n                algo,\n                ordered=True,\n            )\n\n        initial_evaluation: _MetricsT | None = None\n        if not skip_initial_evaluation:\n            initial_evaluation = algo.eval_callback(train_state)\n\n        # Run the epoch loop `self.max_epoch` times.\n        train_state, evaluations = jax.lax.scan(\n            lambda state, epoch: self.epoch_loop(state, epoch, algo=algo),\n            init=train_state,\n            xs=jnp.arange(self.max_epochs),  # type: ignore\n            length=self.max_epochs,\n        )\n\n        if not skip_initial_evaluation:\n            assert initial_evaluation is not None\n            evaluations: _MetricsT = jax.tree.map(\n                lambda i, ev: jnp.concatenate((jnp.expand_dims(i, 0), ev)),\n                initial_evaluation,\n                evaluations,\n            )\n\n        (train_state, evaluations) = jax.block_until_ready((train_state, evaluations))\n\n        self._callback_hook(\"on_fit_end\", self, algo, ts=train_state)\n        self._callback_hook(\"on_train_end\", self, algo, ts=train_state)\n        self._callback_hook(\n            \"teardown\", self, algo, ts=train_state, partial_kwargs={\"stage\": \"fit\"}\n        )\n        for logger in self.loggers:\n            jax.experimental.io_callback(\n                functools.partial(logger.finalize, status=\"success\"),\n                (),\n            )\n        # jax.debug.print(\"Evaluations: {}\", evaluations)\n\n        return train_state, evaluations\n\n    # @jit\n    def epoch_loop(self, ts: Ts, epoch: jax.Array, algo: JaxModule[Ts, _B, _MetricsT]):\n        # todo: Some lightning callbacks try to get the \"trainer.current_epoch\".\n        # FIXME: Hacky: Present a trainer with a different value of `self.current_epoch` to\n        # the callbacks.\n        # chex.assert_scalar_in(epoch, 0, self.max_epochs)\n        # TODO: Can't just set current_epoch to `epoch` as `epoch` is a Traced value.\n        # todo: need to have the callback take in the actual int value.\n        # jax.debug.print(\"Starting epoch {epoch}\", epoch=epoch)\n\n        self = self.replace(current_epoch=epoch)  # doesn't quite work?\n        ts = self.training_epoch(ts=ts, epoch=epoch, algo=algo)\n        eval_metrics = self.eval_epoch(ts=ts, epoch=epoch, algo=algo)\n        return ts, eval_metrics\n\n    # @jit\n    def training_epoch(self, ts: Ts, epoch: int, algo: JaxModule[Ts, _B, _MetricsT]):\n        # Run a few training iterations\n        self._callback_hook(\"on_train_epoch_start\", self, algo, ts=ts)\n\n        ts = jax.lax.fori_loop(\n            0,\n            self.training_steps_per_epoch,\n            # drop training metrics for now.\n            lambda i, state: self.training_step(i, state, algo=algo),\n            ts,\n        )\n\n        self._callback_hook(\"on_train_epoch_end\", self, algo, ts=ts)\n        return ts\n\n    # @jit\n    def eval_epoch(self, ts: Ts, epoch: int, algo: JaxModule[Ts, _B, _MetricsT]):\n        self._callback_hook(\"on_validation_epoch_start\", self, algo, ts=ts)\n\n        # todo: split up into eval batch and eval step?\n        eval_metrics = algo.eval_callback(ts=ts)\n\n        self._callback_hook(\"on_validation_epoch_end\", self, algo, ts=ts)\n\n        return eval_metrics\n\n    # @jit\n    def training_step(self, batch_idx: int, ts: Ts, algo: JaxModule[Ts, _B, _MetricsT]):\n        \"\"\"Training step in pure jax (joined data collection + training).\n\n        *MUCH* faster than using pytorch-lightning, but you lose the callbacks and such.\n        \"\"\"\n        # todo: rename to `get_training_batch`?\n        ts, batch = algo.get_batch(ts, batch_idx=batch_idx)\n\n        self._callback_hook(\"on_train_batch_start\", self, algo, batch, batch_idx, ts=ts)\n\n        ts, metrics = algo.training_step(batch_idx=batch_idx, ts=ts, batch=batch)\n\n        for logger in self.loggers:\n            jax.experimental.io_callback(\n                lambda metrics, batch_index: logger.log_metrics(\n                    jax.tree.map(lambda v: v.mean(), metrics), batch_index\n                ),\n                (),\n                dataclasses.asdict(metrics) if dataclasses.is_dataclass(metrics) else metrics,\n                batch_idx,\n            )\n\n        self._callback_hook(\"on_train_batch_end\", self, algo, metrics, batch, batch_idx, ts=ts)\n\n        return ts\n\n    ### Hooks to mimic those of lightning.Trainer\n\n    def _callback_hook(\n        self,\n        hook_name: str,\n        /,\n        *hook_args,\n        ts: Ts,\n        partial_kwargs: dict | None = None,\n        sharding: jax.sharding.SingleDeviceSharding | None = None,\n        ordered: bool = True,\n        **hook_kwargs,\n    ):\n        \"\"\"Call a hook on all callbacks.\"\"\"\n        # with jax.disable_jit():\n        for i, callback in enumerate(self.callbacks):\n            # assert hasattr(callback, hook_name)\n\n            method = getattr(callback, hook_name)\n            if partial_kwargs:\n                method = functools.partial(method, **partial_kwargs)\n            if self.verbose:\n                jax.debug.print(\n                    \"Epoch {current_epoch}/{max_epochs}: \"\n                    + f\"Calling hook {hook_name} on callback {callback}\"\n                    + \"{i}\",\n                    i=i,\n                    current_epoch=self.current_epoch,\n                    ordered=True,\n                    max_epochs=self.max_epochs,\n                )\n            jax.experimental.io_callback(\n                method,\n                (),\n                *hook_args,\n                **({\"ts\": ts} if isinstance(callback, JaxCallback) else {}),\n                **hook_kwargs,\n                sharding=sharding,\n                ordered=ordered if not isinstance(callback, JaxCallback) else False,\n            )\n\n    # Compat for RichProgressBar\n    @property\n    def is_global_zero(self) -&gt; bool:\n        \"\"\"Check if the current process is the global zero process in a distributed setup.\"\"\"\n        return jax.process_index() == 0\n\n    @property\n    def num_training_batches(self) -&gt; int:\n        return self.training_steps_per_epoch\n\n    @property\n    def loggers(self) -&gt; list[lightning.pytorch.loggers.Logger]:\n        if isinstance(self.logger, list | tuple):\n            return list(self.logger)\n        if self.logger is not None:\n            return [self.logger]\n        return []\n\n    # @property\n    # def progress_bar_metrics(self) -&gt; dict[str, float]:\n\n    #     return {}\n\n    @property\n    def progress_bar_callback(self) -&gt; lightning.pytorch.callbacks.ProgressBar | None:\n        for c in self.callbacks:\n            if isinstance(c, lightning.pytorch.callbacks.ProgressBar):\n                return c\n        return None\n\n    @property\n    def state(self):\n        from lightning.pytorch.trainer.states import (\n            RunningStage,\n            TrainerFn,\n            TrainerState,\n            TrainerStatus,\n        )\n\n        return TrainerState(\n            fn=TrainerFn.FITTING,\n            status=TrainerStatus.RUNNING,\n            stage=RunningStage.TRAINING,\n        )\n        #     self._trainer.state.fn != \"fit\"\n        #     or self._trainer.sanity_checking\n        #     or self._trainer.progress_bar_callback.train_progress_bar_id != task.id\n        # ):\n\n    @property\n    def sanity_checking(self) -&gt; bool:\n        from lightning.pytorch.trainer.states import RunningStage\n\n        return self.state.stage == RunningStage.SANITY_CHECKING\n\n    @property\n    def training(self) -&gt; bool:\n        from lightning.pytorch.trainer.states import RunningStage\n\n        return self.state.stage == RunningStage.TRAINING\n\n    @property\n    def log_dir(self) -&gt; Path | None:\n        # copied from lightning.Trainer\n        if len(self.loggers) &gt; 0:\n            if not isinstance(\n                self.loggers[0],\n                lightning.pytorch.loggers.TensorBoardLogger | lightning.pytorch.loggers.CSVLogger,\n            ):\n                dirpath = self.loggers[0].save_dir\n            else:\n                dirpath = self.loggers[0].log_dir\n        else:\n            dirpath = self.default_root_dir\n        if dirpath:\n            return Path(dirpath)\n        return None\n</code></pre>"},{"location":"examples/llm_finetuning/","title":"Fine-tuning an LLM (\ud83e\udd17+\u26a1)","text":""},{"location":"examples/llm_finetuning/#fine-tuning-llms","title":"Fine-tuning LLMs","text":"<p>This example is based on this language modeling example from the HuggingFace transformers documentation.</p> <p>To better understand what's going on in this example, it is a good idea to read through these tutorials first:</p> <ul> <li>Causal language modeling simple example - HuggingFace docs</li> <li>Fine-tune a language model - Colab Notebook</li> </ul> <p>The main difference between this example and the original example from HuggingFace is that the <code>LLMFinetuningExample</code> is a <code>LightningModule</code>, that is trained by a <code>lightning.Trainer</code>.</p> <p>This also means that this example doesn't use <code>accelerate</code> or the HuggingFace Trainer.</p>"},{"location":"examples/llm_finetuning/#running-the-example","title":"Running the example","text":"<pre><code>python project/main.py experiment=llm_finetuning_example\n</code></pre>"},{"location":"examples/profiling/","title":"Profiling your code\ud83d\udcce","text":""},{"location":"examples/sweeps/","title":"Running sweeps","text":""},{"location":"examples/sweeps/#hyper-parameter-optimization","title":"Hyper-Parameter Optimization","text":"<p>Work-in-progress!</p> <p>Please note that this is very much a work in progress!</p> <p>This is a small example Hydra and submitit make it very easy to launch lots of jobs on SLURM clusters.</p> <p>hyper-parameter optimization (HPO)</p>"},{"location":"examples/sweeps/#hyper-parameter-optimization-with-the-orion-hydra-sweeper","title":"Hyper-Parameter Optimization with the Orion Hydra Sweeper","text":"<p>Here is a configuration file that you can use to launch a hyper-parameter optimization (HPO) sweep</p> Click to show the yaml config file <pre><code># @package _global_\n# NOTE: If you get config 'orion' not found, run `uv add hydra-orion-sweeper`\ndefaults:\n  - example.yaml # A configuration for a single run (that works!)\n  - override /hydra/sweeper: orion # Select the orion sweeper plugin\n\nlog_level: DEBUG\nname: \"local-sweep-example\"\nseed: 123\n\nalgorithm:\n  optimizer:\n    # This here will get overwritten by the sweeper.\n    lr: 0.002\n\ntrainer:\n  accelerator: auto\n  devices: 1\n  max_epochs: 1\n  logger:\n    wandb:\n      _target_: lightning.pytorch.loggers.wandb.WandbLogger\n      project: \"ResearchTemplate\"\n      # TODO: Use the Orion trial name?\n      # name: ${oc.env:SLURM_JOB_ID}_${oc.env:SLURM_ARRAY_TASK_ID,0}_${oc.env:SLURM_PROCID}\n      save_dir: \"${hydra:runtime.output_dir}\"\n      offline: False # set True to store all logs only locally\n      # id: ${oc.env:SLURM_JOB_ID}_${oc.env:SLURM_ARRAY_TASK_ID,0}_${oc.env:SLURM_PROCID} # pass correct id to resume experiment!\n      # entity: \"\"  # set to name of your wandb team\n      log_model: False\n      prefix: \"\"\n      job_type: \"train\"\n      group: [\"${name}\"]\n      tags: [\"${name}\"]\n\nhydra:\n  mode: MULTIRUN\n  run:\n    # output directory, generated dynamically on each run\n    dir: logs/${name}/runs\n  sweep:\n    dir: logs/${name}/multiruns/\n    # subdir: ${hydra.job.num}\n    subdir: ${hydra.job.id}/task${hydra.job.num}\n\n  sweeper:\n    params:\n      algorithm:\n        optimizer:\n          lr: \"loguniform(1e-6, 1.0, default_value=3e-4)\"\n          # weight_decay: \"loguniform(1e-6, 1e-2, default_value=0)\"\n\n    experiment:\n      name: \"${name}\"\n      version: 1\n\n    algorithm:\n      type: tpe\n      config:\n        seed: 1\n\n    worker:\n      n_workers: 1\n      max_broken: 10000\n      max_trials: 10\n\n    storage:\n      type: legacy\n      use_hydra_path: false\n      database:\n        type: pickleddb\n        host: \"logs/${name}/multiruns/database.pkl\"\n    parametrization: null\n</code></pre> <p>You can use it like so:</p> <pre><code>python project/main.py experiment=local_sweep_example\n</code></pre>"},{"location":"examples/sweeps/#hyper-parameter-optimization-on-a-slurm-cluster","title":"Hyper-Parameter Optimization on a SLURM cluster","text":"Click to show the yaml config file <pre><code># @package _global_\n\n# NOTE: If you get config 'orion' not found, run `uv add hydra-orion-sweeper`\n\n# This is an \"experiment\" config, that groups together other configs into a ready-to-run example.\n\ndefaults:\n  - example.yaml # A configuration for a single run (that works!)\n  - override /trainer/logger: wandb\n  - override /hydra/sweeper: orion\n  - override /resources: gpu\n  - override /cluster: ??? # use `current` if you are already on a cluster, otherwise use one of the `cluster` configs.\n\nlog_level: DEBUG\nname: \"sweep-example\"\n\n# Set the seed to be the SLURM_PROCID, so that if we run more than one task per GPU, we get\n# TODO: This should technically be something like the \"run_id\", which would be different than SLURM_PROCID when using &gt;1 gpus per \"run\".\nseed: ${oc.env:SLURM_PROCID,123}\n\nalgorithm:\n  optimizer:\n    # This here will get overwritten by the sweeper.\n    lr: 0.002\n\ntrainer:\n  accelerator: gpu\n  devices: 1\n  max_epochs: 1\n  logger:\n    wandb:\n      project: \"ResearchTemplate\"\n      # TODO: Use the Orion trial name?\n      name: ${oc.env:SLURM_JOB_ID}_${oc.env:SLURM_ARRAY_TASK_ID,0}_${oc.env:SLURM_PROCID}\n      save_dir: \"${hydra:runtime.output_dir}\"\n      offline: False # set True to store all logs only locally\n      id: ${oc.env:SLURM_JOB_ID}_${oc.env:SLURM_ARRAY_TASK_ID,0}_${oc.env:SLURM_PROCID} # pass correct id to resume experiment!\n      # entity: \"\"  # set to name of your wandb team\n      log_model: False\n      prefix: \"\"\n      job_type: \"train\"\n      group: ${oc.env:SLURM_JOB_ID}\n      # tags: [\"${name}\"]\n\nhydra:\n  mode: MULTIRUN\n  # TODO: Make it so running the same command twice in the same job id resumes from the last checkpoint.\n  run:\n    # output directory, generated dynamically on each run\n    dir: logs/${name}/runs\n  sweep:\n    dir: logs/${name}/multiruns/\n    # subdir: ${hydra.job.num}\n    subdir: ${hydra.job.id}/task${oc.env:SLURM_PROCID,0}\n\n  launcher:\n    # todo: bump this up.\n    array_parallelism: 5 # max num of jobs to run in parallel\n    additional_parameters:\n      time: 0-00:10:00 # maximum wall time allocated for the job (D-HH:MM:SS)\n      # TODO: Pack more than one job on a single GPU, and support this with both a\n      # patched submitit launcher as well as our remote submitit launcher, as well as by patching the\n      # orion sweeper to not drop these other results.\n      # ntasks_per_gpu: 1\n  sweeper:\n    params:\n      algorithm:\n        optimizer:\n          lr: \"loguniform(1e-6, 1.0, default_value=3e-4)\"\n          # weight_decay: \"loguniform(1e-6, 1e-2, default_value=0)\"\n      # todo: setup a fidelity parameter. Seems to not be working right now.\n      # trainer:\n      #   # Let the HPO algorithm allocate more epochs to more promising HP configurations.\n      #   max_epochs: \"fidelity(1, 10, default_value=1)\"\n\n    parametrization: null\n    experiment:\n      name: \"${name}\"\n      version: 1\n\n    algorithm:\n      #  BUG: Getting a weird bug with TPE: KeyError in `dum_below_trials = [...]` at line 397.\n      type: tpe\n      config:\n        seed: 1\n\n    worker:\n      n_workers: ${hydra.launcher.array_parallelism}\n      max_broken: 10000\n      max_trials: 10\n\n    storage:\n      type: legacy\n      use_hydra_path: false\n      database:\n        type: pickleddb\n        host: \"logs/${name}/multiruns/database.pkl\"\n</code></pre> <p>Here's how you can easily launch a sweep remotely on the Mila cluster. If you are already on a slurm cluster, use the <code>\"cluster=current\"</code> config.</p> <pre><code>python project/main.py experiment=cluster_sweep_example cluster=mila\n</code></pre>"},{"location":"examples/text_classification/","title":"Text Classification (\ud83e\udd17+\u26a1)","text":""},{"location":"examples/text_classification/#text-classification","title":"Text Classification (\u26a1 + \ud83e\udd17)","text":""},{"location":"examples/text_classification/#overview","title":"Overview","text":"<p>The <code>TextClassifier</code> is a LightningModule for a simple text classification task.</p> <p>It accepts a <code>TextClassificationDataModule</code> as input, along with a network.</p> Click to show the code of the lightningmodule <pre><code>class TextClassifier(LightningModule):\n    \"\"\"Example of a lightning module used to train a huggingface model for text classification.\"\"\"\n\n    def __init__(\n        self,\n        datamodule: TextClassificationDataModule,\n        network: HydraConfigFor[PreTrainedModel],\n        hf_metric_name: str,\n        learning_rate: float = 2e-5,\n        adam_epsilon: float = 1e-8,\n        warmup_steps: int = 0,\n        weight_decay: float = 0.0,\n        init_seed: int = 42,\n    ):\n        super().__init__()\n        self.datamodule = datamodule\n        self.network_config = network\n        self.num_labels = datamodule.num_classes\n        self.task_name = datamodule.task_name\n        self.init_seed = init_seed\n        self.hf_metric_name = hf_metric_name\n        self.learning_rate = learning_rate\n        self.adam_epsilon = adam_epsilon\n        self.warmup_steps = warmup_steps\n        self.weight_decay = weight_decay\n\n        self.metric = evaluate.load(\n            self.hf_metric_name,\n            self.task_name,\n            # todo: replace with hydra job id perhaps?\n            experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"),\n        )\n\n        self.save_hyperparameters(ignore=[\"datamodule\"])\n\n    def configure_model(self) -&gt; None:\n        if self.network is not None:\n            logger.info(\"Network is already instantiated.\")\n            return\n        with torch.random.fork_rng(devices=[self.device]):\n            # deterministic weight initialization\n            torch.manual_seed(self.init_seed)\n            self.network = hydra_zen.instantiate(self.network_config)\n\n        return super().configure_model()\n\n    def forward(self, inputs: dict[str, torch.Tensor]) -&gt; BaseModelOutput:\n        return self.network(**inputs)\n\n    def shared_step(self, batch: dict[str, torch.Tensor], batch_idx: int, stage: str):\n        outputs: CausalLMOutput | SequenceClassifierOutput = self(batch)\n        loss = outputs.loss\n        assert isinstance(loss, torch.Tensor), loss\n        # todo: log the output of the metric.\n        self.log(f\"{stage}/loss\", loss, prog_bar=True)\n        if isinstance(outputs, SequenceClassifierOutput):\n            metric_value = self.metric.compute(\n                # logits=outputs.logits,\n                predictions=outputs.logits.argmax(-1),\n                references=batch[\"labels\"],\n            )\n            assert isinstance(metric_value, dict)\n            for k, v in metric_value.items():\n                self.log(\n                    f\"{stage}/{k}\",\n                    v,\n                    prog_bar=True,\n                )\n        return loss\n\n    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int):\n        return self.shared_step(batch, batch_idx, \"train\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int, dataloader_idx: int = 0\n    ):\n        return self.shared_step(batch, batch_idx, \"val\")\n\n    def configure_optimizers(self):\n        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n        model = self.network\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if not any(nd_param in n for nd_param in no_decay)\n                ],\n                \"weight_decay\": self.weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if any(nd_param in n for nd_param in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.learning_rate,\n            eps=self.adam_epsilon,\n        )\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=self.warmup_steps,\n            num_training_steps=self.trainer.estimated_stepping_batches,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n</code></pre>"},{"location":"examples/text_classification/#config-files","title":"Config files","text":""},{"location":"examples/text_classification/#algorithm-config","title":"Algorithm config","text":"Click to show the Algorithm config <p>Source: project/configs/algorithm/text_classifier.yaml</p> <pre><code># Config for the Text classification example algorithm\n_target_: project.algorithms.text_classifier.TextClassifier\n_recursive_: false\nnetwork:\n  _target_: transformers.models.auto.modeling_auto.AutoModelForSequenceClassification.from_pretrained\n  pretrained_model_name_or_path: albert-base-v2\n\n# NOTE: Why _partial_? Because the config doesn't create the algo directly, it creates a function\n# that will accept the datamodule and network and return the algo.\n_partial_: true\nhf_metric_name: glue\n</code></pre>"},{"location":"examples/text_classification/#datamodule-config","title":"Datamodule config","text":"Click to show the Datamodule config <p>Source: project/configs/datamodule/glue_cola.yaml</p> <pre><code>_target_: project.datamodules.text.text_classification.TextClassificationDataModule\ndata_dir: ${oc.env:SCRATCH,.}/data\nhf_dataset_path: glue\ntask_name: cola\ntext_fields:\n  - \"sentence\"\ntokenizer:\n  _target_: transformers.models.auto.tokenization_auto.AutoTokenizer.from_pretrained\n  use_fast: true\n  # Note: We could interpolate this value with `${/algorithm/network/pretrained_model_name_or_path}`\n  # to avoid duplicating a value, but this also makes it harder to use this by itself or with\n  # another algorithm.\n  pretrained_model_name_or_path: albert-base-v2\n  cache_dir: ${..data_dir}\n  trust_remote_code: true\nnum_classes: 2\nmax_seq_length: 128\ntrain_batch_size: 32\neval_batch_size: 32\n</code></pre>"},{"location":"examples/text_classification/#running-the-example","title":"Running the example","text":"<p>Here is a configuration file that you can use to launch a simple experiment:</p> Click to show the yaml config file <p>Source: project/configs/experiment/text_classification_example.yaml</p> <pre><code># @package _global_\ndefaults:\n  - override /algorithm: text_classifier\n  - override /datamodule: glue_cola\n  - override /trainer/callbacks: none\n\ntrainer:\n  min_epochs: 1\n  max_epochs: 2\n  limit_train_batches: 2\n  limit_val_batches: 1\n  num_sanity_val_steps: 0\n  enable_checkpointing: False\n</code></pre> <p>You can use it like so:</p> <pre><code>python project/main.py experiment=text_classification_example\n</code></pre>"},{"location":"features/","title":"Features \ud83d\udd25","text":""},{"location":"features/#features-unique-to-this-project-template","title":"Features unique to this project template","text":"<p>Here are some cool features that are unique to this particular template:</p> <ul> <li>Support for both Jax and Torch with PyTorch-Lightning (See the Jax example)</li> <li>Your Hydra configs will have an Auto-Generated YAML schemas \ud83d\udd25</li> <li>A comprehensive suite of automated tests for new algorithms, datasets and networks<ul> <li>\ud83e\udd16 Thoroughly tested on the Mila directly with GitHub CI</li> <li>Automated testing on the DRAC clusters will also be added soon.</li> </ul> </li> <li>Easy development inside a [devcontainer with VsCode]</li> <li>Tailor-made for ML researchers that run their jobs on SLURM clusters (with default configurations for the Mila and DRAC clusters.)</li> <li>Rich typing of all parts of the source code</li> </ul> <p>This template is aimed for ML researchers that run their jobs on SLURM clusters. The target audience is researchers and students at Mila. This template should still be useful for others outside of Mila that use PyTorch-Lightning and Hydra.</p>"},{"location":"features/auto_schema/","title":"Magic Config Schemas","text":""},{"location":"features/auto_schema/#auto-schema-for-hydra-configs","title":"Auto Schema for Hydra Configs","text":"<p>\ud83d\udd25 NOTE: This is a feature that is entirely unique to this template! \ud83d\udd25</p> <p>This project template comes with a really neat feature: Your Hydra config files automatically get a Schema associated with them.</p> <p>This greatly improves the experience of developing a project with Hydra:</p> <ul> <li>Saves you time by preventing errors caused by unexpected keys in your config files, or values that are of the wrong type     This can often happen after moving files or renaming a function, for example.</li> <li>While writing a config file you get to see:<ul> <li>the list of available configuration options in a given config</li> <li>the default values for each value</li> <li>the documentation for each value (taken from the source code of the function!)</li> </ul> </li> </ul> <p>Here's a quick demo of what this looks like in practice:</p> <p></p> <p>Here we have a config that will be used to configure the <code>lightning.Trainer</code> class, but any config file in the project will also get a schema automatically, even if it doesn't have a <code>\"_target_\"</code> key directly!</p>"},{"location":"features/devcontainer/","title":"Developing inside a container (advanced)","text":""},{"location":"features/devcontainer/#developing-inside-a-container-advanced","title":"Developing inside a container (advanced)","text":"<p>This repo provides a Devcontainer configuration for Visual Studio Code to use a Docker container as a pre-configured development environment. This avoids struggles setting up a development environment and makes them reproducible and consistent.</p> <p>If that sounds useful to you, we recommend you first make yourself familiar with the container tutorials if you want to use them. The devcontainer.json file assumes that you have a GPU locally by default. If not, you can simply comment out the \"--gpus\" flag in the <code>.devcontainer/devcontainer.json</code> file.</p> <ol> <li> <p>Setup Docker on your local machine</p> <p>On an Linux machine where you have root access, you can install Docker using the following commands:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> <p>On Windows or Mac, follow these installation instructions</p> </li> <li> <p>(optional) Install the nvidia-container-toolkit to use your local machine's GPU(s).</p> </li> <li> <p>Install the Dev Containers extension for Visual Studio Code.</p> </li> <li> <p>When opening repository in Visual Studio Code, you should be prompted to reopen the repository in a container:</p> <p></p> <p>Alternatively, you can open the command palette (Ctrl+Shift+P) and select <code>Dev Containers: Rebuild and Reopen in Container</code>.</p> </li> </ol>"},{"location":"features/devcontainer/#launching-container-jobs-on-slurm-clusters","title":"Launching container jobs on SLURM clusters","text":"<p>This part is still a work in progress. In principle, developing inside a devcontainer should make it easier to ship the images to slurm clusters and run them as jobs.</p>"},{"location":"features/jax/","title":"Jax and Torch support with Lightning \u26a1","text":""},{"location":"features/jax/#using-jax-with-pytorch-lightning","title":"Using Jax with PyTorch-Lightning","text":"<p>\ud83d\udd25 NOTE: This is a feature that is entirely unique to this template! \ud83d\udd25</p> <p>This template includes examples that use either Jax, PyTorch, or both! There's a table describing each example here.</p> <p>You can mix and match both Jax and Torch code. For example, you can use Jax for your dataloading, your network, or the learning algorithm, all while still benefiting from the nice stuff that comes from using PyTorch-Lightning.</p> How does this work? <p>Well, we use torch-jax-interop, another package developed here at Mila \ud83d\ude0e, that allows easy interop between torch and jax code. Feel free to take a look at it if you'd like to use it as part of your own project. \ud83d\ude01</p>"},{"location":"features/jax/#using-pytorch-lightning-to-train-a-jax-network","title":"Using PyTorch-Lightning to train a Jax network","text":"<p>If you'd like to use Jax in your network or learning algorithm, while keeping the same style of training loop as usual, you can!</p> <ul> <li>Use Jax for the forward / backward passes, the parameter updates, dataset preprocessing, etc.</li> <li>Leave the training loop / callbacks / logging / checkpointing / etc to Lightning</li> </ul> <p>The lightning.Trainer will not be able to tell that you're using Jax!</p> <p>Take a look at this image classification example that uses a Jax network.</p>"},{"location":"features/jax/#end-to-end-training-in-jax-the-jaxtrainer","title":"End-to-end training in Jax: the <code>JaxTrainer</code>","text":"<p>The <code>JaxTrainer</code>, used in the Jax RL Example, follows a similar structure as the lightning Trainer. However, instead of training LightningModules, it trains <code>JaxModule</code>s, which are a simplified, jax-based look-alike of <code>lightning.LightningModule</code>s.</p> <p>The \"algorithm\" needs to match the <code>JaxModule</code> protocol: - <code>JaxModule.training_step</code>: train using a batch of data</p>"},{"location":"features/remote_slurm_launcher/","title":"Launching Jobs on Remote Clusters","text":""},{"location":"features/remote_slurm_launcher/#remote-slurm-submitit-launcher","title":"Remote Slurm Submitit Launcher","text":"<p>\ud83d\udd25 NOTE: This is a feature that is entirely unique to this template! \ud83d\udd25</p> <p>This template includes a custom submitit launcher, that can be used to launch jobs on remote slurm clusters. This allows you to develop code locally, and easily ship it to a different cluster. The only prerequisite is that you must have <code>ssh</code> access to the remote cluster.</p> <p>Under the hood, this uses a custom <code>remote-slurm-executor</code> submitit plugin.</p> <p>This feature allows you to launch jobs on remote slurm clusters using two config groups:</p> <ul> <li>The <code>resources</code> config group is used to select the job resources:<ul> <li><code>cpu</code>: CPU job</li> <li><code>gpu</code>: GPU job</li> </ul> </li> <li>The <code>cluster</code> config group controls where to run the job:<ul> <li><code>current</code>: Run on the current cluster. Use this if you're already on a SLURM cluster (e.g. when using <code>mila code</code>). This uses the usual <code>submitit_slurm</code> launcher.</li> <li><code>mila</code>: Launches the job on the Mila cluster.</li> <li><code>narval</code>: Remotely launches the job on the Narval cluster</li> <li><code>cedar</code>: Remotely launches the job on the Cedar cluster</li> <li><code>beluga</code>: Remotely launches the job on the Beluga cluster</li> </ul> </li> </ul>"},{"location":"features/remote_slurm_launcher/#examples","title":"Examples","text":"<p>This assumes that you've already setup SSH access to the clusters (for example using <code>mila init</code>).</p>"},{"location":"features/remote_slurm_launcher/#local-machine-mila","title":"Local machine -&gt; Mila","text":"<pre><code>python project/main.py experiment=example resources=gpu cluster=mila\n</code></pre>"},{"location":"features/remote_slurm_launcher/#local-machine-drac-narval","title":"Local machine -&gt; DRAC (narval)","text":"<pre><code>python project/main.py experiment=example resources=gpu cluster=narval\n</code></pre>"},{"location":"features/remote_slurm_launcher/#mila-drac-narval","title":"Mila -&gt; DRAC (narval)","text":"<p>This assumes that you've already setup SSH access from <code>mila</code> to the DRAC clusters.</p> <p>Note that command is exactly the same as above.</p> <pre><code>python project/main.py experiment=example resources=gpu cluster=narval\n</code></pre> <p>Warning</p> <p>If you want to launch jobs on a remote cluster, it is (currently) necessary to place the \"resources\" config before the \"cluster\" config on the command-line.</p>"},{"location":"features/remote_slurm_launcher/#launching-jobs-on-the-current-slurm-cluster","title":"Launching jobs on the current SLURM cluster","text":"<p>If you develop on a SLURM cluster, you can use the <code>cluster=current</code>, or simply omit the <code>cluster</code> config group and only use a config from the <code>resources</code> group.</p> <pre><code>(mila) $ python project/main.py experiment=example resources=gpu cluster=current\n</code></pre>"},{"location":"features/testing/","title":"Thorough automated testing on SLURM clusters","text":""},{"location":"features/testing/#automated-testing","title":"Automated Testing","text":"<p>Tests are a vital part of any good codebase, especially in Machine Learning. They make it easier to explore and try out new ideas, by giving you the security that your codebase works as intended.</p> <p>This template comes with some easy-to-use test suites as well as some pre-configured GitHub Actions workflows to run them:</p> <ul> <li>Unit tests: quick to run and check small functions / modules / classes.</li> <li>Regression tests: check that your code is reproducible and to let     you know if something changed while you were developing your code.</li> <li>integration tests: run your code end-to-end to make sure that all the     individually-tested components work together as expected.</li> <li>GitHub Actions runs all these tests before you merge your code.</li> </ul>"},{"location":"features/testing/#automated-testing-on-slurm-clusters-with-github-ci","title":"Automated testing on SLURM clusters with GitHub CI","text":"<p>\ud83d\udd25 NOTE: This is a feature that is entirely unique to this template! \ud83d\udd25</p> <p>This template runs all the above-mentioned tests on an actual Compute Node of the Mila cluster automatically. Assuming that you have access to the Mila / DRAC or other Slurm clusters, all you need to do is to setup a local self-hosted GitHub runner for your fork of this repository, launch it on your local machine with access to a Slurm cluster, and voila: Your code will now be tested on an ACTUAL slurm cluster whenever you push or update a PR in your project GitHub repository.</p> <p>Detailed instructions on how to set this up in your project will be added soon.</p>"},{"location":"features/testing/#test-suites","title":"Test-suites","text":"<p>Unit testing in this template is done with pytest.</p> <p>To run tests, simply use <code>pytest</code> on the command-line. You may want to add some useful flags like <code>pytest -x -v</code>. See the pytest docs for more info.</p> <p>The built-in tests cover the following:</p> <ul> <li>For each datamodule config, for each data split<ul> <li>test that the first batch is always the same</li> </ul> </li> <li>For each algorithm config, for all compatible network / datamodule config combinations:<ul> <li>initialization is deterministic &amp; reproducibile;</li> <li>forward pass is deterministic &amp; reproducibile;</li> <li>backward pass is deterministic &amp; reproducibile;</li> </ul> </li> </ul> <p>Take a look at project.algorithms.lightning_module_tests to see the included base tests for algorithms.</p> <p>If you use Visual Studio Code, you may want to look into adding the \"test explorer\" tab to your editor. Then, you'll be able to see and debug the tests using the GUI.</p>"},{"location":"features/testing/#unit-tests","title":"Unit tests","text":"<pre><code>pytest -x -v\n</code></pre>"},{"location":"features/testing/#regression-tests","title":"Regression Tests","text":"<p>We use pytest-regressions to test that code changes don't break things.</p> <ul> <li><code>--gen-missing</code>: Use this flag when you might be missing some of the regression files (for example on the first test run).</li> <li><code>--regen-all</code>: Use this when you want to intentionally re-create the regression files. This should hopefully not be used often!</li> </ul>"},{"location":"features/testing/#first-run","title":"First run","text":"<p>On the first run, you might want to run test with the <code>--gen-missing</code> files, like so:</p> <pre><code>pytest --regen-all\n</code></pre>"},{"location":"features/testing/#integration-tests","title":"integration-tests","text":"<p>To run slower integration tests, use the following:</p> <pre><code>pytest -x -v --slow\n</code></pre>"},{"location":"features/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"reference/SUMMARY/","title":"Reference \ud83e\udd13","text":"<ul> <li>project<ul> <li>algorithms<ul> <li>callbacks<ul> <li>classification_metrics</li> <li>samples_per_second</li> </ul> </li> <li>image_classifier</li> <li>image_classifier_test</li> <li>jax_image_classifier</li> <li>jax_image_classifier_test</li> <li>jax_ppo</li> <li>jax_ppo_test</li> <li>lightning_module_tests</li> <li>llm_finetuning</li> <li>llm_finetuning_test</li> <li>networks<ul> <li>fcnet</li> </ul> </li> <li>no_op</li> <li>text_classifier</li> <li>text_classifier_test</li> </ul> </li> <li>configs<ul> <li>config</li> </ul> </li> <li>conftest</li> <li>datamodules<ul> <li>datamodule_tests</li> <li>image_classification<ul> <li>cifar10</li> <li>cifar10_test</li> <li>fashion_mnist</li> <li>fashion_mnist_test</li> <li>image_classification</li> <li>image_classification_test</li> <li>imagenet</li> <li>imagenet_test</li> <li>inaturalist</li> <li>inaturalist_test</li> <li>mnist</li> <li>mnist_test</li> </ul> </li> <li>text<ul> <li>text_classification</li> <li>text_classification_test</li> </ul> </li> <li>vision</li> <li>vision_test</li> </ul> </li> <li>experiment</li> <li>main</li> <li>main_test</li> <li>trainers<ul> <li>jax_trainer</li> </ul> </li> <li>utils<ul> <li>env_vars</li> <li>hydra_utils</li> <li>remote_launcher_plugin</li> <li>remote_launcher_plugin_test</li> <li>testutils</li> <li>typing_utils<ul> <li>protocols</li> </ul> </li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/project/","title":"Project","text":"<p>Root module for this research project.</p>"},{"location":"reference/project/conftest/","title":"Conftest","text":"<p>Fixtures and test utilities.</p> <p>This module contains PyTest fixtures that are used by tests.</p>"},{"location":"reference/project/conftest/#project.conftest--how-this-works","title":"How this works","text":"<p>Our goal here is to make sure that the way we create networks/datasets/algorithms during tests match as closely as possible how they are created normally in a real run. For example, when running <code>python project/main.py algorithm=image_classifier</code>.</p> <p>We achieve this like so: All the components of an experiment are created using fixtures. The first fixtures to be invoked are the ones that feed the command-line arguments given the choice of configs.</p> <p>Then, the <code>dict_config</code> fixture is created, which is the Hydra config that is created from loading the configs with the command-line arguments. This is the same as the input to the <code>main</code> function: an <code>omegaconf.DictConfig</code>.</p> <p>If there are interpolations in the configs, they are resolved and the result is the <code>config</code> fixture.</p> <p>From there, the different components are created using the <code>config</code> fixture, like the <code>datamodule</code>, <code>trainer</code>, <code>algorithm</code>, etc.</p> <pre><code>---\ntitle: Fixture dependency graph\n---\nflowchart TD\ndatamodule_config[\n    &lt;a href=\"#project.conftest.datamodule_config\"&gt;datamodule_config&lt;/a&gt;\n] -- 'datamodule=A' --&gt; command_line_arguments\nalgorithm_config[\n    &lt;a href=\"#project.conftest.algorithm_config\"&gt;algorithm_config&lt;/a&gt;\n] -- 'algorithm=B' --&gt; command_line_arguments\ncommand_line_overrides[\n    &lt;a href=\"#project.conftest.command_line_overrides\"&gt;command_line_overrides&lt;/a&gt;\n] -- 'seed=123' --&gt; command_line_arguments\ncommand_line_arguments[\n    &lt;a href=\"#project.conftest.command_line_arguments\"&gt;command_line_arguments&lt;/a&gt;\n] -- load configs for 'datamodule=A algorithm=B seed=123' --&gt; dict_config\ndict_config[\n    &lt;a href=\"#project.conftest.dict_config\"&gt;dict_config&lt;/a&gt;\n] -- instantiate objects from configs --&gt; config\nconfig[\n    &lt;a href=\"#project.conftest.config\"&gt;config&lt;/a&gt;\n] --&gt; datamodule &amp; algorithm\ndatamodule[\n    &lt;a href=\"#project.conftest.datamodule\"&gt;datamodule&lt;/a&gt;\n] --&gt; algorithm\nalgorithm[\n    &lt;a href=\"#project.conftest.algorithm\"&gt;algorithm&lt;/a&gt;\n] -- is used by --&gt; some_test\nalgorithm &amp; datamodule -- is used by --&gt; some_other_test</code></pre>"},{"location":"reference/project/conftest/#project.conftest.original_datadir","title":"original_datadir","text":"<pre><code>original_datadir(original_datadir: Path)\n</code></pre> <p>Overwrite the original_datadir fixture value to change where regression files are created.</p> <p>By default, they are in a folder next to the source. Here instead we move them to a different folder to keep the source code folder as neat as we can.</p> <p>TODO: The large regression files (the .npz files containing tensors) could be stored in a cache on $SCRATCH and referenced to via a symlink in the test folder. There could be some issues though if scratch is cleaned up.</p>"},{"location":"reference/project/conftest/#project.conftest.algorithm_config","title":"algorithm_config","text":"<pre><code>algorithm_config(request: FixtureRequest) -&gt; str | None\n</code></pre> <p>The algorithm config to use in the experiment, as if <code>algorithm=&lt;value&gt;</code> was passed.</p> <p>This is parametrized with all the configurations for a given algorithm type when using the included tests, for example as is done in project.algorithms.image_classifier_test.</p>"},{"location":"reference/project/conftest/#project.conftest.datamodule_config","title":"datamodule_config","text":"<pre><code>datamodule_config(request: FixtureRequest) -&gt; str | None\n</code></pre> <p>The datamodule config to use in the experiment, as if <code>datamodule=&lt;value&gt;</code> was passed.</p>"},{"location":"reference/project/conftest/#project.conftest.algorithm_network_config","title":"algorithm_network_config","text":"<pre><code>algorithm_network_config(\n    request: FixtureRequest,\n) -&gt; str | None\n</code></pre> <p>The network config to use in the experiment, as in <code>algorithm/network=&lt;value&gt;</code>.</p>"},{"location":"reference/project/conftest/#project.conftest.command_line_arguments","title":"command_line_arguments","text":"<pre><code>command_line_arguments(\n    algorithm_config: str | None,\n    datamodule_config: str | None,\n    algorithm_network_config: str | None,\n    command_line_overrides: tuple[str, ...],\n    request: FixtureRequest,\n)\n</code></pre> <p>Fixture that returns the command-line arguments that will be passed to Hydra to run the experiment.</p> <p>The <code>algorithm_config</code>, <code>network_config</code> and <code>datamodule_config</code> values here are parametrized indirectly by most tests using the <code>project.utils.testutils.run_for_all_configs_of_type</code> function so that the respective components are created in the same way as they would be by Hydra in a regular run.</p>"},{"location":"reference/project/conftest/#project.conftest.dict_config","title":"dict_config","text":"<pre><code>dict_config(\n    command_line_arguments: tuple[str, ...],\n    tmp_path_factory: TempPathFactory,\n) -&gt; DictConfig\n</code></pre> <p>The <code>omegaconf.DictConfig</code> that is created by Hydra from the command-line arguments.</p> <p>This fixture returns exactly what would be the input to the <code>main</code> function.</p> <p>Any interpolations in the configs will not have been resolved at this point.</p>"},{"location":"reference/project/conftest/#project.conftest.config","title":"config","text":"<pre><code>config(dict_config: DictConfig) -&gt; Config\n</code></pre> <p>The experiment configuration, with all interpolations resolved.</p>"},{"location":"reference/project/conftest/#project.conftest.datamodule","title":"datamodule","text":"<pre><code>datamodule(\n    dict_config: DictConfig,\n) -&gt; LightningDataModule | None\n</code></pre> <p>Fixture that creates the datamodule for the given config.</p>"},{"location":"reference/project/conftest/#project.conftest.algorithm","title":"algorithm","text":"<pre><code>algorithm(\n    config: Config,\n    datamodule: LightningDataModule | None,\n    trainer: Trainer | JaxTrainer,\n    seed: int,\n    device: device,\n)\n</code></pre> <p>Fixture that creates the \"algorithm\" (usually a LightningModule).</p>"},{"location":"reference/project/conftest/#project.conftest.seed","title":"seed","text":"<pre><code>seed(\n    request: FixtureRequest, make_torch_deterministic: None\n)\n</code></pre> <p>Fixture that seeds everything for reproducibility and yields the random seed used.</p>"},{"location":"reference/project/conftest/#project.conftest.accelerator","title":"accelerator","text":"<pre><code>accelerator(request: FixtureRequest)\n</code></pre> <p>Returns the accelerator to use during unit tests.</p> <p>By default, if cuda is available, returns \"cuda\". If the tests are run with -vvv, then also runs CPU.</p>"},{"location":"reference/project/conftest/#project.conftest.devices","title":"devices","text":"<pre><code>devices(\n    accelerator: str, request: FixtureRequest\n) -&gt; Generator[\n    list[int] | int | Literal[\"auto\"], None, None\n]\n</code></pre> <p>Fixture that creates the 'devices' argument for the Trainer config.</p> <p>Splits up the GPUs between pytest-xdist workers when using distributed testing. This isn't currently used in the CI.</p> <p>TODO: Design dilemna here: Should we be parametrizing the <code>devices</code> command-line override and force experiments to run with this value during tests? Or should we be changing things based on this value in the config?</p>"},{"location":"reference/project/conftest/#project.conftest.command_line_overrides","title":"command_line_overrides","text":"<pre><code>command_line_overrides(\n    request: FixtureRequest,\n) -&gt; tuple[str, ...]\n</code></pre> <p>Fixture that makes it possible to specify command-line overrides to use in a given test.</p> <p>Tests that require running an experiment should use the <code>experiment_config</code> fixture below.</p> <p>Multiple test using the same overrides will use the same experiment.</p>"},{"location":"reference/project/conftest/#project.conftest.setup_with_overrides","title":"setup_with_overrides","text":"<pre><code>setup_with_overrides(\n    overrides: (\n        str\n        | ParameterSet\n        | list[str]\n        | list[ParameterSet]\n        | list[str | ParameterSet]\n    ),\n)\n</code></pre> <p>Configures tests to run with the hydra configs that are loaded with these command-line args.</p> <p>The command-line arguments are used to create the Hydra config (the input to the <code>main</code> function). From there the different components (trainer, algorithm, callbacks, optionally datamodule) are created by fixtures with the same names.</p> <p>This should be applied on tests that use some of these components created from Hydra configs, for example:</p> <pre><code>@setup_with_overrides(\"algorithm=example trainer.max_epochs=1\")\ndef test_something(dict_config: omegaconf.DictConfig):\n    '''This test receives the `dict_config` loaded from Hydra with the given overrides.'''\n    assert dict_config[\"algorithm\"][\"_target_\"] == \"project.algorithm.image_classifier.ImageClassifier\"\n    assert dict_config[\"trainer\"][\"max_epochs\"] == 1\n</code></pre>"},{"location":"reference/project/conftest/#project.conftest.make_torch_deterministic","title":"make_torch_deterministic","text":"<pre><code>make_torch_deterministic()\n</code></pre> <p>Set torch to deterministic mode for unit tests that use the tensor_regression fixture.</p>"},{"location":"reference/project/conftest/#project.conftest.pytest_runtest_makereport","title":"pytest_runtest_makereport","text":"<pre><code>pytest_runtest_makereport(item: Function, call: CallInfo)\n</code></pre> <p>Used to setup the <code>pytest.mark.incremental</code> mark, as described in the pytest docs.</p> <p>See this page</p>"},{"location":"reference/project/conftest/#project.conftest.pytest_runtest_setup","title":"pytest_runtest_setup","text":"<pre><code>pytest_runtest_setup(item: Function)\n</code></pre> <p>Used to setup the <code>pytest.mark.incremental</code> mark, as described in this page.</p>"},{"location":"reference/project/conftest/#project.conftest.pytest_generate_tests","title":"pytest_generate_tests","text":"<pre><code>pytest_generate_tests(metafunc: Metafunc) -&gt; None\n</code></pre> <p>Allows one to define custom parametrization schemes or extensions.</p> <p>This is used to implement the <code>parametrize_when_used</code> mark, which allows one to parametrize an argument when it is used.</p> <p>See https://docs.pytest.org/en/7.1.x/how-to/parametrize.html#how-to-parametrize-fixtures-and-test-functions</p>"},{"location":"reference/project/experiment/","title":"Experiment","text":"<p>The training and evaluation functions.</p> <p>NOTE: This has to be in a different file than <code>main</code> because we want to allow registering different variants of the <code>train_and_evaluate</code> function for different algorithms with <code>functools.singledispatch</code>. If we have everything in <code>main.py</code>, the registration doesn't happen correctly.</p>"},{"location":"reference/project/experiment/#project.experiment.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(\n    algorithm,\n    /,\n    *,\n    datamodule: LightningDataModule | None = None,\n    config: Config,\n)\n</code></pre> <p>Generic function that trains and evaluates a learning algorithm.</p> <p>This by default assumes that the algorithm is a LightningModule, but can be extended to implement specific training / evaluation procedures for different algorithms.</p> <p>The default implementation here does roughly the same thing as https://github.com/ashleve/lightning-hydra-template/blob/main/src/train.py</p> <ol> <li>Instantiates the experiment components from the Hydra configuration:<ul> <li>algorithm (already instantiated)</li> <li>trainer</li> <li>datamodule (optional)</li> </ul> </li> <li>Calls <code>trainer.fit</code> to train the algorithm</li> <li>Calls <code>trainer.evaluate</code> or <code>trainer.test</code> to evaluate the model</li> <li>Returns the metrics.</li> </ol>"},{"location":"reference/project/experiment/#project.experiment.train_and_evaluate--extending-to-other-algorithms-or-training-procedures","title":"Extending to other algorithms or training procedures","text":"<p>For example, if your algorithm has to be trained in two distinct phases, or if you want to use a different kind of Trainer that does something other than just call <code>.fit</code> and <code>.evaluate</code>, you could do something like this:</p> <pre><code>@train_and_evaluate.register(MyAlgorithm)\ndef train_and_evaluate_my_algo(algorithm: MyAlgorithm, /, *, trainer, datamodule)\n    # making this up, this isn't doable with any of the datamodules at the moment.\n    datamodule.set_task(1)\n    trainer.fit(algorithm, datamodule)\n\n    datamodule.set_task(2)\n    trainer.fit(algorithm, datamodule)\n</code></pre>"},{"location":"reference/project/experiment/#project.experiment.evaluate_lightning","title":"evaluate_lightning","text":"<pre><code>evaluate_lightning(\n    algorithm: LightningModule,\n    /,\n    *,\n    trainer: Trainer,\n    datamodule: LightningDataModule | None = None,\n) -&gt; tuple[str, float | None, dict]\n</code></pre> <p>Evaluates the algorithm and returns the metrics.</p> <p>By default, if validation is to be performed, returns the validation error. Returns the training error when <code>trainer.overfit_batches != 0</code> (e.g. when debugging or testing). Otherwise, if <code>trainer.limit_val_batches == 0</code>, returns the test error.</p>"},{"location":"reference/project/experiment/#project.experiment.instantiate_trainer","title":"instantiate_trainer","text":"<pre><code>instantiate_trainer(\n    trainer_config: dict | DictConfig,\n) -&gt; Trainer | Any\n</code></pre> <p>Instantiates the callbacks and loggers first, then creates the trainer from its config.</p>"},{"location":"reference/project/experiment/#project.experiment.instantiate_values","title":"instantiate_values","text":"<pre><code>instantiate_values(\n    config_dict: DictConfig | None,\n) -&gt; list[Any] | None\n</code></pre> <p>Returns the list of objects at the values in this dict of configs.</p> <p>This is used for the config of the <code>trainer/logger</code> and <code>trainer/callbacks</code> fields, where we can combine multiple config groups by adding entries in a dict.</p> <p>For example, using <code>trainer/logger=wandb</code> and <code>trainer/logger=tensorboard</code> would result in a dict with <code>wandb</code> and <code>tensorboard</code> as keys, and the corresponding config groups as values.</p> <p>This would then return a list with the instantiated WandbLogger and TensorBoardLogger objects.</p>"},{"location":"reference/project/main/","title":"Main","text":"<p>Training script using Hydra.</p> <p>This does the following: 1. Parses the config using Hydra; 2. Instantiated the components (trainer / algorithm), optionally datamodule and network; 3. Trains the model; 4. Optionally runs an evaluation loop.</p>"},{"location":"reference/project/main/#project.main.main","title":"main","text":"<pre><code>main(dict_config: DictConfig) -&gt; dict\n</code></pre> <p>Main entry point: trains &amp; evaluates a learning algorithm.</p>"},{"location":"reference/project/main/#project.main.instantiate_algorithm","title":"instantiate_algorithm","text":"<pre><code>instantiate_algorithm(\n    algorithm_config: HydraConfigFor[\n        LightningModule | JaxModule\n    ],\n    datamodule: LightningDataModule | None = None,\n) -&gt; LightningModule | JaxModule\n</code></pre> <p>Function used to instantiate the algorithm.</p> <p>It is suggested that your algorithm (LightningModule) take in the <code>datamodule</code> and <code>network</code> as arguments, to make it easier to swap out different networks and datamodules during experiments.</p> <p>The instantiated datamodule will be passed to the algorithm's constructor.</p>"},{"location":"reference/project/main_test/","title":"Main test","text":""},{"location":"reference/project/main_test/#project.main_test.experiment_configs","title":"experiment_configs  <code>module-attribute</code>","text":"<pre><code>experiment_configs = [stem for p in glob('*.yaml')]\n</code></pre> <p>The list of all experiments configs in the <code>configs/experiment</code> directory.</p> <p>This is used to check that all the experiment configs are covered by tests.</p>"},{"location":"reference/project/main_test/#project.main_test.experiment_commands_to_test","title":"experiment_commands_to_test  <code>module-attribute</code>","text":"<pre><code>experiment_commands_to_test: list[str | ParameterSet] = []\n</code></pre> <p>List of experiment commands to run for testing.</p> <p>Consider adding a command that runs simple sanity check for your algorithm, something like one step of training or something similar.</p>"},{"location":"reference/project/main_test/#project.main_test.test_jax_can_use_the_GPU","title":"test_jax_can_use_the_GPU","text":"<pre><code>test_jax_can_use_the_GPU()\n</code></pre> <p>Test that Jax can use the GPU if it we have one.</p>"},{"location":"reference/project/main_test/#project.main_test.test_torch_can_use_the_GPU","title":"test_torch_can_use_the_GPU","text":"<pre><code>test_torch_can_use_the_GPU()\n</code></pre> <p>Test that torch can use the GPU if it we have one.</p>"},{"location":"reference/project/main_test/#project.main_test.test_can_run_experiment","title":"test_can_run_experiment","text":"<pre><code>test_can_run_experiment(\n    command_line_overrides: tuple[str, ...],\n    request: FixtureRequest,\n    monkeypatch: MonkeyPatch,\n)\n</code></pre> <p>Launches the sanity check experiments using the commands from the list above.</p>"},{"location":"reference/project/main_test/#project.main_test.test_setting_just_algorithm_isnt_enough","title":"test_setting_just_algorithm_isnt_enough","text":"<pre><code>test_setting_just_algorithm_isnt_enough(\n    dict_config: DictConfig,\n) -&gt; None\n</code></pre> <p>Check that the datamodule is required on the command-line if the algorithm needs one.</p>"},{"location":"reference/project/main_test/#project.main_test.test_run_auto_schema_via_cli_without_errors","title":"test_run_auto_schema_via_cli_without_errors","text":"<pre><code>test_run_auto_schema_via_cli_without_errors()\n</code></pre> <p>Checks that the command completes without errors.</p>"},{"location":"reference/project/main_test/#project.main_test.test_setup_with_overrides_works","title":"test_setup_with_overrides_works","text":"<pre><code>test_setup_with_overrides_works(dict_config: DictConfig)\n</code></pre> <p>This test receives the <code>dict_config</code> loaded from Hydra with the given overrides.</p>"},{"location":"reference/project/algorithms/","title":"Algorithms","text":""},{"location":"reference/project/algorithms/image_classifier/","title":"Image classifier","text":"<p>Example of a simple algorithm for image classification.</p> <p>This can be run from the command-line like so:</p> <pre><code>python project/main.py algorithm=image_classification datamodule=cifar10\n</code></pre>"},{"location":"reference/project/algorithms/image_classifier/#project.algorithms.image_classifier.ImageClassifier","title":"ImageClassifier","text":"<p>               Bases: <code>LightningModule</code></p> <p>Example learning algorithm for image classification.</p>"},{"location":"reference/project/algorithms/image_classifier/#project.algorithms.image_classifier.ImageClassifier.__init__","title":"__init__","text":"<pre><code>__init__(\n    datamodule: ImageClassificationDataModule,\n    network: HydraConfigFor[Module],\n    optimizer: HydraConfigFor[partial[Optimizer]],\n    init_seed: int = 42,\n)\n</code></pre> <p>Create a new instance of the algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <code>ImageClassificationDataModule</code> <p>Object used to load train/val/test data. See the lightning docs for LightningDataModule for more info.</p> required <code>network</code> <code>HydraConfigFor[Module]</code> <p>The config of the network to instantiate and train.</p> required <code>optimizer</code> <code>HydraConfigFor[partial[Optimizer]]</code> <p>The config for the Optimizer. Instantiating this will return a function                 (a functools.partial) that will create the Optimizer given the hyper-parameters.</p> required <code>init_seed</code> <code>int</code> <p>The seed to use when initializing the weights of the network.</p> <code>42</code>"},{"location":"reference/project/algorithms/image_classifier/#project.algorithms.image_classifier.ImageClassifier.forward","title":"forward","text":"<pre><code>forward(input: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass of the network.</p>"},{"location":"reference/project/algorithms/image_classifier/#project.algorithms.image_classifier.ImageClassifier.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Creates the optimizers.</p> <p>See <code>lightning.pytorch.core.LightningModule.configure_optimizers</code> for more information.</p>"},{"location":"reference/project/algorithms/image_classifier/#project.algorithms.image_classifier.ImageClassifier.configure_callbacks","title":"configure_callbacks","text":"<pre><code>configure_callbacks() -&gt; Sequence[Callback] | Callback\n</code></pre> <p>Creates callbacks to be used by default during training.</p>"},{"location":"reference/project/algorithms/image_classifier_test/","title":"Image classifier test","text":"<p>Example showing how the test suite can be used to add tests for a new algorithm.</p>"},{"location":"reference/project/algorithms/image_classifier_test/#project.algorithms.image_classifier_test.TestImageClassifier","title":"TestImageClassifier","text":"<p>               Bases: <code>LightningModuleTests[ImageClassifier]</code></p> <p>Tests for the <code>ImageClassifier</code>.</p> <p>This runs all the tests included in the base class, with the given parametrizations:</p> <ul> <li><code>algorithm_config</code> will take the value <code>\"image_classifier\"</code><ul> <li>This is because there is an <code>image_classifier.yaml</code> config file in project/configs/algorithms   whose <code>_target_</code> is the <code>ImageClassifier</code>.</li> </ul> </li> <li><code>datamodule_config</code> will take these values: <code>['cifar10', 'fashion_mnist', 'imagenet', 'inaturalist', 'mnist']</code><ul> <li>These are all the configs whose target is an <code>ImageClassificationDataModule</code>.</li> </ul> </li> <li>Similarly, <code>network_config</code> will be parametrized by the names of all configs which produce an nn.Module,   except those that would create a <code>PreTrainedModel</code> from HuggingFace.<ul> <li>This is currently the easiest way for us to say \"any network for image classification.</li> </ul> </li> </ul> <p>Take a look at the <code>LightningModuleTests</code> class if you want to see the actual test code.</p>"},{"location":"reference/project/algorithms/image_classifier_test/#project.algorithms.image_classifier_test.test_example_experiment_defaults","title":"test_example_experiment_defaults","text":"<pre><code>test_example_experiment_defaults(config: Config) -&gt; None\n</code></pre> <p>Test to check that the datamodule is required (even when just an algorithm is set?!).</p>"},{"location":"reference/project/algorithms/jax_image_classifier/","title":"Jax image classifier","text":""},{"location":"reference/project/algorithms/jax_image_classifier/#project.algorithms.jax_image_classifier.JaxCNN","title":"JaxCNN","text":"<p>               Bases: <code>Module</code></p> <p>A simple CNN model.</p> <p>Taken from https://flax.readthedocs.io/en/latest/quick_start.html#define-network</p>"},{"location":"reference/project/algorithms/jax_image_classifier/#project.algorithms.jax_image_classifier.JaxImageClassifier","title":"JaxImageClassifier","text":"<p>               Bases: <code>LightningModule</code></p> <p>Example of a learning algorithm (<code>LightningModule</code>) that uses Jax.</p> <p>In this case, the network is a flax.linen.Module, and its forward and backward passes are written in Jax, and the loss function is in pytorch.</p>"},{"location":"reference/project/algorithms/jax_image_classifier/#project.algorithms.jax_image_classifier.JaxImageClassifier.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Creates the optimizers.</p> <p>See <code>lightning.pytorch.core.LightningModule.configure_optimizers</code> for more information.</p>"},{"location":"reference/project/algorithms/jax_image_classifier_test/","title":"Jax image classifier test","text":""},{"location":"reference/project/algorithms/jax_image_classifier_test/#project.algorithms.jax_image_classifier_test.TestJaxImageClassifier","title":"TestJaxImageClassifier","text":"<p>               Bases: <code>LightningModuleTests[JaxImageClassifier]</code></p> <p>Tests for the Jax image classification algorithm.</p> <p>This simply reuses all the tests in the base test suite, specifying that the <code>datamodule</code> passed to the <code>JaxImageClassifier</code> should be for image classification and the <code>network</code> should be a <code>flax.linen.Module</code>.</p>"},{"location":"reference/project/algorithms/jax_image_classifier_test/#project.algorithms.jax_image_classifier_test.test_demo","title":"test_demo","text":"<pre><code>test_demo(tmp_path: Path)\n</code></pre> <p>Test the demo at the bottom of the module.</p>"},{"location":"reference/project/algorithms/jax_ppo/","title":"Jax ppo","text":"<p>Example of an RL algorithm (PPO) written entirely in Jax.</p> <p>This is based on <code>rejax.PPO</code>. See the <code>JaxRLExample</code> class for a description of the differences w.r.t. <code>rejax.PPO</code>.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.TEnvParams","title":"TEnvParams  <code>module-attribute</code>","text":"<pre><code>TEnvParams = TypeVar(\n    \"TEnvParams\", bound=EnvParams, default=EnvParams\n)\n</code></pre> <p>Type variable for the env params (<code>gymnax.EnvParams</code>).</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.Trajectory","title":"Trajectory","text":"<p>               Bases: <code>PyTreeNode</code></p> <p>A sequence of interactions between an agent and an environment.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.TrajectoryWithLastObs","title":"TrajectoryWithLastObs","text":"<p>               Bases: <code>PyTreeNode</code></p> <p>Trajectory with the last observation and whether the last step is the end of an episode.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.AdvantageMinibatch","title":"AdvantageMinibatch","text":"<p>               Bases: <code>PyTreeNode</code></p> <p>Annotated trajectories with advantages and targets for the critic.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.TrajectoryCollectionState","title":"TrajectoryCollectionState","text":"<p>               Bases: <code>Generic[TEnvState]</code>, <code>PyTreeNode</code></p> <p>Struct containing the state related to the collection of data from the environment.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.PPOState","title":"PPOState","text":"<p>               Bases: <code>Generic[TEnvState]</code>, <code>PyTreeNode</code></p> <p>Contains all the state of the <code>JaxRLExample</code> algorithm.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.PPOHParams","title":"PPOHParams","text":"<p>               Bases: <code>PyTreeNode</code></p> <p>Hyper-parameters for this PPO example.</p> <p>These are taken from <code>rejax.PPO</code> algorithm class.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.JaxRLExample","title":"JaxRLExample","text":"<p>               Bases: <code>PyTreeNode</code>, <code>JaxModule[PPOState[TEnvState], TrajectoryWithLastObs, EvalMetrics]</code>, <code>Generic[TEnvState, TEnvParams]</code></p> <p>Example of an RL algorithm written in Jax: PPO, based on <code>rejax.PPO</code>.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.JaxRLExample--differences-wrt-rejaxppo","title":"Differences w.r.t. rejax.PPO:","text":"<ul> <li>The state / hparams are split into different, fully-typed structs:<ul> <li>The algorithm state is in a typed <code>PPOState</code> struct (vs an untyped,     dynamically-generated struct in rejax).</li> <li>The hyper-parameters are in a typed <code>PPOHParams</code> struct.</li> <li>The state variables related to the collection of data from the environment is a     <code>TrajectoryCollectionState</code> instead of everything being bunched up together.<ul> <li>This makes it easier to call the <code>collect_episodes</code> function with just what it needs.</li> </ul> </li> </ul> </li> <li>The seeds for the networks and the environment data collection are separated.</li> </ul> <p>The logic is exactly the same: The losses / updates are computed in the exact same way.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.JaxRLExample.training_step","title":"training_step","text":"<pre><code>training_step(\n    batch_idx: int,\n    ts: PPOState[TEnvState],\n    batch: TrajectoryWithLastObs,\n)\n</code></pre> <p>Training step in pure jax.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.JaxRLExample.train","title":"train","text":"<pre><code>train(\n    rng: Array,\n    train_state: PPOState[TEnvState] | None = None,\n    skip_initial_evaluation: Static[bool] = False,\n) -&gt; tuple[PPOState[TEnvState], EvalMetrics]\n</code></pre> <p>Full training loop in jax.</p> <p>This is only here to match the API of <code>rejax.PPO.train</code>. This doesn't get called when using the <code>JaxTrainer</code>, since <code>JaxTrainer.fit</code> already does the same thing, but also with support for some <code>JaxCallback</code>s (as well as some <code>lightning.Callback</code>s!).</p> <p>Unfolded version of <code>rejax.PPO.train</code>.</p>"},{"location":"reference/project/algorithms/jax_ppo/#project.algorithms.jax_ppo.field","title":"field","text":"<pre><code>field(\n    *,\n    default: T | _MISSING_TYPE = MISSING,\n    default_factory: (\n        Callable[[], T] | _MISSING_TYPE\n    ) = MISSING,\n    init=True,\n    repr=True,\n    hash=None,\n    compare=True,\n    metadata: Mapping[Any, Any] | None = None,\n    kw_only=MISSING,\n    pytree_node: bool | None = None\n) -&gt; T\n</code></pre> <p>Small Typing fix for <code>flax.struct.field</code>.</p> <ul> <li>Add type annotations so it doesn't drop the signature of the <code>dataclasses.field</code> function.</li> <li>Make the <code>pytree_node</code> has a default value of <code>False</code> for ints and bools, and <code>True</code> for   everything else.</li> </ul>"},{"location":"reference/project/algorithms/jax_ppo_test/","title":"Jax ppo test","text":""},{"location":"reference/project/algorithms/jax_ppo_test/#project.algorithms.jax_ppo_test.PPOLightningModule","title":"PPOLightningModule","text":"<p>               Bases: <code>LightningModule</code></p> <p>Uses the same code as <code>JaxRLExample</code>, but the training loop is run with pytorch-lightning.</p> <p>This is currently only meant to be used to compare the difference fully-jitted training loop and lightning.</p>"},{"location":"reference/project/algorithms/jax_ppo_test/#project.algorithms.jax_ppo_test.RlThroughputCallback","title":"RlThroughputCallback","text":"<p>               Bases: <code>MeasureSamplesPerSecondCallback</code></p> <p>A callback to measure the throughput of RL algorithms.</p>"},{"location":"reference/project/algorithms/jax_ppo_test/#project.algorithms.jax_ppo_test.test_rejax","title":"test_rejax","text":"<pre><code>test_rejax(\n    rng: PRNGKey,\n    results_rejax: tuple[PPO, Any, EvalMetrics],\n    tensor_regression: TensorRegressionFixture,\n    original_datadir: Path,\n    seed: int | Sequence[int],\n)\n</code></pre> <p>Train <code>rejax.PPO</code> with the same parameters.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/","title":"Lightning module tests","text":"<p>Suite of tests for an a <code>LightningModule</code>.</p> <p>See the project.algorithms.image_classifier_test module for an example of how to use this.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests","title":"LightningModuleTests","text":"<p>               Bases: <code>Generic[LightningModuleType]</code>, <code>ABC</code></p> <p>Suite of generic tests for a LightningModule.</p> <p>Simply inherit from this class and decorate the class with the appropriate markers to get a set of decent unit tests that should apply to almost any LightningModule.</p> <p>See the project.algorithms.image_classifier_test module for an example.</p> <p>Other ideas: - pytest-benchmark for regression tests on forward / backward pass / training step speed - pytest-profiling for profiling the training step? (pytorch variant?) - Dataset splits: check some basic stats about the train/val/test inputs, are they somewhat similar? - Define the input as a space, check that the dataset samples are in that space and not too   many samples are statistically OOD? - Test to monitor distributed traffic out of this process?     - Dummy two-process tests (on CPU) to check before scaling up experiments?</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.config","title":"config","text":"<pre><code>config(dict_config: DictConfig) -&gt; Config\n</code></pre> <p>The experiment configuration, with all interpolations resolved.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.algorithm","title":"algorithm","text":"<pre><code>algorithm(\n    config: Config,\n    datamodule: LightningDataModule | None,\n    trainer: Trainer,\n    device: device,\n)\n</code></pre> <p>Fixture that creates the \"algorithm\" (usually a <code>LightningModule</code>).</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.make_torch_deterministic","title":"make_torch_deterministic","text":"<pre><code>make_torch_deterministic()\n</code></pre> <p>Set torch to deterministic mode for unit tests that use the tensor_regression fixture.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.seed","title":"seed","text":"<pre><code>seed(request: FixtureRequest)\n</code></pre> <p>Fixture that seeds everything for reproducibility and yields the random seed used.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.training_step_content","title":"training_step_content","text":"<pre><code>training_step_content(\n    datamodule: LightningDataModule | None,\n    algorithm: LightningModuleType,\n    seed: int,\n    accelerator: str,\n    devices: int | list[int],\n    tmp_path_factory: TempPathFactory,\n)\n</code></pre> <p>Fixture that runs a training step and makes various things available for tests.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.test_initialization_is_reproducible","title":"test_initialization_is_reproducible","text":"<pre><code>test_initialization_is_reproducible(\n    training_step_content: StuffFromFirstTrainingStep,\n    tensor_regression: TensorRegressionFixture,\n    accelerator: str,\n)\n</code></pre> <p>Check that the network initialization is reproducible given the same random seed.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.test_forward_pass_is_reproducible","title":"test_forward_pass_is_reproducible","text":"<pre><code>test_forward_pass_is_reproducible(\n    algorithm: LightningModuleType,\n    training_step_content: StuffFromFirstTrainingStep,\n    tensor_regression: TensorRegressionFixture,\n)\n</code></pre> <p>Check that the forward pass is reproducible given the same input and random seed.</p> <p>Note: There could be more than one call to <code>forward</code> inside a training step. Here we only check the args/kwargs/outputs of the first <code>forward</code> call for now.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.test_backward_pass_is_reproducible","title":"test_backward_pass_is_reproducible","text":"<pre><code>test_backward_pass_is_reproducible(\n    training_step_content: StuffFromFirstTrainingStep,\n    tensor_regression: TensorRegressionFixture,\n    accelerator: str,\n)\n</code></pre> <p>Check that the backward pass is reproducible given the same weights, inputs and random seed.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.test_update_is_reproducible","title":"test_update_is_reproducible","text":"<pre><code>test_update_is_reproducible(\n    algorithm: LightningModuleType,\n    training_step_content: StuffFromFirstTrainingStep,\n    tensor_regression: TensorRegressionFixture,\n    accelerator: str,\n)\n</code></pre> <p>Check that the weights after one step of training are the same given the same seed.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.LightningModuleTests.do_one_step_of_training","title":"do_one_step_of_training","text":"<pre><code>do_one_step_of_training(\n    algorithm: LightningModuleType,\n    datamodule: LightningDataModule | None,\n    accelerator: str,\n    devices: int | list[int] | Literal[\"auto\"],\n    callbacks: list[Callback],\n    tmp_path: Path,\n)\n</code></pre> <p>Performs one step of training.</p> <p>Overwrite this if you train your algorithm differently.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep","title":"StuffFromFirstTrainingStep  <code>dataclass</code>","text":"<p>Dataclass that holds information gathered from a training step and used in tests.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.batch","title":"batch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch: Any | None = None\n</code></pre> <p>The input batch passed to the <code>training_step</code> method.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.forward_args","title":"forward_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forward_args: list[tuple[Any, ...]] = field(\n    default_factory=list\n)\n</code></pre> <p>The inputs args passed to each call to <code>forward</code> during the training step.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.forward_kwargs","title":"forward_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forward_kwargs: list[dict[str, Any]] = field(\n    default_factory=list\n)\n</code></pre> <p>The inputs kwargs apssed to each call to <code>forward</code> during the training step.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.forward_outputs","title":"forward_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forward_outputs: list[Any] = field(default_factory=list)\n</code></pre> <p>The outputs of each call to the <code>forward</code> method during the training step.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.initial_state_dict","title":"initial_state_dict  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>initial_state_dict: dict[str, Tensor] = field(\n    default_factory=dict\n)\n</code></pre> <p>A copy of the state dict before the training step (moved to CPU).</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.grads","title":"grads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>grads: dict[str, Tensor | None] = field(\n    default_factory=dict\n)\n</code></pre> <p>A copy of the gradients of the model parameters after the backward pass (moved to CPU).</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.StuffFromFirstTrainingStep.training_step_output","title":"training_step_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>training_step_output: Tensor | Mapping[str, Any] | None = (\n    None\n)\n</code></pre> <p>The output of the <code>training_step</code> method.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.GetStuffFromFirstTrainingStep","title":"GetStuffFromFirstTrainingStep","text":"<p>               Bases: <code>Callback</code></p> <p>Callback used in tests to get things from the first call to <code>training_step</code>.</p>"},{"location":"reference/project/algorithms/lightning_module_tests/#project.algorithms.lightning_module_tests.convert_list_and_tuples_to_dicts","title":"convert_list_and_tuples_to_dicts","text":"<pre><code>convert_list_and_tuples_to_dicts(value: Tensor) -&gt; Tensor\n</code></pre><pre><code>convert_list_and_tuples_to_dicts(\n    value: dict | tuple | list,\n) -&gt; dict[str, Any]\n</code></pre> <pre><code>convert_list_and_tuples_to_dicts(\n    value: Tensor | dict | tuple | list,\n) -&gt; Tensor | dict[str, Any]\n</code></pre> <p>Converts all lists and tuples in a nested structure to dictionaries.</p> <p>convert_list_and_tuples_to_dicts([1, 2, 3]) {'0': 1, '1': 2, '2': 3} convert_list_and_tuples_to_dicts((1, 2, 3)) {'0': 1, '1': 2, '2': 3} convert_list_and_tuples_to_dicts({\"a\": [1, 2, 3], \"b\": (4, 5, 6)}) {'a': {'0': 1, '1': 2, '2': 3}, 'b': {'0': 4, '1': 5, '2': 6}}</p>"},{"location":"reference/project/algorithms/llm_finetuning/","title":"Llm finetuning","text":"<p>Example: fine-tuning a language model (GPT, GPT-2, CTRL, OPT, etc.) on a text dataset.</p> <p>Large chunks of the code here are taken from this example script in the transformers GitHub repository.</p> <p>If you haven't already, you should definitely check out this walkthrough of that script from the HuggingFace docs.</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.NetworkConfig","title":"NetworkConfig","text":"<p>Configuration options related to the choice of network.</p> <p>When instantiated by Hydra, this calls the <code>target</code> function passed to the decorator. In this case, this creates pulls the pretrained network weights from the HuggingFace model hub.</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.TokenizerConfig","title":"TokenizerConfig","text":"<p>Configuration options for the tokenizer.</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.DatasetConfig","title":"DatasetConfig  <code>dataclass</code>","text":"<p>Configuration options related to the dataset preparation.</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.DatasetConfig.dataset_path","title":"dataset_path  <code>instance-attribute</code>","text":"<pre><code>dataset_path: str\n</code></pre> <p>Name of the dataset \"family\"?</p> <p>For example, to load \"wikitext/wikitext-103-v1\", this would be \"wikitext\".</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.DatasetConfig.dataset_name","title":"dataset_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_name: str | None = None\n</code></pre> <p>Name of the specific dataset?</p> <p>For example, to load \"wikitext/wikitext-103-v1\", this would be \"wikitext-103-v1\".</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.DatasetConfig.validation_split_percentage","title":"validation_split_percentage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>validation_split_percentage: int = 10\n</code></pre> <p>Fraction of the train dataset to use for validation if there isn't already a validation split.</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.LLMFinetuningExample","title":"LLMFinetuningExample","text":"<p>               Bases: <code>LightningModule</code></p> <p>Example of a lightning module used to fine-tune a huggingface model.</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.LLMFinetuningExample.setup","title":"setup","text":"<pre><code>setup(stage: str)\n</code></pre> <p>Hook from Lightning that is called at the start of training, validation and testing.</p> <p>TODO: Later perhaps we could do the preprocessing in a distributed manner like this: https://discuss.huggingface.co/t/how-to-save-datasets-as-distributed-with-save-to-disk/25674/2</p>"},{"location":"reference/project/algorithms/llm_finetuning/#project.algorithms.llm_finetuning.LLMFinetuningExample.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Prepare optimizer and schedule (linear warmup and decay)</p>"},{"location":"reference/project/algorithms/llm_finetuning_test/","title":"Llm finetuning test","text":"<p>Unit tests for the llm finetuning example.</p>"},{"location":"reference/project/algorithms/llm_finetuning_test/#project.algorithms.llm_finetuning_test.TestLLMFinetuningExample","title":"TestLLMFinetuningExample","text":"<p>               Bases: <code>LightningModuleTests[LLMFinetuningExample]</code></p> <p>Tests for the LLM fine-tuning example.</p>"},{"location":"reference/project/algorithms/no_op/","title":"No op","text":""},{"location":"reference/project/algorithms/no_op/#project.algorithms.no_op.NoOp","title":"NoOp","text":"<p>               Bases: <code>LightningModule</code></p> <p>Algorithm that does no learning and is used to benchmark the dataloading speed.</p>"},{"location":"reference/project/algorithms/text_classifier/","title":"Text classifier","text":""},{"location":"reference/project/algorithms/text_classifier/#project.algorithms.text_classifier.TextClassifier","title":"TextClassifier","text":"<p>               Bases: <code>LightningModule</code></p> <p>Example of a lightning module used to train a huggingface model for text classification.</p>"},{"location":"reference/project/algorithms/text_classifier/#project.algorithms.text_classifier.TextClassifier.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Prepare optimizer and schedule (linear warmup and decay)</p>"},{"location":"reference/project/algorithms/text_classifier_test/","title":"Text classifier test","text":""},{"location":"reference/project/algorithms/text_classifier_test/#project.algorithms.text_classifier_test.TestTextClassifier","title":"TestTextClassifier","text":"<p>               Bases: <code>LightningModuleTests[TextClassifier]</code></p> <p>Tests for the HF example.</p>"},{"location":"reference/project/algorithms/text_classifier_test/#project.algorithms.text_classifier_test.TestTextClassifier.test_overfit_batch","title":"test_overfit_batch","text":"<pre><code>test_overfit_batch(\n    algorithm: TextClassifier,\n    datamodule: TextClassificationDataModule,\n    tmp_path: Path,\n    num_steps: int = 3,\n)\n</code></pre> <p>Test that the loss decreases on a single batch.</p>"},{"location":"reference/project/algorithms/callbacks/","title":"Callbacks","text":""},{"location":"reference/project/algorithms/callbacks/#project.algorithms.callbacks.ClassificationMetricsCallback","title":"ClassificationMetricsCallback","text":"<p>               Bases: <code>Callback</code></p> <p>Callback that adds classification metrics to a LightningModule.</p>"},{"location":"reference/project/algorithms/callbacks/classification_metrics/","title":"Classification metrics","text":""},{"location":"reference/project/algorithms/callbacks/classification_metrics/#project.algorithms.callbacks.classification_metrics.ClassificationOutputs","title":"ClassificationOutputs","text":"<p>               Bases: <code>TypedDict</code></p> <p>The outputs that should be minimally returned from the training/val/test_step of classification LightningModules so that metrics can be added aumatically by the <code>ClassificationMetricsCallback</code>.</p>"},{"location":"reference/project/algorithms/callbacks/classification_metrics/#project.algorithms.callbacks.classification_metrics.ClassificationOutputs.loss","title":"loss  <code>instance-attribute</code>","text":"<pre><code>loss: NotRequired[Tensor | float]\n</code></pre> <p>The loss at this step.</p>"},{"location":"reference/project/algorithms/callbacks/classification_metrics/#project.algorithms.callbacks.classification_metrics.ClassificationOutputs.logits","title":"logits  <code>instance-attribute</code>","text":"<pre><code>logits: Required[Tensor]\n</code></pre> <p>The un-normalized logits.</p>"},{"location":"reference/project/algorithms/callbacks/classification_metrics/#project.algorithms.callbacks.classification_metrics.ClassificationOutputs.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: Required[Tensor]\n</code></pre> <p>The class labels.</p>"},{"location":"reference/project/algorithms/callbacks/classification_metrics/#project.algorithms.callbacks.classification_metrics.ClassificationMetricsCallback","title":"ClassificationMetricsCallback","text":"<p>               Bases: <code>Callback</code></p> <p>Callback that adds classification metrics to a LightningModule.</p>"},{"location":"reference/project/algorithms/callbacks/samples_per_second/","title":"Samples per second","text":""},{"location":"reference/project/algorithms/networks/","title":"Networks","text":"<p>Network definitions.</p>"},{"location":"reference/project/algorithms/networks/#project.algorithms.networks.FcNet","title":"FcNet","text":"<p>               Bases: <code>Sequential</code></p> <p>Simple fully-connected network.</p>"},{"location":"reference/project/algorithms/networks/#project.algorithms.networks.FcNet.HParams","title":"HParams  <code>dataclass</code>","text":"<p>Dataclass containing the network hyper-parameters.</p> <p>This is an example of how Pydantic can be used to validate configs and command-line arguments.</p>"},{"location":"reference/project/algorithms/networks/#project.algorithms.networks.FcNet.HParams.dropout_rate","title":"dropout_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout_rate: float = 0.5\n</code></pre> <p>Dropout rate.</p> <p>Set to 0 to disable dropout.</p>"},{"location":"reference/project/algorithms/networks/fcnet/","title":"Fcnet","text":"<p>An example of a simple fully connected network.</p>"},{"location":"reference/project/algorithms/networks/fcnet/#project.algorithms.networks.fcnet.FcNet","title":"FcNet","text":"<p>               Bases: <code>Sequential</code></p> <p>Simple fully-connected network.</p>"},{"location":"reference/project/algorithms/networks/fcnet/#project.algorithms.networks.fcnet.FcNet.HParams","title":"HParams  <code>dataclass</code>","text":"<p>Dataclass containing the network hyper-parameters.</p> <p>This is an example of how Pydantic can be used to validate configs and command-line arguments.</p>"},{"location":"reference/project/algorithms/networks/fcnet/#project.algorithms.networks.fcnet.FcNet.HParams.dropout_rate","title":"dropout_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout_rate: float = 0.5\n</code></pre> <p>Dropout rate.</p> <p>Set to 0 to disable dropout.</p>"},{"location":"reference/project/configs/","title":"Configs","text":"<p>All the configuration classes for the project.</p>"},{"location":"reference/project/configs/#project.configs.Config","title":"Config  <code>dataclass</code>","text":"<p>The options required for a run. This dataclass acts as a structure for the Hydra configs.</p> <p>For more info, see https://hydra.cc/docs/tutorials/structured_config/schema/</p>"},{"location":"reference/project/configs/#project.configs.Config.algorithm","title":"algorithm  <code>instance-attribute</code>","text":"<pre><code>algorithm: Any\n</code></pre> <p>Configuration for the algorithm (a LightningModule).</p> <p>It is suggested for this class to accept a <code>datamodule</code> and <code>network</code> as arguments. The instantiated datamodule and network will be passed to the algorithm's constructor.</p> <p>For more info, see the instantiate_algorithm function.</p>"},{"location":"reference/project/configs/#project.configs.Config.datamodule","title":"datamodule  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datamodule: Optional[Any] = None\n</code></pre> <p>Configuration for the datamodule (dataset + transforms + dataloader creation).</p> <p>This should normally create a LightningDataModule. See the MNISTDataModule for an example.</p>"},{"location":"reference/project/configs/#project.configs.Config.trainer","title":"trainer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trainer: dict = field(default_factory=dict)\n</code></pre> <p>Configuration for the 'Trainer'.</p>"},{"location":"reference/project/configs/#project.configs.Config.log_level","title":"log_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_level: str = 'info'\n</code></pre> <p>Logging level.</p>"},{"location":"reference/project/configs/#project.configs.Config.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int = field(\n    default_factory=lambda: randint(0, int(100000.0))\n)\n</code></pre> <p>Random seed for reproducibility.</p> <p>If None, a random seed is generated.</p>"},{"location":"reference/project/configs/#project.configs.Config.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'default'\n</code></pre> <p>Name for the experiment.</p>"},{"location":"reference/project/configs/#project.configs.Config.ckpt_path","title":"ckpt_path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ckpt_path: str | None = None\n</code></pre> <p>Path to a checkpoint to load the training state and resume the training run.</p> <p>This is the same as the <code>ckpt_path</code> argument in the <code>lightning.Trainer.fit</code> method.</p>"},{"location":"reference/project/configs/config/","title":"Config","text":""},{"location":"reference/project/configs/config/#project.configs.config.Config","title":"Config  <code>dataclass</code>","text":"<p>The options required for a run. This dataclass acts as a structure for the Hydra configs.</p> <p>For more info, see https://hydra.cc/docs/tutorials/structured_config/schema/</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.algorithm","title":"algorithm  <code>instance-attribute</code>","text":"<pre><code>algorithm: Any\n</code></pre> <p>Configuration for the algorithm (a LightningModule).</p> <p>It is suggested for this class to accept a <code>datamodule</code> and <code>network</code> as arguments. The instantiated datamodule and network will be passed to the algorithm's constructor.</p> <p>For more info, see the instantiate_algorithm function.</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.datamodule","title":"datamodule  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datamodule: Optional[Any] = None\n</code></pre> <p>Configuration for the datamodule (dataset + transforms + dataloader creation).</p> <p>This should normally create a LightningDataModule. See the MNISTDataModule for an example.</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.trainer","title":"trainer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trainer: dict = field(default_factory=dict)\n</code></pre> <p>Configuration for the 'Trainer'.</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.log_level","title":"log_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_level: str = 'info'\n</code></pre> <p>Logging level.</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int = field(\n    default_factory=lambda: randint(0, int(100000.0))\n)\n</code></pre> <p>Random seed for reproducibility.</p> <p>If None, a random seed is generated.</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'default'\n</code></pre> <p>Name for the experiment.</p>"},{"location":"reference/project/configs/config/#project.configs.config.Config.ckpt_path","title":"ckpt_path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ckpt_path: str | None = None\n</code></pre> <p>Path to a checkpoint to load the training state and resume the training run.</p> <p>This is the same as the <code>ckpt_path</code> argument in the <code>lightning.Trainer.fit</code> method.</p>"},{"location":"reference/project/datamodules/","title":"Datamodules","text":"<p>Datamodules (datasets + preprocessing + dataloading)</p> <p>See the <code>lightning.LightningDataModule</code> class for more information.</p>"},{"location":"reference/project/datamodules/datamodule_tests/","title":"Datamodule tests","text":""},{"location":"reference/project/datamodules/datamodule_tests/#project.datamodules.datamodule_tests.DataModuleTests","title":"DataModuleTests","text":"<p>               Bases: <code>Generic[DataModuleType]</code>, <code>ABC</code></p>"},{"location":"reference/project/datamodules/datamodule_tests/#project.datamodules.datamodule_tests.DataModuleTests.datamodule","title":"datamodule","text":"<pre><code>datamodule(dict_config: DictConfig) -&gt; DataModuleType\n</code></pre> <p>Fixture that creates the datamodule instance, given the current Hydra config.</p>"},{"location":"reference/project/datamodules/vision/","title":"Vision","text":""},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule","title":"VisionDataModule","text":"<p>               Bases: <code>LightningDataModule</code>, <code>DataModule[BatchType_co]</code></p> <p>A LightningDataModule for image datasets.</p> <p>(Taken from pl_bolts which is not very well maintained.)</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = ''\n</code></pre> <p>Dataset name.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.dataset_cls","title":"dataset_cls  <code>class-attribute</code>","text":"<pre><code>dataset_cls: type[VisionDataset]\n</code></pre> <p>Dataset class to use.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.dims","title":"dims  <code>instance-attribute</code>","text":"<pre><code>dims: tuple[C, H, W]\n</code></pre> <p>A tuple describing the shape of the data.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.__init__","title":"__init__","text":"<pre><code>__init__(\n    data_dir: str | Path = DATA_DIR,\n    val_split: int | float = 0.2,\n    num_workers: int = NUM_WORKERS,\n    normalize: bool = False,\n    batch_size: int = 32,\n    seed: int = 42,\n    shuffle: bool = True,\n    pin_memory: bool = True,\n    drop_last: bool = False,\n    train_transforms: Callable | None = None,\n    val_transforms: Callable | None = None,\n    test_transforms: Callable | None = None,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | Path</code> <p>Where to save/load the data</p> <code>DATA_DIR</code> <code>val_split</code> <code>int | float</code> <p>Percent (float) or number (int) of samples to use for the validation split</p> <code>0.2</code> <code>num_workers</code> <code>int</code> <p>How many workers to use for loading data</p> <code>NUM_WORKERS</code> <code>normalize</code> <code>bool</code> <p>If true applies image normalize</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>How many samples per batch to load</p> <code>32</code> <code>seed</code> <code>int</code> <p>Random seed to be used for train/val/test splits</p> <code>42</code> <code>shuffle</code> <code>bool</code> <p>If true shuffles the train data every epoch</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If true, the data loader will copy Tensors into CUDA pinned memory before         returning them</p> <code>True</code> <code>drop_last</code> <code>bool</code> <p>If true drops the last incomplete batch</p> <code>False</code> <code>train_transforms</code> <code>Callable | None</code> <p>transformations you can apply to train dataset</p> <code>None</code> <code>val_transforms</code> <code>Callable | None</code> <p>transformations you can apply to validation dataset</p> <code>None</code> <code>test_transforms</code> <code>Callable | None</code> <p>transformations you can apply to test dataset</p> <code>None</code>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data() -&gt; None\n</code></pre> <p>Saves files to data_dir.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.default_transforms","title":"default_transforms  <code>abstractmethod</code>","text":"<pre><code>default_transforms() -&gt; Callable\n</code></pre> <p>Default transform for the dataset.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader(\n    _dataloader_fn: Callable[\n        Concatenate[Dataset, P], DataLoader\n    ] = DataLoader,\n    *args: args,\n    **kwargs: kwargs\n) -&gt; DataLoader\n</code></pre> <p>The train dataloader.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader(\n    _dataloader_fn: Callable[\n        Concatenate[Dataset, P], DataLoader\n    ] = DataLoader,\n    *args: args,\n    **kwargs: kwargs\n) -&gt; DataLoader\n</code></pre> <p>The val dataloader.</p>"},{"location":"reference/project/datamodules/vision/#project.datamodules.vision.VisionDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader(\n    _dataloader_fn: Callable[\n        Concatenate[Dataset, P], DataLoader\n    ] = DataLoader,\n    *args: args,\n    **kwargs: kwargs\n) -&gt; DataLoader\n</code></pre> <p>The test dataloader.</p>"},{"location":"reference/project/datamodules/vision_test/","title":"Vision test","text":""},{"location":"reference/project/datamodules/vision_test/#project.datamodules.vision_test.VisionDataModuleTests","title":"VisionDataModuleTests","text":"<p>               Bases: <code>DataModuleTests[VisionDataModuleType]</code></p> <p>Tests for a datamodule/dataset for vision tasks.</p> <p>This is a simple data regression test for now. For each of the <code>train_dataloader</code>, <code>valid_dataloader</code></p>"},{"location":"reference/project/datamodules/image_classification/","title":"Image classification","text":""},{"location":"reference/project/datamodules/image_classification/cifar10/","title":"Cifar10","text":""},{"location":"reference/project/datamodules/image_classification/cifar10/#project.datamodules.image_classification.cifar10.CIFAR10DataModule","title":"CIFAR10DataModule","text":"<p>               Bases: <code>ImageClassificationDataModule</code></p> <p>.. figure:: https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/01/     Plot-of-a-Subset-of-Images-from-the-CIFAR-10-Dataset.png     :width: 400     :alt: CIFAR-10</p> Specs <ul> <li>10 classes (1 per class)</li> <li>Each image is (3 x 32 x 32)</li> </ul> <p>Standard CIFAR10, train, val, test splits and transforms</p> <p>Transforms::</p> <pre><code>transforms = transform_lib.Compose([\n    transform_lib.ToImage(),\n    transform_lib.ToDtype(torch.float32, scale=True),\n    transform_lib.Normalize(\n        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n    )\n])\n</code></pre> <p>Example::</p> <pre><code>from pl_bolts.datamodules import CIFAR10DataModule\n\ndm = CIFAR10DataModule(PATH)\nmodel = LitModel()\n\nTrainer().fit(model, datamodule=dm)\n</code></pre> <p>Or you can set your own transforms</p> <p>Example::</p> <pre><code>dm.train_transforms = ...\ndm.test_transforms = ...\ndm.val_transforms  = ...\n</code></pre>"},{"location":"reference/project/datamodules/image_classification/cifar10_test/","title":"Cifar10 test","text":""},{"location":"reference/project/datamodules/image_classification/fashion_mnist/","title":"Fashion mnist","text":""},{"location":"reference/project/datamodules/image_classification/fashion_mnist/#project.datamodules.image_classification.fashion_mnist.FashionMNISTDataModule","title":"FashionMNISTDataModule","text":"<p>               Bases: <code>MNISTDataModule</code></p> <p>.. figure:: https://storage.googleapis.com/kaggle-datasets-images/2243/3791/9384af51de8baa77f6320901f53bd26b/dataset-cover.png     :width: 400     :alt: Fashion MNIST</p> Specs <ul> <li>10 classes (1 per type)</li> <li>Each image is (1 x 28 x 28)</li> </ul> <p>Standard FashionMNIST, train, val, test splits and transforms</p> <p>Transforms::</p> <pre><code>mnist_transforms = transform_lib.Compose([\n    transform_lib.ToTensor()\n])\n</code></pre> <p>Example::</p> <pre><code>from pl_bolts.datamodules import FashionMNISTDataModule\n\ndm = FashionMNISTDataModule('.')\nmodel = LitModel()\n\nTrainer().fit(model, datamodule=dm)\n</code></pre>"},{"location":"reference/project/datamodules/image_classification/fashion_mnist_test/","title":"Fashion mnist test","text":""},{"location":"reference/project/datamodules/image_classification/image_classification/","title":"Image classification","text":""},{"location":"reference/project/datamodules/image_classification/image_classification/#project.datamodules.image_classification.image_classification.ImageClassificationDataModule","title":"ImageClassificationDataModule","text":"<p>               Bases: <code>VisionDataModule[ImageBatchType]</code>, <code>ClassificationDataModule[ImageBatchType]</code></p> <p>Lightning data modules for image classification.</p>"},{"location":"reference/project/datamodules/image_classification/image_classification/#project.datamodules.image_classification.image_classification.ImageClassificationDataModule.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes: int\n</code></pre> <p>Number of classes in the dataset.</p>"},{"location":"reference/project/datamodules/image_classification/image_classification/#project.datamodules.image_classification.image_classification.ImageClassificationDataModule.dims","title":"dims  <code>instance-attribute</code>","text":"<pre><code>dims: tuple[C, H, W]\n</code></pre> <p>A tuple describing the shape of the data.</p>"},{"location":"reference/project/datamodules/image_classification/image_classification_test/","title":"Image classification test","text":""},{"location":"reference/project/datamodules/image_classification/image_classification_test/#project.datamodules.image_classification.image_classification_test.ImageClassificationDataModuleTests","title":"ImageClassificationDataModuleTests","text":"<p>               Bases: <code>VisionDataModuleTests[ImageClassificationDataModuleType]</code></p> <p>Tests for a datamodule/dataset for image classification.</p> <p>This is a simple data regression test for now. For each of the <code>train_dataloader</code>, <code>valid_dataloader</code></p>"},{"location":"reference/project/datamodules/image_classification/imagenet/","title":"Imagenet","text":""},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule","title":"ImageNetDataModule","text":"<p>               Bases: <code>ImageClassificationDataModule</code></p> <p>ImageNet datamodule.</p> <p>Extracted from https://github.com/Lightning-Universe/lightning-bolts/blob/master/src/pl_bolts/datamodules/imagenet_datamodule.py - Made this a subclass of VisionDataModule</p> <p>Notes:</p> <ul> <li>train_dataloader uses the train split of imagenet2012 and puts away a portion of it for the validation split.</li> <li>val_dataloader uses the part of the train split of imagenet2012  that was not used for training via     <code>num_imgs_per_val_class</code></li> <li>test_dataloader uses the validation split of imagenet2012 for testing.<ul> <li>TODO: need to pass num_imgs_per_class=-1 for test dataset and split=\"test\".</li> </ul> </li> </ul>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = 'imagenet'\n</code></pre> <p>Dataset name.</p>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule.dataset_cls","title":"dataset_cls  <code>class-attribute</code>","text":"<pre><code>dataset_cls: type[VisionDataset] = ImageNet\n</code></pre> <p>Dataset class to use.</p>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule.dims","title":"dims  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dims: tuple[C, H, W] = (\n    C(3),\n    H(image_size),\n    W(image_size),\n)\n</code></pre> <p>A tuple describing the shape of the data.</p>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule.__init__","title":"__init__","text":"<pre><code>__init__(\n    data_dir: str | Path = DATA_DIR,\n    *,\n    val_split: int | float = 0.01,\n    num_workers: int = NUM_WORKERS,\n    normalize: bool = False,\n    image_size: int = 224,\n    batch_size: int = 32,\n    seed: int = 42,\n    shuffle: bool = True,\n    pin_memory: bool = True,\n    drop_last: bool = False,\n    train_transforms: Callable | None = None,\n    val_transforms: Callable | None = None,\n    test_transforms: Callable | None = None,\n    **kwargs\n)\n</code></pre> <p>Creates an ImageNet datamodule (doesn't load or prepare the dataset yet).</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | Path</code> <p>path to the imagenet dataset file</p> <code>DATA_DIR</code> <code>val_split</code> <code>int | float</code> <p>save <code>val_split</code>% of the training data of each class for validation.</p> <code>0.01</code> <code>image_size</code> <code>int</code> <p>final image size</p> <code>224</code> <code>num_workers</code> <code>int</code> <p>how many data workers</p> <code>NUM_WORKERS</code> <code>batch_size</code> <code>int</code> <p>batch_size</p> <code>32</code> <code>shuffle</code> <code>bool</code> <p>If true shuffles the data every epoch</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If true, the data loader will copy Tensors into CUDA pinned memory before                         returning them</p> <code>True</code> <code>drop_last</code> <code>bool</code> <p>If true drops the last incomplete batch</p> <code>False</code>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule.train_transform","title":"train_transform","text":"<pre><code>train_transform() -&gt; Module\n</code></pre> <p>The standard imagenet transforms.</p> <pre><code>transforms.Compose([\n    transforms.RandomResizedCrop(self.image_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n</code></pre>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.ImageNetDataModule.val_transform","title":"val_transform","text":"<pre><code>val_transform() -&gt; Compose\n</code></pre> <p>The standard imagenet transforms for validation.</p> <p>.. code-block:: python</p> <pre><code>transforms.Compose([\n    transforms.Resize(self.image_size + 32),\n    transforms.CenterCrop(self.image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n</code></pre>"},{"location":"reference/project/datamodules/image_classification/imagenet/#project.datamodules.image_classification.imagenet.prepare_imagenet","title":"prepare_imagenet","text":"<pre><code>prepare_imagenet(\n    root: Path,\n    *,\n    split: Literal[\"train\", \"val\"] = \"train\",\n    network_imagenet_dir: Path\n) -&gt; None\n</code></pre> <p>Custom preparation function for ImageNet, using @obilaniu's tar magic in Python form.</p> <p>The core of this is equivalent to these bash commands:</p> <pre><code>mkdir -p $SLURM_TMPDIR/imagenet/val\ncd       $SLURM_TMPDIR/imagenet/val\ntar  -xf /network/scratch/b/bilaniuo/ILSVRC2012_img_val.tar\nmkdir -p $SLURM_TMPDIR/imagenet/train\ncd       $SLURM_TMPDIR/imagenet/train\ntar  -xf /network/datasets/imagenet/ILSVRC2012_img_train.tar          --to-command='mkdir ${TAR_REALNAME%.tar}; tar -xC ${TAR_REALNAME%.tar}'\n</code></pre>"},{"location":"reference/project/datamodules/image_classification/imagenet_test/","title":"Imagenet test","text":""},{"location":"reference/project/datamodules/image_classification/inaturalist/","title":"Inaturalist","text":""},{"location":"reference/project/datamodules/image_classification/inaturalist/#project.datamodules.image_classification.inaturalist.INaturalistDataModule","title":"INaturalistDataModule","text":"<p>               Bases: <code>VisionDataModule</code></p>"},{"location":"reference/project/datamodules/image_classification/inaturalist/#project.datamodules.image_classification.inaturalist.INaturalistDataModule.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = 'inaturalist'\n</code></pre> <p>Dataset name.</p>"},{"location":"reference/project/datamodules/image_classification/inaturalist/#project.datamodules.image_classification.inaturalist.INaturalistDataModule.dataset_cls","title":"dataset_cls  <code>class-attribute</code>","text":"<pre><code>dataset_cls: type[VisionDataset] = INaturalist\n</code></pre> <p>Dataset class to use.</p>"},{"location":"reference/project/datamodules/image_classification/inaturalist/#project.datamodules.image_classification.inaturalist.INaturalistDataModule.dims","title":"dims  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dims: tuple[C, H, W] = (C(3), H(224), W(224))\n</code></pre> <p>A tuple describing the shape of the data.</p>"},{"location":"reference/project/datamodules/image_classification/inaturalist/#project.datamodules.image_classification.inaturalist.INaturalistDataModule.default_transforms","title":"default_transforms","text":"<pre><code>default_transforms() -&gt; Callable\n</code></pre> <p>Default transform for the dataset.</p>"},{"location":"reference/project/datamodules/image_classification/inaturalist_test/","title":"Inaturalist test","text":""},{"location":"reference/project/datamodules/image_classification/mnist/","title":"Mnist","text":""},{"location":"reference/project/datamodules/image_classification/mnist/#project.datamodules.image_classification.mnist.MNISTDataModule","title":"MNISTDataModule","text":"<p>               Bases: <code>ImageClassificationDataModule</code></p> <p>.. figure:: https://miro.medium.com/max/744/1*AO2rIhzRYzFVQlFLx9DM9A.png     :width: 400     :alt: MNIST</p> Specs <ul> <li>10 classes (1 per digit)</li> <li>Each image is (1 x 28 x 28)</li> </ul> <p>Standard MNIST, train, val, test splits and transforms</p> <p>Transforms::</p> <pre><code>mnist_transforms = transform_lib.Compose([\n    transform_lib.ToTensor()\n])\n</code></pre> <p>Example::</p> <pre><code>from pl_bolts.datamodules import MNISTDataModule\n\ndm = MNISTDataModule('.')\nmodel = LitModel()\n\nTrainer().fit(model, datamodule=dm)\n</code></pre>"},{"location":"reference/project/datamodules/image_classification/mnist/#project.datamodules.image_classification.mnist.MNISTDataModule.__init__","title":"__init__","text":"<pre><code>__init__(\n    data_dir: str | Path = DATA_DIR,\n    val_split: int | float = 0.2,\n    num_workers: int = 0,\n    normalize: bool = False,\n    batch_size: int = 32,\n    seed: int = 42,\n    shuffle: bool = True,\n    pin_memory: bool = True,\n    drop_last: bool = False,\n    *args: Any,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | Path</code> <p>Where to save/load the data</p> <code>DATA_DIR</code> <code>val_split</code> <code>int | float</code> <p>Percent (float) or number (int) of samples to use for the validation split</p> <code>0.2</code> <code>num_workers</code> <code>int</code> <p>How many workers to use for loading data</p> <code>0</code> <code>normalize</code> <code>bool</code> <p>If true applies image normalize</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>How many samples per batch to load</p> <code>32</code> <code>seed</code> <code>int</code> <p>Random seed to be used for train/val/test splits</p> <code>42</code> <code>shuffle</code> <code>bool</code> <p>If true shuffles the train data every epoch</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If true, the data loader will copy Tensors into CUDA pinned memory before         returning them</p> <code>True</code> <code>drop_last</code> <code>bool</code> <p>If true drops the last incomplete batch</p> <code>False</code>"},{"location":"reference/project/datamodules/image_classification/mnist_test/","title":"Mnist test","text":""},{"location":"reference/project/datamodules/text/","title":"Text","text":""},{"location":"reference/project/datamodules/text/text_classification/","title":"Text classification","text":"<p>Example algorithm that can train a huggingface model.</p> <p>Also check out this link for more detailed example script:</p> <p>https://github.com/lebrice/mila-docs/blob/llm_training/docs/examples/distributed/LLM_training/main.py</p>"},{"location":"reference/project/datamodules/text/text_classification/#project.datamodules.text.text_classification.TextClassificationDataModule","title":"TextClassificationDataModule","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning data module for HF text classification datasets.</p> <p>This is based on this tutorial: https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/text-transformers.html</p>"},{"location":"reference/project/datamodules/text/text_classification_test/","title":"Text classification test","text":""},{"location":"reference/project/trainers/","title":"Trainers","text":"<p>Trainers: Responsible for running the training loop.</p>"},{"location":"reference/project/trainers/jax_trainer/","title":"Jax trainer","text":"<p>A simplified version of the <code>lightning.Trainer</code> with a fully jitted training loop.</p> <p>This is used by the <code>JaxRLExample</code> algorithm (PPO) in the <code>jax_ppo.py</code> module.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.Ts","title":"Ts  <code>module-attribute</code>","text":"<pre><code>Ts = TypeVar('Ts', bound=PyTreeNode, default=PyTreeNode)\n</code></pre> <p>Type Variable for the training state.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxModule","title":"JaxModule","text":"<p>               Bases: <code>Protocol[Ts, _B, _MetricsT]</code></p> <p>A protocol for algorithms that can be trained by the <code>JaxTrainer</code>.</p> <p>The <code>JaxRLExample</code> is an example that follows this structure and can be trained with a <code>JaxTrainer</code>.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxModule.init_train_state","title":"init_train_state","text":"<pre><code>init_train_state(rng: PRNGKey) -&gt; Ts\n</code></pre> <p>Create the initial training state.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxModule.get_batch","title":"get_batch","text":"<pre><code>get_batch(ts: Ts, batch_idx: int) -&gt; tuple[Ts, _B]\n</code></pre> <p>Produces a batch of data.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxModule.training_step","title":"training_step","text":"<pre><code>training_step(\n    batch_idx: int, ts: Ts, batch: _B\n) -&gt; tuple[Ts, PyTreeNode]\n</code></pre> <p>Update the training state using a \"batch\" of data.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxModule.eval_callback","title":"eval_callback","text":"<pre><code>eval_callback(ts: Ts) -&gt; _MetricsT\n</code></pre> <p>Perform evaluation and return metrics.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer","title":"JaxTrainer","text":"<p>               Bases: <code>PyTreeNode</code></p> <p>A simplified version of the <code>lightning.Trainer</code> with a fully jitted training loop.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer--assumptions","title":"Assumptions:","text":"<ul> <li>The algo object must match the <code>JaxModule</code> protocol (in other words, it should implement its   methods).</li> </ul>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer--training-loop","title":"Training loop","text":"<p>This is the training loop, which is fully jitted:</p> <pre><code>ts = algo.init_train_state(rng)\n\nsetup(\"fit\")\non_fit_start()\non_train_start()\n\neval_metrics = []\nfor epoch in range(self.max_epochs):\n    on_train_epoch_start()\n\n    for step in range(self.training_steps_per_epoch):\n\n        batch = algo.get_batch(ts, step)\n\n        on_train_batch_start()\n\n        ts, metrics = algo.training_step(step, ts, batch)\n\n        on_train_batch_end()\n\n    on_train_epoch_end()\n\n    # Evaluation \"loop\"\n    on_validation_epoch_start()\n    epoch_eval_metrics = self.eval_epoch(ts, epoch, algo)\n    on_validation_epoch_start()\n\n    eval_metrics.append(epoch_eval_metrics)\n\nreturn ts, eval_metrics\n</code></pre>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer--caveats","title":"Caveats","text":"<ul> <li>Some lightning callbacks can be used with this trainer and work well, but not all of them.</li> <li>You can either use Regular pytorch-lightning callbacks, or use <code>jax.vmap</code> on the <code>fit</code> method,   but not both.</li> <li>If you want to use jax.vmap on the <code>fit</code> method, just remove the callbacks on the     Trainer for now.</li> </ul>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer--todos-ideas","title":"TODOs / ideas","text":"<ul> <li>Add a checkpoint callback with orbax-checkpoint?</li> </ul>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer.is_global_zero","title":"is_global_zero  <code>property</code>","text":"<pre><code>is_global_zero: bool\n</code></pre> <p>Check if the current process is the global zero process in a distributed setup.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer.fit","title":"fit","text":"<pre><code>fit(\n    algo: JaxModule[Ts, _B, _MetricsT],\n    rng: PRNGKey,\n    train_state: Ts | None = None,\n    skip_initial_evaluation: Static[bool] = False,\n) -&gt; tuple[Ts, _MetricsT]\n</code></pre> <p>Full training loop in pure jax (a lot faster than pytorch-lightning).</p> <p>Some of the lightning Callbacks can be used with this Trainer.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.JaxTrainer.training_step","title":"training_step","text":"<pre><code>training_step(\n    batch_idx: int,\n    ts: Ts,\n    algo: JaxModule[Ts, _B, _MetricsT],\n)\n</code></pre> <p>Training step in pure jax (joined data collection + training).</p> <p>MUCH faster than using pytorch-lightning, but you lose the callbacks and such.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.get_error_from_metrics","title":"get_error_from_metrics","text":"<pre><code>get_error_from_metrics(metrics: Any) -&gt; tuple[str, float]\n</code></pre> <p>Return the 'error' to minimize for hyperparameter optimization from a set of metrics.</p>"},{"location":"reference/project/trainers/jax_trainer/#project.trainers.jax_trainer.hparams_to_dict","title":"hparams_to_dict","text":"<pre><code>hparams_to_dict(algo: PyTreeNode) -&gt; dict\n</code></pre> <p>Convert the learner struct to a serializable dict.</p>"},{"location":"reference/project/utils/","title":"Utils","text":""},{"location":"reference/project/utils/env_vars/","title":"Env vars","text":""},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.SLURM_JOB_ID","title":"SLURM_JOB_ID  <code>module-attribute</code>","text":"<pre><code>SLURM_JOB_ID: int | None = (\n    int(environ[\"SLURM_JOB_ID\"])\n    if \"SLURM_JOB_ID\" in environ\n    else None\n)\n</code></pre> <p>The value of the 'SLURM_JOB_ID' environment variable.</p> <p>See https://slurm.schedmd.com/sbatch.html#OPT_SLURM_JOB_ID.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.SLURM_TMPDIR","title":"SLURM_TMPDIR  <code>module-attribute</code>","text":"<pre><code>SLURM_TMPDIR: Path | None = (\n    Path(environ[\"SLURM_TMPDIR\"])\n    if \"SLURM_TMPDIR\" in environ\n    else (\n        tmp\n        if SLURM_JOB_ID is not None and exists()\n        else None\n    )\n)\n</code></pre> <p>The SLURM temporary directory, the fastest storage available.</p> <ul> <li>Extract your dataset to this directory at the start of your job.</li> <li>Remember to move any files created here to $SCRATCH since everything gets deleted at the end of the job.</li> </ul> <p>See https://docs.mila.quebec/Information.html#slurm-tmpdir for more information.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.SCRATCH","title":"SCRATCH  <code>module-attribute</code>","text":"<pre><code>SCRATCH = (\n    Path(environ[\"SCRATCH\"])\n    if \"SCRATCH\" in environ\n    else None\n)\n</code></pre> <p>Network directory where temporary logs / checkpoints / custom datasets should be saved.</p> <p>Note that this is temporary storage. Files that you wish to be saved long-term should be saved to the <code>ARCHIVE</code> directory.</p> <p>See https://docs.mila.quebec/Information.html#scratch for more information.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.ARCHIVE","title":"ARCHIVE  <code>module-attribute</code>","text":"<pre><code>ARCHIVE = (\n    Path(environ[\"ARCHIVE\"])\n    if \"ARCHIVE\" in environ\n    else None\n)\n</code></pre> <p>Network directory for long-term storage. Only accessible from the login or cpu-only compute nodes.</p> <p>See https://docs.mila.quebec/Information.html#archive for more information.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.NETWORK_DATASETS_DIR","title":"NETWORK_DATASETS_DIR  <code>module-attribute</code>","text":"<pre><code>NETWORK_DATASETS_DIR = (\n    Path(environ[\"NETWORK_DIR\"])\n    if \"NETWORK_DIR\" in environ\n    else (\n        _mila_network_datasets_dir\n        if exists()\n        else (\n            _drac_network_datasets_dir if exists() else None\n        )\n    )\n)\n</code></pre> <p>The (read-only) network directory that contains pre-downloaded datasets.</p> <p>When running outside of the mila/DRAC clusters, this will be <code>None</code>, but can be mocked by setting the <code>NETWORK_DIR</code> environment variable.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.REPO_ROOTDIR","title":"REPO_ROOTDIR  <code>module-attribute</code>","text":"<pre><code>REPO_ROOTDIR = parent\n</code></pre> <p>The root directory of this repository on this machine.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.DATA_DIR","title":"DATA_DIR  <code>module-attribute</code>","text":"<pre><code>DATA_DIR = Path(\n    get(\n        \"DATA_DIR\",\n        SLURM_TMPDIR or SCRATCH or REPO_ROOTDIR / \"data\",\n    )\n)\n</code></pre> <p>Local Directory where datasets should be extracted on this machine.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.torchvision_dir","title":"torchvision_dir  <code>module-attribute</code>","text":"<pre><code>torchvision_dir: Path | None = None\n</code></pre> <p>Network directory with torchvision datasets.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.NUM_WORKERS","title":"NUM_WORKERS  <code>module-attribute</code>","text":"<pre><code>NUM_WORKERS = int(\n    get(\n        \"SLURM_CPUS_PER_TASK\",\n        get(\n            \"SLURM_CPUS_ON_NODE\",\n            (\n                len(sched_getaffinity(0))\n                if hasattr(os, \"sched_getaffinity\")\n                else cpu_count()\n            ),\n        ),\n    )\n)\n</code></pre> <p>Default number of workers to be used by dataloaders, based on the number of CPUs and/or tasks.</p>"},{"location":"reference/project/utils/env_vars/#project.utils.env_vars.get_constant","title":"get_constant","text":"<pre><code>get_constant(*names: str)\n</code></pre> <p>Resolver for Hydra to get the value of a constant in this file.</p>"},{"location":"reference/project/utils/hydra_utils/","title":"Hydra utils","text":"<p>Utility functions related to working with Hydra.</p>"},{"location":"reference/project/utils/hydra_utils/#project.utils.hydra_utils.get_attr","title":"get_attr","text":"<pre><code>get_attr(obj: Any, *attributes: str)\n</code></pre> <p>Recursive version of <code>getattr</code> when the attribute is like 'a.b.c'.</p>"},{"location":"reference/project/utils/hydra_utils/#project.utils.hydra_utils.register_instance_attr_resolver","title":"register_instance_attr_resolver","text":"<pre><code>register_instance_attr_resolver(\n    instantiated_objects_cache: dict[str, Any]\n) -&gt; None\n</code></pre> <p>Registers the <code>instance_attr</code> custom resolver with OmegaConf.</p>"},{"location":"reference/project/utils/hydra_utils/#project.utils.hydra_utils.resolve_dictconfig","title":"resolve_dictconfig","text":"<pre><code>resolve_dictconfig(dict_config: DictConfig) -&gt; Config\n</code></pre> <p>Resolve all interpolations in the <code>DictConfig</code>.</p> <p>Returns a <code>Config</code> object, which is a simple dataclass used to give nicer type hints for the contents of an experiment config.</p>"},{"location":"reference/project/utils/hydra_utils/#project.utils.hydra_utils.instance_attr","title":"instance_attr","text":"<pre><code>instance_attr(\n    *attributes: str,\n    _instantiated_objects_cache: (\n        MutableMapping[str, Any] | None\n    ) = None\n)\n</code></pre> <p>Allows interpolations of the instantiated objects attributes (rather than configs).</p> <p>This is very hacky</p> <p>This is quite hacky and very dependent on the code of Hydra / OmegaConf not changing too much in the future. For this reason, consider pinning the versions of these libraries in your project if you intend do use this feature.</p> <p>This works during a call to <code>hydra.utils.instantiate</code>, by looking at the stack trace to find the instantiated objects, which are in a variable in that function.</p> <p>If there is a <code>${instance_attr:datamodule.num_classes}</code> interpolation in a config, this will:</p> <ol> <li>instantiate the <code>datamodule</code> config</li> <li> <p>store it at the key <code>'datamodule'</code> in the instantiated objects cache dict (if passed).</p> <p>(This is useful since it makes it possible for us to later reuse this instantiated datamodule instead of re-instantiating it.)</p> </li> <li> <p>Retrieve the value of the attribute (<code>getattr(datamodule, 'num_classes')</code>) and return it.</p> </li> </ol>"},{"location":"reference/project/utils/hydra_utils/#project.utils.hydra_utils.make_config_and_store","title":"make_config_and_store","text":"<pre><code>make_config_and_store(\n    target: Callable[..., Target],\n    *,\n    store: ZenStore,\n    **overrides\n)\n</code></pre> <p>Creates a config dataclass for the given target and stores it in the config store.</p> <p>This uses hydra_zen.builds to create the config dataclass and stores it at the name <code>config_name</code>, or <code>target.__name__</code>.</p>"},{"location":"reference/project/utils/remote_launcher_plugin/","title":"Remote launcher plugin","text":""},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.PatchedSlurmQueueConf","title":"PatchedSlurmQueueConf  <code>dataclass</code>","text":"<p>               Bases: <code>_AddedArgumentsConf</code>, <code>SlurmQueueConf</code></p> <p>Adds more SLURM parameters to the config for the SLURM submitit launcher of Hydra.</p>"},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.PatchedSlurmQueueConf.signal_delay_s","title":"signal_delay_s  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_delay_s: int = 120\n</code></pre> <p>USR1 signal delay before timeout.</p>"},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.PatchedSlurmQueueConf.max_num_timeout","title":"max_num_timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_num_timeout: int = 0\n</code></pre> <p>Maximum number of retries on job timeout.</p> <p>Change this only after you confirmed your code can handle re-submission by properly resuming from the latest stored checkpoint. check the following for more info on slurm_max_num_timeout https://github.com/facebookincubator/submitit/blob/master/docs/checkpointing.md</p>"},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.PatchedSlurmQueueConf.additional_parameters","title":"additional_parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_parameters: dict[str, Any] = field(\n    default_factory=dict\n)\n</code></pre> <p>Useful to add parameters which are not currently available in the plugin.</p> <p>Eg: {\"mail-user\": \"blublu@fb.com\", \"mail-type\": \"BEGIN\"}</p>"},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.PatchedSlurmQueueConf.array_parallelism","title":"array_parallelism  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>array_parallelism: int = 256\n</code></pre> <p>Maximum number of jobs running in parallel.</p>"},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.PatchedSlurmQueueConf.setup","title":"setup  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setup: list[str] | None = None\n</code></pre> <p>A list of commands to run in sbatch before running srun.</p>"},{"location":"reference/project/utils/remote_launcher_plugin/#project.utils.remote_launcher_plugin.get_slurm_accounts","title":"get_slurm_accounts","text":"<pre><code>get_slurm_accounts(cluster: str) -&gt; list[str]\n</code></pre> <p>Gets the SLURM accounts of the user using sacctmgr on the slurm cluster.</p>"},{"location":"reference/project/utils/remote_launcher_plugin_test/","title":"Remote launcher plugin test","text":""},{"location":"reference/project/utils/remote_launcher_plugin_test/#project.utils.remote_launcher_plugin_test.test_can_load_configs","title":"test_can_load_configs","text":"<pre><code>test_can_load_configs(command_line_args: str)\n</code></pre> <p>Test that the cluster and resource configs can be loaded without errors.</p>"},{"location":"reference/project/utils/testutils/","title":"Testutils","text":"<p>Utility functions useful for testing.</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.default_marks_for_config_name","title":"default_marks_for_config_name  <code>module-attribute</code>","text":"<pre><code>default_marks_for_config_name: dict[\n    str, list[MarkDecorator]\n] = {\n    \"inaturalist\": [\n        slow,\n        needs_network_dataset_dir(\"inat\"),\n    ],\n    \"imagenet\": [slow, needs_network_dataset_dir(\"inat\")],\n    \"vision\": [\n        skip(\n            reason=\"Base class, shouldn't be instantiated.\"\n        )\n    ],\n}\n</code></pre> <p>Dict with some default marks for some configs name.</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.default_marks_for_config_combinations","title":"default_marks_for_config_combinations  <code>module-attribute</code>","text":"<pre><code>default_marks_for_config_combinations: dict[\n    tuple[str, ...], list[MarkDecorator]\n] = {\n    (\"imagenet\", \"fcnet\"): [\n        xfail(\n            reason=\"FcNet shouldn't be applied to the ImageNet datamodule. It can lead to nans in the parameters.\"\n        )\n    ],\n    (\"imagenet\", \"jax_fcnet\"): [\n        xfail(\n            reason=\"FcNet shouldn't be applied to the ImageNet datamodule. It can lead to nans in the parameters.\"\n        )\n    ],\n    (\"imagenet\", \"jax_cnn\"): [\n        xfail(\n            reason=\"todo: parameters contain nans when overfitting on one batch? Maybe we're using too many iterations?\"\n        )\n    ],\n    None: {\n        (resnet_config, mnist_dataset_config): [\n            skip(\n                reason=\"ResNets don't work with MNIST datasets because the image resolution is too small.\"\n            )\n        ]\n        for (\n            resnet_config,\n            mnist_dataset_config,\n        ) in product(\n            get_all_configs_in_group_of_type(\n                \"algorithm/network\", ResNet\n            ),\n            [\"mnist\", \"fashion_mnist\"],\n        )\n    },\n}\n</code></pre> <p>Dict with some default marks to add to tests when some config combinations are present.</p> <p>For example, ResNet networks can't be applied to the MNIST datasets.</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.needs_network_dataset_dir","title":"needs_network_dataset_dir","text":"<pre><code>needs_network_dataset_dir(dataset_name: str | None = None)\n</code></pre> <p>Gives a mark that skips the test if the predownloaded dataset directory is not available.</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.get_target_of_config","title":"get_target_of_config","text":"<pre><code>get_target_of_config(\n    config_group: str,\n    config_name: str,\n    _cs: ConfigStore | None = None,\n) -&gt; Callable\n</code></pre> <p>Returns the class that is to be instantiated by the given config name.</p> <p>In the case of inner dataclasses (e.g. Model.HParams), this returns the outer class (Model).</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.get_all_configs_in_group_of_type","title":"get_all_configs_in_group_of_type","text":"<pre><code>get_all_configs_in_group_of_type(\n    config_group: str,\n    config_target_type: type | tuple[type, ...],\n    include_subclasses: bool = True,\n    excluding: type | tuple[type, ...] = (),\n) -&gt; list[str]\n</code></pre> <p>Returns the names of all the configs in the given config group that have this target or a subclass of it.</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.run_for_all_configs_of_type","title":"run_for_all_configs_of_type","text":"<pre><code>run_for_all_configs_of_type(\n    config_group: str,\n    target_type: type,\n    excluding: type | tuple[type, ...] = (),\n)\n</code></pre> <p>Parametrizes a test to run with all the configs in the given group that have targets which are subclasses of the given type.</p> <p>For example:</p> <pre><code>@run_for_all_configs_of_type(\"algorithm\", torch.nn.Module)\ndef test_something_about_the_algorithm(algorithm: torch.nn.Module):\n    ''' This test will run with all the configs in the 'algorithm' group that create nn.Modules! '''\n</code></pre> <p>Concretely, this works by indirectly parametrizing the <code>f\"{config_group}_config\"</code> fixture. To learn more about indirect parametrization in PyTest, take a look at https://docs.pytest.org/en/stable/example/parametrize.html#indirect-parametrization</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.parametrize_when_used","title":"parametrize_when_used","text":"<pre><code>parametrize_when_used(\n    arg_name_or_fixture: str | Callable,\n    values: list,\n    indirect: bool | None = None,\n) -&gt; MarkDecorator\n</code></pre> <p>Fixture that applies <code>pytest.mark.parametrize</code> only when the argument is used (directly or indirectly).</p> <p>When <code>pytest.mark.parametrize</code> is applied to a class, all test methods in that class need to use the parametrized argument, otherwise an error is raised. This function exists to work around this and allows writing test methods that don't use the parametrized argument.</p> <p>For example, this works, but would not be possible with <code>pytest.mark.parametrize</code>:</p> <pre><code>import pytest\n\n@parametrize_when_used(\"value\", [1, 2, 3])\nclass TestFoo:\n    def test_foo(self, value):\n        ...\n\n    def test_bar(self, value):\n        ...\n\n    def test_something_else(self):  # This will cause an error!\n        pass\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>arg_name_or_fixture</code> <code>str | Callable</code> <p>The name of the argument to parametrize, or a fixture to parametrize             indirectly.</p> required <code>values</code> <code>list</code> <p>The values to be used to parametrize the test.</p> required <p>Returns:</p> Type Description <code>MarkDecorator</code> <p>A <code>pytest.MarkDecorator</code> that parametrizes the test with the given values only when the             argument is used (directly or indirectly) by the test.</p>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.run_for_all_configs_in_group","title":"run_for_all_configs_in_group","text":"<pre><code>run_for_all_configs_in_group(\n    group_name: str,\n    config_name_to_marks: (\n        Mapping[str, MarkDecorator | list[MarkDecorator]]\n        | None\n    ) = None,\n)\n</code></pre> <p>Apply this marker to a test to make it run with all configs in a given group.</p> <p>This assumes that a \"<code>group_name</code>_config\" fixture is defined, for example, <code>algorithm_config</code>, <code>datamodule_config</code>, <code>network_config</code>. This then does an indirect parametrization of that fixture, so that it receives the config name as a parameter and returns it.</p> <p>The test wrapped test will uses all config from that group if they are used either as an input argument to the test function or if it the input argument to a fixture function.</p> <p>Parameters:</p> Name Type Description Default <code>group_name</code> <code>str</code> <p>List of datamodule names to use for tests.             By default, lists out the generic datamodules (the datamodules that aren't specific             to a single algorithm, for example the InfGendatamodules of WakeSleep.)</p> required <code>config_name_to_marks</code> <code>Mapping[str, MarkDecorator | list[MarkDecorator]] | None</code> <p>Dictionary from config names to pytest marks (e.g.             <code>pytest.mark.xfail</code>, <code>pytest.mark.skip</code>) to use for that particular config.</p> <code>None</code>"},{"location":"reference/project/utils/testutils/#project.utils.testutils.total_vram_gb","title":"total_vram_gb","text":"<pre><code>total_vram_gb() -&gt; float\n</code></pre> <p>Returns the total VRAM in GB.</p>"},{"location":"reference/project/utils/utils/","title":"Utils","text":""},{"location":"reference/project/utils/utils/#project.utils.utils.print_config","title":"print_config","text":"<pre><code>print_config(\n    config: DictConfig,\n    print_order: Sequence[str] = (\n        \"algorithm\",\n        \"datamodule\",\n        \"trainer\",\n    ),\n    resolve: bool = True,\n) -&gt; None\n</code></pre> <p>Prints content of DictConfig using Rich library and its tree structure.</p> <p>TAKEN FROM https://github.com/ashleve/lightning-hydra-template/blob/6a92395ed6afd573fa44dd3a054a603acbdcac06/src/utils/__init__.py#L56</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>Configuration composed by Hydra.</p> required <code>print_order</code> <code>Sequence[str]</code> <p>Determines in what order config components are printed.</p> <code>('algorithm', 'datamodule', 'trainer')</code> <code>resolve</code> <code>bool</code> <p>Whether to resolve reference fields of DictConfig.</p> <code>True</code>"},{"location":"reference/project/utils/typing_utils/","title":"Typing utils","text":"<p>Utilities to help annotate the types of values in the project.</p>"},{"location":"reference/project/utils/typing_utils/#project.utils.typing_utils.HydraConfigFor","title":"HydraConfigFor  <code>module-attribute</code>","text":"<pre><code>HydraConfigFor = Builds[type[T]]\n</code></pre> <p>Type annotation to say \"a hydra config that returns an object of type T when instantiated\".</p>"},{"location":"reference/project/utils/typing_utils/#project.utils.typing_utils.DataModule","title":"DataModule","text":"<p>               Bases: <code>Protocol[BatchType]</code></p> <p>Protocol that shows the minimal attributes / methods of the <code>LightningDataModule</code> class.</p> <p>This is used to type hint the batches that are yielded by the DataLoaders.</p>"},{"location":"reference/project/utils/typing_utils/#project.utils.typing_utils.is_sequence_of","title":"is_sequence_of","text":"<pre><code>is_sequence_of(\n    object: Any, item_type: type[V] | tuple[type[V], ...]\n) -&gt; TypeGuard[Sequence[V]]\n</code></pre> <p>Used to check (and tell the type checker) that <code>object</code> is a sequence of items of this type.</p>"},{"location":"reference/project/utils/typing_utils/#project.utils.typing_utils.is_mapping_of","title":"is_mapping_of","text":"<pre><code>is_mapping_of(\n    object: Any, key_type: type[K], value_type: type[V]\n) -&gt; TypeGuard[Mapping[K, V]]\n</code></pre> <p>Used to check (and tell the type checker) that <code>object</code> is a mapping with keys and values of the given types.</p>"},{"location":"reference/project/utils/typing_utils/protocols/","title":"Protocols","text":""},{"location":"reference/project/utils/typing_utils/protocols/#project.utils.typing_utils.protocols.Module","title":"Module","text":"<p>               Bases: <code>Protocol[P, OutT]</code></p> <p>Small protocol that can be used to annotate the input/output types of <code>torch.nn.Module</code>s.</p>"},{"location":"reference/project/utils/typing_utils/protocols/#project.utils.typing_utils.protocols.DataModule","title":"DataModule","text":"<p>               Bases: <code>Protocol[BatchType]</code></p> <p>Protocol that shows the minimal attributes / methods of the <code>LightningDataModule</code> class.</p> <p>This is used to type hint the batches that are yielded by the DataLoaders.</p>"},{"location":"reference/project/utils/typing_utils/protocols/#project.utils.typing_utils.protocols.ClassificationDataModule","title":"ClassificationDataModule","text":"<p>               Bases: <code>DataModule[BatchType]</code>, <code>Protocol</code></p> <p>Protocol that matches \"datamodules with a 'num_classes' int attribute.</p>"}]}