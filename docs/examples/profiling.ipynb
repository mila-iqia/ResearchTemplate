{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mila Research Template leverages built-in PyTorch and Lightning functionality to make model profiling and benchmarking accessible and flexible.  \n",
    "Make sure to read the Mila Docs page on profiling before going through this example.  \n",
    "[PLACEHOLDER - Profiling](https://docs.mila.quebec/) . \n",
    "\n",
    "The research template profiling notebook extends the examples in the official documentation with additional tools: notably, native WandB integration to monitor performance and using hydra multiruns to compare the available GPUs on the official Mila cluster. See below. The goal of this notebook is to introduce profiling, present tools useful for doing so and to provide general concepts and guidelines for optimizing your code, within the Mila cluster ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Set the working directory to the project root\n",
    "notebook_path = Path().resolve()  \n",
    "project_root = notebook_path.parent.parent\n",
    "os.chdir(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a deep learning researcher, training comparatively slow models as opposed to faster, optimized ones can greatly influence your research output. In addition, being a user of a shared cluster, being efficient about the use of institutional resources is a benefit to all the users in the ecosystem. Given the ample variety of available resources and training schemes to achieve the same modeling objective, optimizing your code isn't necessarily a straightforward task. \n",
    "\n",
    "While there's many costs involved in getting a model to train, some are more relevant than others when it comes to making your code more efficient. Setting a performance baseline, by observing said costs and identifying underperforming components in the code while properly contextualizing them within a broader training scheme is the very first step to optimizing your code. Once a baseline performance expectation is set, we can modify and observe our code's performance in a comparative manner to then determine if the performed optimizations are better. A profiler can help us in this endeavor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a profiler and what is it good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A profiler is a tool that allows you to measure the time and memory consumption of the modelâ€™s operators. Specifically, the PyTorch profiler output provides clues about operations relevant to model training. Examples include the total amount of time spent doing low-level mathematical operations in the GPU, and whether these are unexpectedly slow or take a disproportionate amount of time, indicating they should be avoided or optimized. Identifying problematic operations can greatly help us validate or rethink our baseline model performance expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting baseline model performance expectations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model size\n",
    "# https://discuss.pytorch.org/t/finding-model-size/130275/2\n",
    "model = ???\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MBU = \n",
    "\\frac{\\# \\text{ Params} \\cdot \\text{bytes per param} \\cdot \\text{tokens per second}}{\\text{Memory Bandwidth}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying potential bottleneck sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a bottleneck is not necessarily straightforward or clear from the start. A sensible first step is to determine whether a potential slowdown originates from data loading or model computation. Querying both ends of the process can be done to determine whether the master process has a significant stall when fetching the next batch, or not. \n",
    "If it's close to 0, then data loading outpaces compute, and compute is the bottleneck. \n",
    "If it's much greater than 0, then compute outpaces data loading, and data loading is the bottleneck. \n",
    "You might not care about CPU usage by the master process and data loaders, so long as the GPU remains fully utilized. \n",
    "Nonetheless, a profiler may record that anyways. How do you look out for relevant stuff? Here are a few ideas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexerNoViableAltException: \\\n",
      "                           ^\n",
      "See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "!python project/main.py \\\n",
    "    algorithm=no_op \\\n",
    "    datamodule=imagenet \\\n",
    "    ++hydra=profiling_multirun \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.limit_train_batches=30 \\\n",
    "    ++trainer.limit_val_batches=30 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python project/main.py \\\n",
    "    algorithm=example \\\n",
    "    datamodule=imagenet \\\n",
    "    ++logger=wandb \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.limit_train_batches=30 \\\n",
    "    ++trainer.limit_val_batches=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for throughput across GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Mila Research template is built with hydra as a configuration manager, it integrates [Multi-runs](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run/) by default. This makes it possible to sweep over different parameters for profiling or throughput testing purposes or both. For example, suppose we wanted to figure out how different GPUs perform relative to each other.  \n",
    "[Mila's official documentation](https://docs.mila.quebec/Information.html) has a comprehensive rundown of the GPUs that are installed on the cluster. Typing ```savail``` on the command line, when logged into the cluster, shows their current availability. Testing their capacity can yield insights into their suitability for different training cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU               Avail / Total \n",
      "===============================\n",
      "2g.20gb              12 / 48 \n",
      "3g.40gb               0 / 48 \n",
      "4g.40gb               0 / 24 \n",
      "a100                  0 / 32 \n",
      "a100l                 0 / 88 \n",
      "a6000                 0 / 8 \n",
      "rtx8000               8 / 408 \n",
      "v100                  2 / 56 \n"
     ]
    }
   ],
   "source": [
    "!savail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these jobs are part of the cluster, [Submitit](https://hydra.cc/docs/plugins/submitit_launcher/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the following prominent GPU classes: a100, a100l, a6000, rtx8000, v100 and MiG partitions with sizes 2g.20gb, 3g.40gb, 4g.40gb.  \n",
    "We will now proceed to specify different GPUs over training runs and compare their throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an example of a sweep over some parameters, \n",
    "# with the training throughput as the metric, \n",
    "# :: callbacks/samples_per_second, \n",
    "# or add a devicestatsmonitor in\n",
    "# and using different kinds of GPUs. \n",
    "\n",
    "## salloc --gres=gpu:a100:1 -c 6 --mem=32G -t 48:00:00 --partition=unkillable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sense of the former: if a GPU with lower maximum capacity is readily available, training on it may be more time and resource effective than waiting for higher capacity GPUs to become available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging with Weights & Biases (wandb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mila Research template integrates wandb functionality as a logger specification.   \n",
    "This has the advantage of being able to track additional metrics and create accompanying visualizations.  \n",
    "We will now create a wandb report comparing throughput between GPUs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a wandb report with the throughput comparison \n",
    "# between the different GPU types.\n",
    "# i.e. specify wandb as the logger and log the throughput\n",
    "!python project/main.py \\\n",
    "    algorithm=no_op \\\n",
    "    datamodule=imagenet \\\n",
    "    ++logger=wandb \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.limit_train_batches=30 \\\n",
    "    ++trainer.limit_val_batches=30 \\\n",
    "    hydra=profiling_multirun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to maximize our throughput given GPU choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the best datamodule parameters to maximize the throughput \n",
    "## (batches per second) without training (NoOP algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measure the performance on different GPUS using the optimal datamodule \n",
    "### params from before (and keeping other parameters the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now sweep over model hyper-parameters to maximize the utilization of our selected GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using the results from before, do a simple sweep over model hyper-parameters \n",
    "#### to maximize the utilization of the selected GPU (which was selected as a tradeoff \n",
    "#### between performance and difficulty to get an allocation). For example if the \n",
    "#### RTX8000's are 20% slower than A100s but 5x easier to get an allocation on, use those instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "[GPU Training (Basic) - LightningAI](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html)  \n",
    "[DeviceStatsMonitor class - LightningAI](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.DeviceStatsMonitor.html)  \n",
    "[PyTorch Profiler + W&B integration - Weights & Biases](https://wandb.ai/wandb/trace/reports/Using-the-PyTorch-Profiler-with-W-B--Vmlldzo5MDE3NjU)   \n",
    "[Advanced profiling for model optimization - Accelerating Generative AI with PyTorch: Segment Anything, Fast](https://pytorch.org/blog/accelerating-generative-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchtemplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
