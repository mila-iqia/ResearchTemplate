{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mila Research Template leverages built-in PyTorch and Lightning functionality to make model profiling and benchmarking accessible and flexible.  \n",
    "Make sure to read the Mila Docs page on [PLACEHOLDER - profiling](https://docs.mila.quebec/) before going through this example. \n",
    "\n",
    "The Research Template's profiling notebook extends the examples in the official documentation with additional tools: notably, native WandB integration to monitor performance and using hydra multiruns to compare the available GPUs on the official Mila cluster. See below. The goal of this notebook is to introduce profiling, present tools useful for doing so and to provide general concepts and guidelines for optimizing your code, within the Mila cluster ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Set the working directory to the project root\n",
    "notebook_path = Path().resolve()  \n",
    "project_root = notebook_path.parent.parent\n",
    "os.chdir(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a deep learning researcher, training comparatively slow models as opposed to faster, optimized ones can greatly impact your research output. In addition, as a user of a shared cluster, being efficient about the use of institutional resources is a benefit to all the users in the ecosystem. Given the ample variety of available resources and training schemes to achieve the same modeling objective, optimizing your code isn't necessarily a straightforward task. \n",
    "\n",
    "While there's many costs involved in getting a model to train, some are more relevant than others when it comes to making your code more efficient. Setting a performance baseline, by observing said costs and identifying underperforming components in the code while properly contextualizing them within a broader training scheme is the very first step to optimizing your code. Once a baseline performance expectation is set, we can modify and observe our code's performance in a comparative manner to then determine if the performed optimizations are better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumenting your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up artifacts within your code to monitor metrics of interest can help set a cost baseline and evidence potential areas for improvement. Common metrics to watch for include but are not limited to:\n",
    " \n",
    "- Training speed (samples/s)\n",
    "- CPU/GPU utilization \n",
    "- RAM/VRAM utilization\n",
    "\n",
    "In the Mila Research Template, this can be done by passing a callback to the trainer. Supported configs are found within the project template at `configs/trainer/callbacks`. Throughout this tutorial, we will use the default callback, which in turn implements early stopping and tracks the learning rate, device utilisation and throughput, each through a specific callback instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a baseline and logging metrics on WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to specifying callbacks, the Mila Research template integrates using WandB as a logger, which enables the tracking of additional metrics through visualizations and dashboard creation. Given the flexibility and widespread adoption of the WandB logger, we'll be using it for the remainder of this tutorial, which will then be visualizable at `wandb_url` , as supporting information for the experiments contained herein. We will now proceed to establish a baseline to profile, diagnose and optimize throughout this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    trainer.logger.wandb.name=\"1 RTX8000 GPU 1 CPU Training - ResNet-18 - ImageNet\" \\\n",
    "    trainer.logger.wandb.tags=[\"Training baseline\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying potential bottleneck sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding bottlenecks in your code is not necessarily clear or straightforward from the start. A sensible first step is to determine whether potential slowdowns originate from data loading or model computation. Running a model with and without training and contrasting the obtained outputs can help us determine whether the master process has a significant stall when fetching the next batch for training or not. Analyzing the difference between outputs can tell us the following about our model: \n",
    "\n",
    "- If the difference between data loading and training is close to 0, then the data loading procedure outpaces model computation, and computation is the bottleneck. \n",
    "- If the difference between data loading and training is much greater than 0, then model computation outpaces data loading, and data loading is the bottleneck. \n",
    "\n",
    "We will proceed to run a series of experiments to identify potential bottlenecks: changing the workers involved in the dataloading process and the numbers of cpu assigned per task when training on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    algorithm=no_op \\\n",
    "    resources=cpu \\\n",
    "    trainer.logger.wandb.tags=[\"1 CPU Dataloading\"] \\\n",
    "    hydra.launcher.cpus_per_task=1 \\\n",
    "    datamodule.num_workers=1,4,8,16,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    algorithm=no_op \\\n",
    "    resources=cpu \\\n",
    "    trainer.logger.wandb.tags=[\"2 CPU Dataloading\"] \\\n",
    "    hydra.launcher.cpus_per_task=2 \\\n",
    "    datamodule.num_workers=1,4,8,16,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    algorithm=no_op \\\n",
    "    resources=cpu \\\n",
    "    trainer.logger.wandb.tags=[\"3 CPU Dataloading\"] \\\n",
    "    hydra.launcher.cpus_per_task=3 \\\n",
    "    datamodule.num_workers=1,4,8,16,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    algorithm=no_op \\\n",
    "    resources=cpu \\\n",
    "    trainer.logger.wandb.tags=[\"4 CPU Dataloading\"] \\\n",
    "    hydra.launcher.cpus_per_task=4 \\\n",
    "    datamodule.num_workers=1,4,8,16,32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've determined the optimal number of workers and CPUs in terms of dataloading throughput, we can train a model similar to our baseline, albeit with the newly obtained parameters, to then compare throughput and determine if there was a sizeable increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture ----- RUN WITH OPTIMAL PARAMETERS ONCE DETERMINED, LOCALLY -----\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    trainer.logger.wandb.name=\"1 RTX8000 GPU 1 CPU Training - ResNet-18 - ImageNet\" \\\n",
    "    trainer.logger.wandb.tags=[\"Optimized\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The advantages of training models with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advancements in Graphical Processing Units (GPUs) are widely known to have enabled the deep learning revolution, particularly through faster computation, relative to CPUs. Given that we have the option to run both GPU and CPU workloads, let's compare their throughput. In most workflows, the speedup provided by a GPU is dramatic. For a few select workloads, particularly those with a low number of steps or lighter computation requirements, if a 1.5-2x slower performance is observed when using a CPU, as opposed to a GPU, the former may be worth considering, as they're a far less contested resource on the cluster and pose far fewer availability issues.  \n",
    "\n",
    "In this section, we'll train a model that's analogous to our ImageNet baseline - entirely on the CPU. We will also train two smaller fully connected networks on MNIST, a smaller dataset than ImageNet, to compare and contrast the differences in throughput when training with and without a GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture ### USE OPTIMIZED NUM CPUS, WORKERS, BEFORE RUNNING\n",
    "!python project/main.py \\\n",
    "    experiment=profiling \\\n",
    "    resources=cpu \\\n",
    "    trainer.logger.wandb.name=\"1 CPU Training - ResNet-18 - ImageNet\" \\\n",
    "    trainer.logger.wandb.tags=[\"CPU Training\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/19/24 15:49:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=854152;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=476561;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m19\u001b[0m/\u001b[1;36m15\u001b[0m-\u001b[1;36m49\u001b[0m-\u001b[1;36m06\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m : \u001b[33mnetwork\u001b[0m=\u001b[35mfcnet\u001b[0m \u001b]8;id=577341;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=695335;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mmnist\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mexperiment\u001b[0m=\u001b[35mprofiling\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m1\u001b[0m\\ RTX8000\\ GPU\\ \u001b[1;36m1\u001b[0m\\ CPU\\   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         Training                   \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    network=fcnet \\\n",
    "    datamodule=mnist \\\n",
    "    experiment=profiling \\\n",
    "    hydra.launcher.gres='gpu:rtx8000:1' \\\n",
    "    ###hydra.launcher.cpus_per_task=1 \\ USE OPTIMIZED NUM CPUS, WORKERS\n",
    "    trainer.logger.wandb.name=\"1 RTX8000 GPU 1 CPU Training - FcNet - MNIST\"\n",
    "    trainer.logger.wandb.tags=[\"GPU Training\", \"MNIST\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture ### USE OPTIMIZED NUM CPUS, WORKERS, BEFORE RUNNING\n",
    "!python project/main.py \\\n",
    "    network=fcnet \\\n",
    "    datamodule=mnist \\\n",
    "    experiment=profiling \\\n",
    "    resources=cpu \\\n",
    "    trainer.logger.wandb.name=\"1 CPU Training - FcNet - MNIST\" \\\n",
    "    trainer.logger.wandb.tags=[\"CPU Training\", \"MNIST\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput across GPU types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the former, we've made a solid case for utilizing GPUs for model training. Furthermore, when using GPUs, these vary in throughput; some are more powerful than others. [Mila's official documentation](https://docs.mila.quebec/Information.html) has a comprehensive rundown of the GPUs that are installed on the cluster. Typing ```savail``` on the command line when logged into the cluster, shows their current availability. Testing their capacity can yield insights into their suitability for different training workloads. Let's see what's available on the Mila cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU               Avail / Total \n",
      "===============================\n",
      "2g.20gb               6 / 48 \n",
      "3g.40gb               3 / 48 \n",
      "4g.40gb               1 / 24 \n",
      "a100                  0 / 32 \n",
      "a100l                 0 / 88 \n",
      "a6000                 0 / 8 \n",
      "rtx8000              14 / 408 \n",
      "v100                  0 / 56 \n"
     ]
    }
   ],
   "source": [
    "!savail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the following prominent GPU classes:\n",
    "\n",
    "- NVIDIA Tensor Core GPUs: A100, A100L, V100 (previous gen)\n",
    "- NVIDIA RTX GPUs: A6000, RTX8000\n",
    "- Multi-Instance GPU (MiG) partitions: 2g.20gb, 3g.40gb, 4g.40gb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Mila Research Template is built with hydra as a configuration manager, it integrates [Multi-runs](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run/) by default. This makes it possible to specify particular GPU resources for a given run, or sweeping over different parameters for profiling or throughput testing purposes or both.  \n",
    "For example, suppose we wanted to figure out how different GPUs perform relative to each other.  \n",
    "We are able to do this by specifying different GPUs over training runs and comparing their throughput. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 13:33:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=246287;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666892;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m13\u001b[0m-\u001b[1;36m33\u001b[0m-\u001b[1;36m03\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=835344;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=966187;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mGPU\u001b[0m\\ -\\ A100               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mGPU\u001b[0m\\ types                \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture ### USE OPTIMIZED NUM CPUS, WORKERS, BEFORE RUNNING\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:a100:1 \\\n",
    "    trainer.logger.wandb.name=\"A100\" \\\n",
    "    datamodule.num_workers=#optimal params as determined before\n",
    "    trainer.logger.wandb.name=\"A100 GPU X CPU X Num_workers - ResNet-18 - ImageNet\" \\\n",
    "    trainer.logger.wandb.tags=[\"GPU Training\", \"ImageNet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture ### USE OPTIMIZED NUM CPUS, WORKERS, BEFORE RUNNING\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:v100:1 \\\n",
    "    trainer.logger.wandb.name=\"V100\" \\\n",
    "    datamodule.num_workers=#optimal param as determined before \n",
    "    trainer.logger.wandb.name=\"V100 GPU X CPU X Num_workers - ResNet-18 - ImageNet\" \\\n",
    "    trainer.logger.wandb.tags=[\"GPU Training\", \"ImageNet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing GPU efficiency and utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a clear difference in throughput between GPU types, if a GPU with lower maximum capacity is readily available, training on it may be more time and resource effective than waiting for higher capacity GPUs to become available. Optimizing a lower capacity GPU may be sufficient for your use case. How well are is a given GPU being utilized? Once we've done a few preliminary runs with candidate GPU configurations that we'd want to use, the GPU utilization can be measured and optimized.  \n",
    "We generally aim for high GPU utilization. Is the GPU utilization high? (>80%)? If it's low (<80%), then we can use the PyTorch profiler (or similar tools) to try to figure out where the bottleneck lies, and further tune our parameters to increase our utilization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 11:50:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=78076;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=898758;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m11\u001b[0m-\u001b[1;36m50\u001b[0m-\u001b[1;36m48\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=612036;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=652546;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mbatch_size\u001b[0m=\u001b[1;36m1\u001b[0m    \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mRTX8000\u001b[0m\\ -\\ batch\\ size\\ \u001b[1;36m1\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mBatch\u001b[0m\\ size\\ tests        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##%%capture ------ PLACEHOLDER: OPTIMAL PARAMETERS FROM SECTION 3 REQUIRED BEFORE RUNNING ------\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:rtx8000:1 \\\n",
    "    datamodule.batch_size=1,8,32,64,128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a profiler and what is it good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The former process, while straightforward, was a bit contrived - would having a bird's eye view of our models performance be of aid when trying to optimize its parameters? It certainly wouldn't hurt. Enter the profiler.  \n",
    "A profiler is a tool that allows you to measure the time and memory consumption of the model’s operators. Specifically, the PyTorch profiler output provides clues about operations relevant to model training. Examples include the total amount of time spent doing low-level mathematical operations in the GPU, and whether these are unexpectedly slow or take a disproportionate amount of time, indicating they should be avoided or optimized. Identifying problematic operations can greatly help us validate or rethink our baseline model performance expectations.\n",
    "\n",
    "[Multiple](https://developer.nvidia.com/blog/profiling-and-optimizing-deep-neural-networks-with-dlprof-and-pyprof/) [profilers](https://github.com/plasma-umass/scalene) [exist](https://docs.python.org/3/library/profile.html). For the purposes of this example we'll use the default [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    cudaDeviceSynchronize       100.00%      14.444us       100.00%      14.444us      14.444us           0 b           0 b             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.444us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import ProfilerActivity, profile \n",
    "\n",
    "profiler = profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    ")\n",
    "profiler.start()\n",
    "profiler.stop()\n",
    "print(profiler.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "[GPU Training (Basic) - LightningAI](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html)  \n",
    "[DeviceStatsMonitor class - LightningAI](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.DeviceStatsMonitor.html)  \n",
    "[PyTorch Profiler + W&B integration - Weights & Biases](https://wandb.ai/wandb/trace/reports/Using-the-PyTorch-Profiler-with-W-B--Vmlldzo5MDE3NjU)   \n",
    "[Advanced profiling for model optimization - Accelerating Generative AI with PyTorch: Segment Anything, Fast](https://pytorch.org/blog/accelerating-generative-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchtemplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
