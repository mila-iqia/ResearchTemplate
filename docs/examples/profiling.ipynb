{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mila Research Template leverages built-in PyTorch and Lightning functionality to make model profiling and benchmarking accessible and flexible.  \n",
    "Make sure to read the Mila Docs page on [PLACEHOLDER - profiling](https://docs.mila.quebec/) before going through this example. \n",
    "\n",
    "The research template profiling notebook extends the examples in the official documentation with additional tools: notably, native WandB integration to monitor performance and using hydra multiruns to compare the available GPUs on the official Mila cluster. See below. The goal of this notebook is to introduce profiling, present tools useful for doing so and to provide general concepts and guidelines for optimizing your code, within the Mila cluster ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Set the working directory to the project root\n",
    "notebook_path = Path().resolve()  \n",
    "project_root = notebook_path.parent.parent\n",
    "os.chdir(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a deep learning researcher, training comparatively slow models as opposed to faster, optimized ones can greatly impact your research output. In addition, as a user of a shared cluster, being efficient about the use of institutional resources is a benefit to all the users in the ecosystem. Given the ample variety of available resources and training schemes to achieve the same modeling objective, optimizing your code isn't necessarily a straightforward task. \n",
    "\n",
    "While there's many costs involved in getting a model to train, some are more relevant than others when it comes to making your code more efficient. Setting a performance baseline, by observing said costs and identifying underperforming components in the code while properly contextualizing them within a broader training scheme is the very first step to optimizing your code. Once a baseline performance expectation is set, we can modify and observe our code's performance in a comparative manner to then determine if the performed optimizations are better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumenting your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up artifacts within your code to monitor metrics of interest can help set a cost baseline and evidence potential areas for improvement. Common metrics to watch for include but are not limited to:\n",
    " \n",
    "- Training speed (samples/s)\n",
    "- CPU/GPU utilization \n",
    "- RAM/VRAM utilization\n",
    "\n",
    "In the Mila ResearchTemplate, this can be done by passing a callback to the trainer. Supported configs are found within the project template at `configs/trainer/callbacks`. Here, we will use the default callback, which in turn implements early stopping and tracks the learning rate, device utilisation and throughput, each through a specific callback instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KCreating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/46\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36m[09/16/24 16:09:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Unable to properly create the    \u001b]8;id=800276;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py\u001b\\\u001b[2mauto_schema.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=394219;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py#368\u001b\\\u001b[2m368\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         schema for                       \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         experiment/cluster_sweep_example \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         .yaml last time. Trying again.   \u001b[2m                  \u001b[0m\n",
      "Creating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/46\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KCreating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/46\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Unable to create a schema for    \u001b]8;id=497869;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py\u001b\\\u001b[2mauto_schema.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=538994;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py#396\u001b\\\u001b[2m396\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         config                           \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         experiment/cluster_sweep_example \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         .yaml: In \u001b[32m'hydra/config'\u001b[0m: Could  \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         not find \u001b[32m'hydra/sweeper/orion'\u001b[0m   \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         Available options in             \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'hydra/sweeper'\u001b[0m:                 \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 basic                    \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         Config search path:              \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mhydra\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mpkg\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhydra.conf\u001b[0m            \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mmain\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mpkg\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mproject.configs\u001b[0m       \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mhydra\u001b[0m-colorlog, \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mpkg\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhydra_plugins.hydra_c\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[95molorlog.conf\u001b[0m                     \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mschema\u001b[0m,         \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mstructured\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m               \u001b[2m                  \u001b[0m\n",
      "Creating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/46\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KCreating schemas for Hydra config files...\u001b[35m 100%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m46/â€¦\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m0:0â€¦\u001b[0m , \u001b[31m7,5â€¦\u001b[0m ]\n",
      "                                                                          \u001b[31mit/s\u001b[0m  \n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36m[09/16/24 16:09:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Updated the yaml schemas in the  \u001b]8;id=35654;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py\u001b\\\u001b[2mauto_schema.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=593628;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py#522\u001b\\\u001b[2m522\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         vscode settings file at          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/mila/c/cesar.valdez/idt/Re\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35msearchTemplate/.vscode/\u001b[0m\u001b[95msettings.\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[95mjson.\u001b[0m                            \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Instantiated the config at       \u001b]8;id=907199;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/hydra_utils.py\u001b\\\u001b[2mhydra_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=326931;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/hydra_utils.py#370\u001b\\\u001b[2m370\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'datamodule'\u001b[0m while trying to     \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         find one of the                  \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0m\u001b[32m'datamodule.num_classes'\u001b[0m, \u001b[1;36m1000\u001b[0m\u001b[1m)\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         attributes.                      \u001b[2m                  \u001b[0m\n",
      "seed manually set to 30897\n",
      "Seed set to 30897\n",
      "/home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python project/main.py algorithm=no_op datamodule=imagenet  ...\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\u001b[2;36m[16:09:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Datamodule was already instantiated        \u001b]8;id=422359;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=276611;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/experiment.py#175\u001b\\\u001b[2m175\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0mprobably to interpolate a field value\u001b[1m)\u001b[0m.   \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[33mdatamodule_config\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mproject.datamodules.ima\u001b[0m \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;95mge_classification.imagenet.ImageNetDataMod\u001b[0m \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;95mule\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fc41bf0de40\u001b[0m\u001b[1m>\u001b[0m              \u001b[2m                 \u001b[0m\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: MeasureSamplesPerSecondCallback\n",
      "\u001b[2;36m[16:09:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Train archive already fully extracted.       \u001b]8;id=765323;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py\u001b\\\u001b[2mimagenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=527622;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py#382\u001b\\\u001b[2m382\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Skipping.                                    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation split already extracted.          \u001b]8;id=300968;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py\u001b\\\u001b[2mimagenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=508269;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py#339\u001b\\\u001b[2m339\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Skipping.                                    \u001b[2m               \u001b[0m\n",
      "^C\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!python project/main.py \\\n",
    "    algorithm=no_op \\\n",
    "    datamodule=imagenet \\\n",
    "    trainer=profiling \\\n",
    "    trainer/callbacks=default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging metrics on WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to callback specification, the Mila Research template integrates wandb as a logger specification, which enables the tracking of additional metrics through visualizations and dashboard creation. Given the flexibility and widespread adoption of using WandB as a logger, we'll be using it for the remainder of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KCreating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/45\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36m[09/16/24 16:31:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Unable to properly create the    \u001b]8;id=965865;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py\u001b\\\u001b[2mauto_schema.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=902936;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py#368\u001b\\\u001b[2m368\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         schema for                       \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         experiment/cluster_sweep_example \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         .yaml last time. Trying again.   \u001b[2m                  \u001b[0m\n",
      "Creating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/45\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KCreating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/45\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Unable to create a schema for    \u001b]8;id=381213;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py\u001b\\\u001b[2mauto_schema.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=757802;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py#396\u001b\\\u001b[2m396\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         config                           \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         experiment/cluster_sweep_example \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         .yaml: In \u001b[32m'hydra/config'\u001b[0m: Could  \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         not find \u001b[32m'hydra/sweeper/orion'\u001b[0m   \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         Available options in             \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'hydra/sweeper'\u001b[0m:                 \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 basic                    \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         Config search path:              \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mhydra\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mpkg\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhydra.conf\u001b[0m            \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mmain\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mpkg\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mproject.configs\u001b[0m       \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mhydra\u001b[0m-colorlog, \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mpkg\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhydra_plugins.hydra_c\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[95molorlog.conf\u001b[0m                     \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                 \u001b[33mprovider\u001b[0m=\u001b[35mschema\u001b[0m,         \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpath\u001b[0m=\u001b[35mstructured\u001b[0m:\u001b[35m/\u001b[0m\u001b[35m/\u001b[0m               \u001b[2m                  \u001b[0m\n",
      "Creating schemas for Hydra config files...\u001b[35m   0%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m0/45\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m-:-â€¦\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KCreating schemas for Hydra config files...\u001b[35m 100%\u001b[0m \u001b[90mâ”â”â”â”\u001b[0m \u001b[32m45/â€¦\u001b[0m [ \u001b[33m0:0â€¦\u001b[0m < \u001b[36m0:0â€¦\u001b[0m , \u001b[31m8,5â€¦\u001b[0m ]\n",
      "                                                                          \u001b[31mit/s\u001b[0m  \n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36m[09/16/24 16:31:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Updated the yaml schemas in the  \u001b]8;id=106742;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py\u001b\\\u001b[2mauto_schema.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=184714;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/auto_schema.py#522\u001b\\\u001b[2m522\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         vscode settings file at          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/mila/c/cesar.valdez/idt/Re\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35msearchTemplate/.vscode/\u001b[0m\u001b[95msettings.\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[95mjson.\u001b[0m                            \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Instantiated the config at       \u001b]8;id=771749;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/hydra_utils.py\u001b\\\u001b[2mhydra_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=511000;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/utils/hydra_utils.py#370\u001b\\\u001b[2m370\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'datamodule'\u001b[0m while trying to     \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         find one of the                  \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0m\u001b[32m'datamodule.num_classes'\u001b[0m, \u001b[1;36m1000\u001b[0m\u001b[1m)\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         attributes.                      \u001b[2m                  \u001b[0m\n",
      "seed manually set to 77069\n",
      "Seed set to 77069\n",
      "/home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python project/main.py experiment=profiling_cpu trainer/log ...\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2;36m[16:31:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Datamodule was already instantiated        \u001b]8;id=903950;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=805041;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/experiment.py#175\u001b\\\u001b[2m175\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0mprobably to interpolate a field value\u001b[1m)\u001b[0m.   \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[33mdatamodule_config\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mproject.datamodules.ima\u001b[0m \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;95mge_classification.imagenet.ImageNetDataMod\u001b[0m \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;95mule\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f40d1f33ac0\u001b[0m\u001b[1m>\u001b[0m              \u001b[2m                 \u001b[0m\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: MeasureSamplesPerSecondCallback\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Train archive already fully extracted.       \u001b]8;id=618681;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py\u001b\\\u001b[2mimagenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=810172;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py#382\u001b\\\u001b[2m382\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Skipping.                                    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation split already extracted.          \u001b]8;id=485875;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py\u001b\\\u001b[2mimagenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=801119;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py#339\u001b\\\u001b[2m339\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Skipping.                                    \u001b[2m               \u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcesar-valdez\u001b[0m (\u001b[33mcesar-valdez-mcgill-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240916_163130-bd8pa1aw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mWandB logging test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cesar-valdez-mcgill-university/ResearchTemplate\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cesar-valdez-mcgill-university/ResearchTemplate/runs/bd8pa1aw\u001b[0m\n",
      "â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName           \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0mâ”‚ network         â”‚ ResNet            â”‚ 11.7 M â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.conv1   â”‚ Conv2d            â”‚  9.4 K â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.bn1     â”‚ BatchNorm2d       â”‚    128 â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.relu    â”‚ ReLU              â”‚      0 â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.maxpool â”‚ MaxPool2d         â”‚      0 â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.layer1  â”‚ Sequential        â”‚  147 K â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.layer2  â”‚ Sequential        â”‚  525 K â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.layer3  â”‚ Sequential        â”‚  2.1 M â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.layer4  â”‚ Sequential        â”‚  8.4 M â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0mâ”‚ network.avgpool â”‚ AdaptiveAvgPool2d â”‚      0 â”‚ train â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0mâ”‚ network.fc      â”‚ Linear            â”‚  513 K â”‚ train â”‚\n",
      "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\u001b[1mTrainable params\u001b[0m: 11.7 M                                                        \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 11.7 M                                                            \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 46                                      \n",
      "\u001b[1mModules in train mode\u001b[0m: 68                                                       \n",
      "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \n",
      "/home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2K\u001b[37mEpoch 0/1 \u001b[0m \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[37m4.45it/s\u001b[0m \u001b[37mv_num: a1aw\u001b[0m37mv_num: a1aw\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[37mEpoch 0/1 \u001b[0m \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[37m4.45it/s\u001b[0m \u001b[37mv_num: a1aw\u001b[0m\n",
      "\u001b[2KEpoch 1/1  \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m0/2\u001b[0m \u001b[37m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[37m0.00it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m7mv_num: a1aw\u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m0/2\u001b[0m \u001b[37m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[37m0.00it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[37m1/2\u001b[0m \u001b[37m0:00:01 â€¢ -:--:--\u001b[0m \u001b[37m0.00it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[37m1/2\u001b[0m \u001b[37m0:00:01 â€¢ -:--:--\u001b[0m \u001b[37m0.00it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m286.046            \u001b[0m`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 1/1  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m2/2\u001b[0m \u001b[37m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[37m2.72it/s\u001b[0m \u001b[37mv_num: a1aw        \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m175.405            \u001b[0m\n",
      "                                                             \u001b[37mtrain/samples_per_â€¦\u001b[0m\n",
      "                                                             \u001b[37m175.405            \u001b[0m\n",
      "\u001b[?25h/home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python project/main.py experiment=profiling_cpu trainer/log ...\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: MeasureSamplesPerSecondCallback\n",
      "\u001b[2;36m[16:31:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Train archive already fully extracted.       \u001b]8;id=817434;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py\u001b\\\u001b[2mimagenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=930994;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py#382\u001b\\\u001b[2m382\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Skipping.                                    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[16:31:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Validation split already extracted.          \u001b]8;id=584010;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py\u001b\\\u001b[2mimagenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=413818;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/datamodules/image_classification/imagenet.py#339\u001b\\\u001b[2m339\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Skipping.                                    \u001b[2m               \u001b[0m\n",
      "\u001b[2Kâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“â”â”â”â”\u001b[0m \u001b[37m1/1\u001b[0m \u001b[37m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[37m0.00it/s\u001b[0m \n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚\u001b[36m \u001b[0m\u001b[36m        val/loss         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.3719692826271057    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\u001b[2K\u001b[37mValidation\u001b[0m \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[37m1/1\u001b[0m \u001b[37m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[37m0.00it/s\u001b[0m \n",
      "\u001b[?25h\u001b[2;36m[16:32:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Assuming that the objective to minimize is the   \u001b]8;id=785781;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/main.py\u001b\\\u001b[2mmain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=487120;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/project/main.py#160\u001b\\\u001b[2m160\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         loss metric.                                     \u001b[2m           \u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 21.7%\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          epoch â–â–â–…â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         lr-SGD â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/samples_per_second_epoch â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            trainer/global_step â–â–ƒâ–ƒâ–…â–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       val/loss â–ˆâ–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          epoch 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         lr-SGD 0.123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/samples_per_second_epoch 175.40546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            trainer/global_step 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       val/loss 0.37197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mWandB logging test\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cesar-valdez-mcgill-university/ResearchTemplate/runs/bd8pa1aw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cesar-valdez-mcgill-university/ResearchTemplate\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240916_163130-bd8pa1aw/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb version 0.18.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    trainer/logger=wandb \\\n",
    "    trainer.logger.wandb.name=\"WandB logging test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the results of our run at `wandb_url`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying potential bottleneck sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding bottlenecks in your code is not necessarily clear or straightforward from the start. A sensible first step is to determine whether potential slowdowns originate from data loading or model computation. Running a model with and without training and contrasting the obtained outputs can help us determine whether the master process has a significant stall when fetching the next batch for training or not. Analyzing the difference between outputs can tell us the following about our model: \n",
    "\n",
    "- If the difference between data loading and training is close to 0, then the data loading procedure outpaces model computation, and computation is the bottleneck. \n",
    "- If the difference between data loading and training is much greater than 0, then model computation outpaces data loading, and data loading is the bottleneck. \n",
    "\n",
    "To showcase the former, we will proceed to run two separate model loops on imagenet: the first one doing data loading without any training, followed by one with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 16:44:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=44774;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=174608;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m16\u001b[0m-\u001b[1;36m44\u001b[0m-\u001b[1;36m50\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=55055;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=351128;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mexperiment\u001b[0m=\u001b[35mprofiling_cpu\u001b[0m   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mDataloading\u001b[0m\\ only          \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    trainer.logger.wandb.name=\"Dataloading only\" \\\n",
    "    datamodule.num_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 17:02:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=318334;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=306596;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m17\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m14\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=259532;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=63312;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mexperiment\u001b[0m=\u001b[35mprofiling_cpu\u001b[0m   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mnum_workers\u001b[0m=\u001b[1;36m4\u001b[0m   \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    datamodule.num_workers=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    datamodule.num_workers=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    datamodule.num_workers=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    datamodule.num_workers=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advancements in Graphical Processing Units (GPUs) are widely known to have enabled the deep learning revolution, particularly through faster computation, relative to CPUs. Given that we have the option to run both GPU and CPU workloads, let's compare their throughput. In most workflows, the speedup provided by a GPU is dramatic. For a few select workloads, particularly those with a low number of steps or lighter computation requirements, if a 1.5-2x slower performance is observed when using a CPU, as opposed to a GPU, the former may be worth considering, as they're a far less contested resource on the cluster and pose far fewer availability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_cpu \\\n",
    "    algorithm=example \\\n",
    "    trainer.logger.wandb.name=\"Dataloading + Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK FOR ACCURACY ON DASHBOARD - As evidenced in the former, adding training to our run results in a difference in the ballpark of 100 samples/s. This would indicate that we have a computation bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Mila's official documentation](https://docs.mila.quebec/Information.html) has a comprehensive rundown of the GPUs that are installed on the cluster. Typing ```savail``` on the command line when logged into the cluster, shows their current availability. Testing their capacity can yield insights into their suitability for different training workloads.\n",
    "As the Mila Research template is built with hydra as a configuration manager, it integrates [Multi-runs](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run/) by default. This makes it possible to sweep over different parameters for profiling or throughput testing purposes or both. For example, suppose we wanted to figure out how different GPUs perform relative to each other.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU               Avail / Total \n",
      "===============================\n",
      "2g.20gb               6 / 48 \n",
      "3g.40gb               3 / 48 \n",
      "4g.40gb               1 / 24 \n",
      "a100                  0 / 32 \n",
      "a100l                 0 / 88 \n",
      "a6000                 0 / 8 \n",
      "rtx8000              14 / 408 \n",
      "v100                  0 / 56 \n"
     ]
    }
   ],
   "source": [
    "!savail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the following prominent GPU classes:\n",
    "\n",
    "- NVIDIA Tensor Core GPUs: A100, A100L, V100 (previous gen)\n",
    "- NVIDIA RTX GPUs: A6000, RTX8000\n",
    "- Multi-Instance GPU (MiG) partitions: 2g.20gb, 3g.40gb, 4g.40gb  \n",
    "\n",
    "We will now proceed to specify different GPUs over training runs and compare their throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:rtx8000:1 \\\n",
    "    trainer.logger.wandb.name=\"RTX8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 13:33:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=246287;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666892;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m13\u001b[0m-\u001b[1;36m33\u001b[0m-\u001b[1;36m03\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=835344;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=966187;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mGPU\u001b[0m\\ -\\ A100               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mGPU\u001b[0m\\ types                \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:a100:1 \\\n",
    "    trainer.logger.wandb.name=\"A100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:v100:1 \\\n",
    "    trainer.logger.wandb.name=\"V100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sense of the former: if a GPU with lower maximum capacity is readily available, training on it may be more time and resource effective than waiting for higher capacity GPUs to become available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well are we using the GPU? Once we've done a few preliminary runs with candidate GPUs that we'd want to use, the GPU utilization can be measured and optimized. We generally aim for high GPU utilization. Is the GPU utilization high? (>80%?)  \n",
    "If it's low (<80%), then we can use the PyTorch profiler (or similar tools) to try to figure out where the bottleneck lies, and further tune our parameters to increase our utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 11:50:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=78076;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=898758;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m11\u001b[0m-\u001b[1;36m50\u001b[0m-\u001b[1;36m48\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=612036;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=652546;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mbatch_size\u001b[0m=\u001b[1;36m1\u001b[0m    \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mRTX8000\u001b[0m\\ -\\ batch\\ size\\ \u001b[1;36m1\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mBatch\u001b[0m\\ size\\ tests        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:rtx8000:1 \\\n",
    "    datamodule.batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 12:06:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=731053;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=366557;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m12\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m21\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=462864;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=920530;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mbatch_size\u001b[0m=\u001b[1;36m8\u001b[0m    \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mRTX8000\u001b[0m\\ -\\ batch\\ size\\ \u001b[1;36m8\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mBatch\u001b[0m\\ size\\ tests        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:rtx8000:1 \\\n",
    "    datamodule.batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 12:20:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=271112;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=705864;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m12\u001b[0m-\u001b[1;36m20\u001b[0m-\u001b[1;36m26\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=897324;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=330527;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mbatch_size\u001b[0m=\u001b[1;36m32\u001b[0m   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mRTX8000\u001b[0m\\ -\\ batch\\ size\\   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m32\u001b[0m                         \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mBatch\u001b[0m\\ size\\ tests        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:rtx8000:1 \\\n",
    "    datamodule.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 12:35:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=831424;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=96207;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m12\u001b[0m-\u001b[1;36m35\u001b[0m-\u001b[1;36m50\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=663693;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=91463;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mbatch_size\u001b[0m=\u001b[1;36m128\u001b[0m  \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mRTX8000\u001b[0m\\ -\\ batch\\ size\\   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m128\u001b[0m                        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mBatch\u001b[0m\\ size\\ tests        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    hydra.launcher.gres=gpu:rtx8000:1 \\\n",
    "    datamodule.batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once GPU selection and a reasonable batch size are chosen, more can be done to speed up a model's computation.\n",
    "- a\n",
    "- la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[09/16/24 13:05:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Submitit \u001b[32m'slurm'\u001b[0m sweep     \u001b]8;id=865887;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=436119;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         output dir :               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         logs/default/multiruns/\u001b[1;36m202\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m16\u001b[0m/\u001b[1;36m13\u001b[0m-\u001b[1;36m05\u001b[0m-\u001b[1;36m32\u001b[0m           \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         #\u001b[1;36m0\u001b[0m :               \u001b]8;id=50432;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py\u001b\\\u001b[2msubmitit_launcher.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=381740;file:///home/mila/c/cesar.valdez/idt/ResearchTemplate/.venv/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33malgorithm\u001b[0m=\u001b[35mexample\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mdatamodule\u001b[0m=\u001b[35mimagenet\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mbatch_size\u001b[0m=\u001b[1;36m32\u001b[0m   \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         datamodule.\u001b[33mpin_memory\u001b[0m=\u001b[3;91mFals\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[3;91me\u001b[0m \u001b[33mtrainer\u001b[0m=\u001b[35mprofiling\u001b[0m        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mresources\u001b[0m=\u001b[35mone_gpu\u001b[0m          \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer/\u001b[33mlogger\u001b[0m=\u001b[35mwandb\u001b[0m       \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mname\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mGPU\u001b[0m\\ -\\ RTX8000\\ -\\        \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         pinned\\ memory             \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mgroup\u001b[0m \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         =\u001b[35mPin\u001b[0m\\ memory               \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         trainer.logger.wandb.\u001b[33mtags\u001b[0m= \u001b[2m                        \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[35mprofiling\u001b[0m                  \u001b[2m                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!python project/main.py \\\n",
    "    experiment=profiling_gpu \\\n",
    "    datamodule.batch_size=32\n",
    "    datamodule.num_workers=4 ## optimal parameter from above tests, check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a profiler and what is it good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The former process, while straightforward, was a bit contrived - would having a bird's eye view of our models performance be of aid when trying to optimize its parameters? It certainly wouldn't hurt. Enter the profiler.  \n",
    "A profiler is a tool that allows you to measure the time and memory consumption of the modelâ€™s operators. Specifically, the PyTorch profiler output provides clues about operations relevant to model training. Examples include the total amount of time spent doing low-level mathematical operations in the GPU, and whether these are unexpectedly slow or take a disproportionate amount of time, indicating they should be avoided or optimized. Identifying problematic operations can greatly help us validate or rethink our baseline model performance expectations.\n",
    "\n",
    "[Multiple](https://developer.nvidia.com/blog/profiling-and-optimizing-deep-neural-networks-with-dlprof-and-pyprof/) [profilers](https://github.com/plasma-umass/scalene) [exist](https://docs.python.org/3/library/profile.html). For the purposes of this example we'll use the default [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    cudaDeviceSynchronize       100.00%      14.444us       100.00%      14.444us      14.444us           0 b           0 b             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.444us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import ProfilerActivity, profile \n",
    "\n",
    "profiler = profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    ")\n",
    "profiler.start()\n",
    "profiler.stop()\n",
    "print(profiler.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "[GPU Training (Basic) - LightningAI](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html)  \n",
    "[DeviceStatsMonitor class - LightningAI](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.DeviceStatsMonitor.html)  \n",
    "[PyTorch Profiler + W&B integration - Weights & Biases](https://wandb.ai/wandb/trace/reports/Using-the-PyTorch-Profiler-with-W-B--Vmlldzo5MDE3NjU)   \n",
    "[Advanced profiling for model optimization - Accelerating Generative AI with PyTorch: Segment Anything, Fast](https://pytorch.org/blog/accelerating-generative-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchtemplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
