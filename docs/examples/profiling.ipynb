{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mila Research Template leverages built-in PyTorch and Lightning functionality to make benchmarking accessible and flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding bottlenecks: Dataloading vs Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential use of .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the working directory to the project root\n",
    "notebook_path = Path().resolve()  \n",
    "project_root = notebook_path.parent.parent\n",
    "os.chdir(str(project_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python project/main.py \\\n",
    "    algorithm=example \\\n",
    "    datamodule=imagenet \\\n",
    "    ++trainer.max_epochs=0 \\\n",
    "    ++trainer.limit_train_batches=0\\\n",
    "\n",
    "#!python ../../project/main.py throws an error about relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!HYDRA_FULL_ERROR=1 python project/main.py \\\n",
    "#    algorithm=example \\\n",
    "#    trainer.max_epochs=1 \\\n",
    "#    +trainer.limit_train_batches=0.01\\\n",
    "#    +trainer.limit_val_batches=0.01\\\n",
    "#    datamodule=imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for throughput across GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Mila Research template, it is possible to sweep over different parameters for testing purposes.  \n",
    "For example, suppose we wanted to figure out how different GPUs perform relative to each other.  \n",
    "\n",
    "[Mila's official documentation](https://docs.mila.quebec/Information.html) shows which GPUs are installed on the cluster. Typing ```savail``` on the command line shows their current availability.  \n",
    "Testing their capacity can yield insights into their suitability for different training cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!savail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the following prominent GPU classes: a100, a100l, a6000, rtx8000, v100.  \n",
    "We will now proceed to specify different GPUs over training runs and compare their throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an example of a sweep over some parameters, \n",
    "# with the training throughput as the metric, \n",
    "# :: callbacks/samples_per_second, ### or add a devicestatsmonitor in\n",
    "# and using different kinds of GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sense of the former: if a GPU with lower maximum capacity is readily available, training on it may be more time and resource effective than waiting for higher capacity GPUs to become available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging with Weights & Biases (wandb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mila Research template integrates wandb functionality as a logger specification.   \n",
    "This has the advantage of being able to track additional metrics and create accompanying visualizations.  \n",
    "We will now create a wandb report comparing throughput between GPUs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a wandb report with the throughput comparison \n",
    "# between the different GPU types.\n",
    "# i.e. specify wandb as the logger and log the throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to maximize our throughput given GPU choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the best datamodule parameters to maximize the throughput \n",
    "## (batches per second) without training (NoOP algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measure the performance on different GPUS using the optimal datamodule \n",
    "### params from before (and keeping other parameters the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now sweep over model hyper-parameters to maximize the utilization of our selected GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using the results from before, do a simple sweep over model hyper-parameters \n",
    "#### to maximize the utilization of the selected GPU (which was selected as a tradeoff \n",
    "#### between performance and difficulty to get an allocation). For example if the \n",
    "#### RTX8000's are 20% slower than A100s but 5x easier to get an allocation on, use those instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "[GPU Training (Basic) - LightningAI](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html)  \n",
    "[DeviceStatsMonitor class - LightningAI](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.DeviceStatsMonitor.html)  \n",
    "[PyTorch Profiler + W&B integration - Weights & Biases](https://wandb.ai/wandb/trace/reports/Using-the-PyTorch-Profiler-with-W-B--Vmlldzo5MDE3NjU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchtemplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
